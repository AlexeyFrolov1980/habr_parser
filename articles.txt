Ссылка: https://habr.com/ru/articles/780224/
Как мы (не) вкатились в геймдев. Часть 2
Zinemay  (https://habr.com/ru/users/Zinemay/)
 Сияющие огоньки средь темного леса - что они скрывают?Это вторая часть из моей серии "Как мы не вкатились в геймдев". Первую можно прочитать кликнув вот сюда - клик.За усердной работой над персонажами и Расчетно Пояснительной Запиской как-то совершенно незаметно пролетела осень. Холодные порывы декабрьского ветра ощущались дыханием подкрадывающегося дедлайна на затылке, что очень мотивировало ускорять свой темп. Справедливости ради надо отметить, что шло всё более чем хорошо и в график – времени хватало и на заказы с фриланса, и на диплом, и на сами пары (что ещё шли в первое полугодие финального курса).На руках уже имелись: все необходимые инфографики, три нарисованные локации, один анимированный персонаж, ряд концепт-артов и РПЗ в 60+ страниц.Что было ещё было необходимо сделать: экраны главного меню, интерфейс, логотип игры, а также тизер-трейлер. В финале всё необходимо оформить под вид презентации и подготовится к защите.Расписав каждый пункт в списке дел и распределив задачи в нашем маленьком коллективе, мы двинулись дальше.Экраны главного менюКаждый раз, когда я думаю о хорошем интерфейсе – я думаю о его диегетическом исполнении в серии игр «Dead Space». Мне очень нравится эта идея о стирании барьера между игроком и игрой путем различных стилистических и визуальных решении. Потому, когда пришла пора работать над интерфейсом в своей игре – очень захотелось сделать что-то хотя бы отдаленно похожее. Да, индикаторов здоровья или патронов в нашем квесте нет, однако есть само меню и можно попытаться придумать что-то с ним. К примеру - сделать страницы главного меню не просто набором кнопок и экранов, но чем-то более личным для главной героини и самого игрока. Чем-то, что завлекало бы в сюжет с самых первых секунд запуска игры и являлось неотъемлемой частью выдуманного нами мира. По этой причине мы добавили на экраны предметы Сони, сделав их ключевыми элементами взаимодействия с игрой. Этими предметами стали: личный дневник, плеер, карманный фонарик и билет на автобус из лагеря.Главное - быстро передать основную задумку, не углубляясь в деталиНа скетче данная задумка выглядела хорошо, а потому - я решил продолжить ее реализацию. Немало помогал факт наличия уже нарисованных локации – на них получилось отработать общую стилистику проекта и структурированный принцип по работе с иллюстрацией. Ещё - не было необходимости осваивать новый софт и учиться чему-то, как это было с анимацией и игровым движком. От этого мы выигрывали в скорости перехода от скетча к чистовику и могли позволить себе чуть больше экспериментов.При работе над чистовиком нередко возникают новые идеи, детали и образы. К примеру – Единорог на дневнике изначально задумывался как плоский «принт» на обложке, однако в итоге стал объемной наклейкой с блёстками.Выше –  результат с одной оговоркой относительно страницы настроек. Сейчас добавлю чуть подробнее про каждый из экранов, а также о самой оговорке.Главный экранЛегкая степень потрепанности добавляет ему особый шармДля главного экрана была выбрана данная композиция со следующей задумкой – каждый раз, когда главная героиня в ходе истории знакомится с новым персонажем, олицетворяющий его предмет появляется на столе рядом с дневником. Для Артёма таким предметом станет медиатор, для Дедушки в Панамке – ручка. В перспективе, ближе к финалу игры экран был бы полон различных предметов, визуально награждая игрока за прогресс.СохраненияПерсонажи милой зарисовки на заднем фоне – Дрим и Терция из проекта “Puppet Hero”Желая начать или продолжить свое приключение, игрок открывает дневник Сони. Внутри – слоты сохранении и последние скриншоты, стилизованные под вид фотокарточек.Мне очень нравится, как всё в итоге получилось – страницы ощущаются фактурными, а кнопка-закладка играет на общее погружение. Фотокарточки вклеены скотчем под углом, добавляя пару баллов к ощущению «ручной работы». Довершает вид очень милая иллюстрация, которую выполнила Настя. Мне кажется, она особенно хорошо попала в стиль «зарисовок на полях» и без ее вклада страницы выглядели бы скучнее.НастройкиПервый вариант страницы настроекСтраница настроек, выраженная в двух предметах: плеер и фонарик, взаимодействуя с которыми игрок может регулировать громкость звука и уровень яркости в игре. Большую часть времени и буквально до момента перед защитой – эта страница сохраняла свой нынешний вид. А дальше, когда до часа «икс» оставались считанные дни, мы с Джереми поняли, что нынешний результат нас не устраивает.Регулировать звук, нажимая на кнопки плеера было не очень удобно, хоть и добавляло к ощущению погружения в игру. Мы решили заменить эти кнопки на ползунки для более точной регулировки. Ещё – хотелось большей возможности настроить игру под себя: поменять язык, клавиши, отключить эффекты. Нам необязательно было действительно добавлять эти функции в нашу короткую демо-версию, но дать иллюзию такого контроля было бы здорово. Как итог – второпях и жертвуя сном, но мы учтём все недостатки предыдущей версии и поменяем эту страницу. Теперь - лучше.Помимо технических правок я также внес и визуальные – перерисовал плеер и фонарик с нуля, потратив чуть больше времени на проработку. Кнопки и надписи сделал чуть контрастнее их фону.Пользуясь случаем хочу ещё раз подчеркнуть вклад своих товарищей и их самоотдачу. Когда вы среди ночи вносите такое количество изменении в страницу и это лишь укрепляет общий раж сделать что-то хорошее – невозможно не почувствовать себя счастливым.ВыходЕсли с дневником, плеером и фонариком всё было понятно с самого начала, то насчёт идеи для страницы выхода пришлось подумать чуть больше. Это мог быть ключ, открытая дверь или обратная сторона дневника - но все эти варианты казались скучными, неподходящими. А затем - меня осенило. Если автобус привозит главную героиню и игрока в мистический лагерь, то пусть обратный билет и будет выходом из игры.Люблю работать над проработкой стилизованной бумаги. Все эти аккуратные складки, потертости и кремовая палитра очень приятно складываются в итоговый результат.Вопрос о выходе и два варианта ответа. Немного, но больше вроде и не нужно.Интерфейс внутри игрыПосле экранов главного меню мы двинулись дальше, занявшись разработкой интерактивных иконок внутри уровней.Вообще, здесь было очень большое количество идей – внести инвентарь, личный дневник с альманахом и «записками», маркеры квеста и многое другое.Мне повезло сохранить некоторые бумаги, на которых я набрасывал идеи и раскадровкиНо мы обучены горьким опытом и не ломимся вперёд. Вместо этого рисуем скетчи на бумаге, обсуждаем каждую функцию и приходим к выводу, что практически всё придуманное нами поверх стандартного геймплея в квестах – лишнее. Для рассказа полноценной истории хватит и базовых функции по типу интеракции с предметами и диалогов, а потому - лучше направить свои силы и время на них.Иконки интеракции и окно для текстаПри нахождении рядом с интерактивным предметом должна возникать иконка. Требования к ней просты: ее хорошо видно на любом фоне, она визуально объясняет к чему приведет взаимодействие, а также отображает кнопку ее активации (на случай если объектов несколько, или же к одному объекту применимо несколько действии).Тоже самое распространяется и для текстового окна, в котором всплывает описание объекта или мысли главной героини относительно него – высокий контраст, легкое чтение содержания.Лаконичные и контрастные.Текст легко считывается.Иконки нам понравились, однако мы решили на них не останавливаться и дать ещё один маркер для взаимодействия в виде яркой внешней линии объекта, что проявлялась при приближении к нему. Объект интеракции сразу выходит на передний фон с помощью обводки.Так игрок точно видел с чем взаимодействует, если объектов слишком много. А ещё - изначально идеи иконок интеракции и внешней линии даже ставились друг-другу в контру, однако оказалось, что их лучше попросту совместить.Диалоговое окноВ ходе сюжета персонажи должны были вести беседы, но показывать весь текст лишь через всплывающие облака – нам не хотелось. Быстрый поиск по референсам привёл нас к визуальным новеллам, где обычно демонстрируется более детальный портрет персонажа и его реплика в нижней части экрана. Так мы и поступили.Сияет энтузиазмом.Я перерисовывал портрет детализированной Сони из раза в раз, пока не достиг результата на изображении сверху. Лица - вообще не моя стихия, но благо упрощенная стилистика помогала не утонуть в тонкостях анатомии. А вот само оформление диалогового окна было удобным. Этот способ позволял нам показать игроку чуть более проработанный облик персонажей и тем самым рассказать о них больше. Ещё – передавать мимику говорящего и его состояние.Немного про айдентику: иконка для рабочего стола и логотипИконкаСмотрят прямо тебе в душу!У Сони было много «лиц», но тех, что использовались в качестве иконки для рабочего стола у нас набралось три. Первая иконка устарела с того момента, как мы поменяли облик главной героини. Вторая – очень нравилась нам, однако с большого расстояния читалась не так хорошо. Третья была вырезанным элементом с более проработанного рисунка Сони и оказалась лучшим вариантом.Она хорошо работала сразу по нескольким причинам: рыжие волосы и бежевая кожа делили квадрат на две равные половины по диагонали, создавая приятную комбинацию цвета и форм. Глаз, изображенный на иконке должен был сработать как «психологический крючок», из соображения о том, что именно в глаза мы смотрим при встрече с кем-либо. Ее мы и оставили.ЛоготипПомимо иконки также было необходимо разработать и логотип. Для вдохновения мы обратились к основным аналогам и увидели, что это практически всегда стилизованное название игры в витиеватом исполнении.Список наших основных аналогов и вместе с тем - моих любимых игр.Мне хотелось последовать примеру и стилизовать логотип, но при этом не искажать его слишком сильно. Все надписи заголовков и названии у нас уже были закреплены за строгим и ровным шрифтом «Bebas Neue». Его решили придерживаться и в дальнейшем.Однажды я обязательно увижу тебя где-нибудь в Стиме. Обещаю.Я представлял, как будет выглядеть этот логотип при публикации материалов по игре где-либо, а потому выдал ему линии-рамки, дабы можно было легко добавлять текст как сверху, так и снизу. Стилизовал букву «О» под глаз Сони и постарался продолжить ощущение движения слева-направо добавив ресницы на букву «А». Сейчас наработки по диплому опубликованы на Artstation-е, где мне удалось увидеть задумку с рамкой-линиями вокруг логотипа в действии. По-моему, выглядит неплохо.Не очень вписывается в ряд к остальным работам в портфолио ввиду стилистики, но не опубликовать свое любимое творение - я не мог.Откровенно говоря, задача по созданию логотипа игры откладывалась каждый раз и смещалась более интересными и комплексными задачами. Это продолжалось до тех пор, пока все остальные дела попросту не кончились. Думаю, это потому что за все годы обучения в университете я так и не сумел прикипеть душой к графическому дизайну и строгому вектору – рисование всегда тянуло меня больше.Логотип нашей бандыА вот с логотипом для нашей маленькой команды получилось чуть интереснее. Сделать мы его решили для поднятия боевого духа и более солидной атмосферы. Чтобы потом, аки крупные ребята продемонстрировать название своей «студии» в трейлере.Сама концепция была следующей – каждый из участников выбрал свой объект-талисман, а после всё объединилось в одном образе и названии. Настя выбрала моль (Moth), Джереми выбрал кота (Cat), я – шляпу (Hat). Пару раз встряхнув буквы и убрав повторяющиеся -  получили «Mohat».За визуальное воплощение задумки в жизнь взялась Настя, создав ряд очень милых, динамичных и приятных скетчей Итак, с ее разрешения я публикую первоначальные зарисовки далее.Каждый словно рассказывает маленькую историю. Варианты с Суши вместо Кота обоснованы тем, что Джереми предлагал два объекта на выбор.Мы попытались провести выбор через обычное обсуждение, но правда в том, что нам нравилось всё. Всё, увы, выбрать было нельзя. Пришлось устраивать голосование с постепенным «выбыванием», которое привело к победе номера восемь. Но даже сейчас, спустя время, мы все ещё не уверены в том, что же из списка действительно лучше.Далее скетч был передан мне на переработку в вектор. В процессе я всеми силами старался уберечь шарм и сказочность оригинала. Надеюсь, у меня получилось.Чуть позже выяснили - оказывается, студия Mohat уже существует. Мы выкрутились, добавив себе слово "entertainment".Промежуточные итогиДанный промежуток работы над дипломным проектом выдался довольно спокойным и в какой-то мере даже однообразным. Если в самом начале я имел дело с написанием большого объема текста и изучением новых программ и техник, то сейчас просто занимался тем же, чем и на фрилансе – рисовал скетчи, утверждал их, а затем превращал в полноценные страницы.В университет больше не было нужды ходить. Пары подошли к концу и лишь куратор время от времени выходил на связь для проверки прогресса, да и то – делал это онлайн. Подобный расклад сильно менял подход к работе и позволял меньше переживать по поводу успеваемости, ведь теперь в моем распоряжении было намного больше времени.Наша команда осела в телеграме, создав там общий чат. Джереми и Настя быстро сдружились и теперь весь фидбек, правки и идеи обсуждались в одном месте, что ускорило обратную связь. Помимо чата были попытки улучшить нашу производительность через различные сервисы по типу Trello, однако они не увенчались успехом и ощущались лишними.         Так, уверенно и методично, мы шли по списку задач и впереди оставалась лишь самая малость: тизер-трейлер, презентация, оформление экспозиции и сама защита дипломного проекта. О них я расскажу в следующей части.Надеюсь, читать данную статью было интересно. Большое спасибо за ваше внимание и спасибо за поддержку под предыдущей статьей!    
--------------------------
Ссылка: https://habr.com/ru/articles/779652/
RFM-сегментация в оптимизации CRM-стратегий
Dilemma  (https://habr.com/ru/users/Dilemma/)
 Сегодня мы продолжим погружение в тему оптимизации CRM маркетинга при рассылке промо-предложений и рассмотрим опыт применения RFM-сегментации для решения этой задачи в рамках сервиса доставки еды и продуктов притания Delivery Club, ныне присоединившегося к группе компаний крупного e-com сервиса.Основной вопрос менеджера CRM аналитикуМатематические аспекты, продуктовые аспекты и программный код, представленные в статье, разработаны или изучены автором и переработаны специально для лекций курса и workshop'ов и семинаров с целью достижения необходимого уровня обфускации данных.Иллюстрации взяты из открытых источников и также созданы самим автором данной статьи.СодержаниеI. RFM-анализ для оптимизации промо-акций:Общее описание задачи и целей оптимизации промо;Общий алгоритм RFM-сегментации;Формирование стратегии коммуникаций;Post-hoc анализ.II. Методы формирования RFM-сегментов.     Общее описание:Статистический анализ;Кластеризация методами машинного обучения.III. Последующие шаги развития системы: куда и как двигаться?     Заключение     Acknowledgments RFM-анализ для промо-акций1. Общее описание задачи и целей оптимизации промоПопулярной механикой работы с клиентской базой в рамках CRM маркетинга в e-commerce сервисах является рассылка промо-кодов определенным группам пользователей. Ключевой задачей рассылки промо-кодов является работа с лояльностью клиентов: удержание потенциально попадающих в отток покупателей, увеличение продаж в денежном эквиваленте и прибыли с этих продаж и снижение маркетинговой стоимости каждой отдельно взятой покупки клиента и общих затрат на скидочные предложения. Обозначенные задачи можно формализовать в ряд метрик, с которыми можно работать в процессе оптимизации. Среди них, к примеру, могут быть: Retention Rate или RR, Stickiness, AOV или средний чек, RPO, RPU, Revenue кумулятивный, GMV или оборот, Churn Rate, кумулятивные затраты на повышение RR, LTV, CPO, CPU и др.Метрики, оптимизируемые в процессе работы с промо-акциями в CRM маркетингеОдним из наиболее простых и эффективных способов оптимизации стоимости компаний по рассылке промо-кодов и повышения Retention является сегментирование аудитории покупателей-клиентов в приложении.2. Общий алгоритм RFM-сегментацииRFM-анализ — метод исследования особенностей поведения пользователей в процессе покупки, применяемый при сегментации пользователей, основанный на оценке метрик давности (recency или R), частоты (Frequency или F) и суммы затрат на покупки (Monetory или M), призванный помочь определить клиентов, приносящих больший оборот продукции, GMV, и своевременно выявить клиентов, которые потенциально могут уйти в отток.Recency, Frequency, MonetoryИсходя из исходной постановки целей оптимизации, RFM-анализ может применяться для нескольких задач:Формирование стратегии коммуникаций;Post-hoc анализ.Примеры использования результатов:Витрина сегментов аудитории, tracking перетекания и оттока;Формирование стратегий коммуникации для различных сегментов: передача id кластера в качестве дополнительной метки пользователя в CRM-систему.3. Формирование стратегии коммуникацийПроведя сегментацию клиентов, мы сможем увидеть ярко выраженные паттерны поведения групп пользователей в отдельных кластерах по R, F и M метрикам, например:Как и ожидается при кластеризации, можно видеть определенные особенности в значениях метрик, соответственно:Давность захода R: - 1 — недавние клиенты в рамках рассматриваемого периода,- 2 — клиенты со средней и чуть ниже средней давностью захода в рамках периода;- 3 — клиенты со средней и чуть выше средней давностью захода в рамках периода;- 4 — клиенты с давним последним заходом, относящимся к началу рассматриваемого периода и не заходившие давно;Частота F:- 1 — высокочастотные клиенты в рамках рассматриваемого периода, с большим количеством покупок,- 2 — клиенты со средней частотой и чуть выше, чем средней частотой покупок в рамках периода;- 3 — клиенты со средней частотой и чуть ниже, чем средней частотой покупок в рамках периода,- 4 — низкочастотные клиенты с наименьшим числом покупок в рамках рассматриваемого периода;Сумма покупок M:- 1 — клиенты с наибольшей суммой покупок в рамках рассматриваемого периода,- 2 — клиенты со средней и чуть выше средней суммой покупок в рамках периода;- 3 — клиенты со средней и чуть ниже, чем средней суммой покупок в рамках периода ,- 4 — клиенты с наименьшей суммой покупок в рамках рассматриваемого периода;Исходя из значений RFM показателей в кластерах можно выделить определенные группы и особенности поведения пользователей этих кластеров, описав каждый кластер или набор из нескольких кластеров по показателям метрик в этом кластере или наборе:Сегменты клиентов по кластерам RFИсходя из особенностей группы формируются и стратегии коммуникации с нею:группу риска нужно прорабатывать, присылая им наиболее выгодные предложения, одновременно балансируя нагрузку, чтобы не привести к спам-эффекту, информировать об актуальных предложениях, категориях товаров, полезных для них новинках, составляя предложения на основе интересов этих пользователей,отточников нужно возвращать уже при помощи более настойчивых стратегий и, возможно, прибавляя к CRM-механикам, включающим пуши, SMS, ещё и performance-рекламные компании — предлагайте им альтернативные товары и категории, не скупитесь на скидки, рассказывайте о нововведениях, особых возможностях, новостях об улучшении интерфейсов и функциональности приложения, актуальных предложениях, специальных персональных скидках и снижении цен, улучшении состава товаров в каталогах и их качественных усовершенствованиях,лояльных можно сделать VIP-группой, которой предлагают продукты из premium-категорий, более дорогие по цене, наблюдая за реакцией, их нужно не трогать большую часть времени, однако подкреплять их интерес периодически пушами с благодарностью и, возможно, предлагая anniversary codes на какие-то особые даты, или редкими пушами с потенциально интересными категориями товаров, которые клиенты ещё не приобретали, но по схожести их интересов могут быть заинтересованы;перспективных нужно поддерживать и аккуратно периодически триггерить на увеличение частотности покупки и среднего чека, предлагая актуальные предложения, не задабривая избыточными промо-кодами и не заспамливая их эфир, чтобы они не отключили пуши или вовсе не ушли из приложения, перестав им пользоваться или даже удалив его.Спрашивайте лояльных и перспективных клиентах о об отзывах для промотирования вашего приложения во вне.Важно отметить новичков, c которыми нужно продумать отдельные стратегии коммуникации: новые клиенты, зарегистрировавшиеся в рамках периода недавно, пока сложны для проведения анализов и составления выводов — о них у нас не так много данных и, следовательно, о них сложно сделать выводы в моменте. Кроме того, возможно им нужен онбординг больше, чем скидочные предложения.4. Post-hoc анализRFM-анализ позволяет определять принадлежности конкретных пользователей к конкретному сегменту для принятия решений о стратегиях коммуникации с ними в моменте и также наблюдать состояние аудитории сервиса в динамике, трекая изменения процентного соотношения сегментов.Изменение доли сегментов с течением времениЭто позволяет делать выводы о “здоровье” сервиса, например, если растет доля пользователей в оттоке, но при этом достаточно высока доля новых пользователей, это сигнал, что пользователи не задерживаются на сервисе и время проводить исследование причин — плохой сервис и недовольство пользователей, неправильные таргетинги с привлечением нерелевантной аудитории, изменение коньюнктуры рынка и в особенности спроса под воздействием внешних обстоятельств или какие-то другие причины, и найдя их, начать “лечить”.Изменения доли сегментов на протяжении месяца до и месяца после рассылкиПосле проведения очередного цикла промо-акций, можно провести повторную сегментацию пользователей и посмотреть на изменения соотношений в сегментах. Кроме того, можно посмотреть распределение долей сегментов на тех же пользователях, которые участвовали в предыдущем цикле рассылок, без учета новых регистраций и покупателей, привлеченных уже после старта предыдущего цикла рассылок — это позволит понять влияние именно на retention core-аудитории, не фокусируясь на результатах работы acquisition стратегий, которые непосредственно влияют на долю новых.Изменения доли сегментов на протяжении месяца до и месяца после рассылкиТаким образом RFM-сегментацию можно использовать также и для проведения Post-hoc анализа проводимых промо-акций и больших промо-компаний.Методы формирования RFM-сегментов. Общее описаниеМы поговорили о практических продуктовых аспектах применения RFM-сегментации в CRM-маркетинге и теперь давайте познакомимся с вариантами технической реализации создания RFM-сегментов. Обычно на практике с этой целью используются методы кластеризации или простые статистические методы, например, разбиение на кластера по квантилям.1. Статистический анализИтак, к нам поступили данные о пользователях приложения и их покупкам в нем, приступим к моделированию, предварительно проведя анализ EDA:import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings("ignore")

data = pd.read_csv('/data/rfm_data.csv')

data.describe()Визуализируем распредение наших данных:plt.figure(figsize=(12,12))
plt.subplot(311)
data['Recency'].hist(bins=40, alpha=0.8, label='Value', 
                     edgecolor='white', linewidth=2)
plt.ylabel('Recency')
plt.subplot(312)
data['Frequency'].hist(bins=40, color = 'green', alpha=0.8, label='Value', 
                       edgecolor='white', linewidth=2)
plt.ylabel('Frequency')
plt.subplot(313)
data['Monetory'].hist(bins=40, color = 'yellow', alpha=0.8, label='Value', 
                      edgecolor='white', linewidth=2)
plt.ylabel('Monetory')
plt.show()Первый простой метод, которым мы воспользуемся, это квантильный RFM:### 1. Квантильный RFM:
def rfm_quantile(rfm_data: pd.DataFrame() = None, metrics: list() = None, q: int = 4):
    if rfm_data is None:
        return None
    if metrics is None:
        metrics = rfm_data.columns[1:]
    Recency, Frequency, Monetary = metrics
    rfm_data[metrics] = rfm_data[metrics].dropna().astype('int64')

    labels = range(q, 0, -1)
    rfm_data['R'] = pd.qcut(rfm_data[Recency], q=q, labels=labels[::-1]).values
    rfm_data['F'] = pd.qcut(rfm_data[Frequency], q=q, labels=labels).values
    rfm_data['M'] = pd.qcut(rfm_data[Monetary], q=q, labels=labels).values
    rfm_data['RFM_Segment'] = rfm_data.apply(lambda x: str(x['R']) +
                                                       str(x['F']) +
                                                       str(x['M']), axis=1)
    rfm_data['RF_Segment'] = rfm_data.apply(lambda x: str(x['R']) +
                                                       str(x['F']), axis=1)
    rfm_data['FM_Segment'] = rfm_data.apply(lambda x: str(x['F']) +
                                                       str(x['M']), axis=1)
    rfm_data['RFM_Score'] = rfm_data[['R', 'F', 'M']].sum(axis=1)
    return rfm_dataСуть метода очень проста: в качестве кластеров принимаются границы квартилей [0..25), [25..50), [50..75), [75..100]. Проведем моделирование и посмотрим на результаты:rfm_result = rfm_quantile(data, metrics = ['Recency', 'Frequency', 'Monetory'], q = 4)Посмотрим на тепловую карту получившихся кластеров:rfm_aggr = rfm_result[['user_id', 'R', 'F']].groupby(['R', 'F']).user_id.nunique().reset_index()
rfm_pivot = rfm_aggr.pivot(values = 'user_id', index='R', columns='F')
sns.heatmap(rfm_pivot, annot=True, fmt=".0f")Количество пользователей в кластерах, построенных методом квартилейКак видим, меньше всего у нас новичков с малым числом покупок. Посмотрим соотношение в долях по кластерам к общему количеству пользователей:rfm_aggr['users_share'] = rfm_aggr['user_id']*1.0/\
                          rfm_aggr['user_id'].sum()*100.0
sns.heatmap(rfm_aggr.pivot(values = 'users_share', 
                           index='R', columns='F'), annot=True, fmt=".3g")Доля пользователей в кластерах, построенных методом квартилейМожно видеть, что метод дает очень наглядные представления о распределении пользователей по сегментам, однако возникает вопрос: можем ли мы улучшить его точность? Да, можем, и на помощь нам приходят методы кластеризации в машинном обучении.2. Кластеризация методами машинного обученияВ качестве метода кластеризации мы воспользуемся простым алгоритмом k-средних, который заключается в определении элементов множества, точек, ближайшим образом расположенных к предварительно случайным образом выбранным центроидам кластеров. При этом реализуем как кластеризацию по каждому отдельному параметру из множества R, F, M, так и по всем 3-м параметрам вместе. Ниже представлена разработанная на основе этого требования функция кластеризации:def rfm_kmeans(rfm_data: pd.DataFrame() = None, metrics: list() = None, name: str = '', n_clusters: int = 4):
    if rfm_data is None:
        return None
    if metrics is None:
        metrics = rfm_data.columns[1:]
    # Сlustering
    Recency, Frequency, Monetary = metrics

    kmeans_clusters = KMeans(n_clusters=n_clusters, init='k-means++', 
                             max_iter=300, random_state=0)
    kmeans_clusters.fit(rfm_data[[Recency]].dropna().astype('int64'))
    rfm_data['R_Cluster'] = 4-kmeans_clusters.labels_

    kmeans_clusters = KMeans(n_clusters=n_clusters, init='k-means++', 
                             max_iter=300, random_state=0)
    kmeans_clusters.fit(rfm_data[[Frequency]].dropna().astype('int64'))
    rfm_data['F_Cluster'] = kmeans_clusters.labels_+1

    kmeans_clusters = KMeans(n_clusters=n_clusters, init='k-means++', 
                             max_iter=300, random_state=0)
    kmeans_clusters.fit(rfm_data[[Monetary]].dropna().astype('int64'))
    rfm_data['M_Cluster'] = kmeans_clusters.labels_+1

    kmeans_clusters = KMeans(n_clusters=n_clusters, init='k-means++', 
                             max_iter=300, random_state=0)
    kmeans_clusters.fit(rfm_data[metrics].dropna().astype('int64'))
    # Assign the clusters to datamart
    rfm_data['K_Cluster'] = kmeans_clusters.labels_+1
    return rfm_dataДля запуска алгоритма кластеризации k-средних необходимо предварительно определить количество кластеров, поможет в этом нам метод “локтя”, то есть моделирования метрики внутри-кластерной суммы квадратов расстояний между точками кластера и центроидами кластера, иначе называемой инерцией, и выбора точки, после которой при увеличении количества кластеров инерция перестает снижаться и выходит фактически на плато:def wcss(rfm_data: pd.DataFrame() = None, metrics: list() = None):
    if rfm_data is None:
        return None
    if metrics is None:
        metrics = rfm_data.columns

    wcss = {}
    for k in range(1, 11):
        kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300)
        kmeans.fit(rfm_data)
        wcss[k] = kmeans.inertia_
    # plot the WCSS values
    sns.pointplot(x=list(wcss.keys()), y=list(wcss.values()))
    plt.xlabel('K Numbers')
    plt.ylabel('WCSS')
    plt.show()
    return

wcss(data[['Recency', 'Frequency', 'Monetory']])Инерция в зависимости от количества кластеров, ручной методКак можно видеть, не смотря на хорошую сходимость метода иногде определить на графике “локоть”, точку перегиба — из графика не очевидно, это точка с количеством кластеров 3 или 4 или какая-то другая.Воспользуемся библиотекой YelloBrick для более точного определения количества кластеров. YelloBrick — это удобная библиотека для автоматизации и визуализации подобных расчетов и исследований в машинном обучении, упрощающая многие операции предварительного анализа для последующего моделирования методами ML. Определение количества кластеров при помощи билиотеки YellowBrick очень простое и наглядное:from yellowbrick.cluster import KElbowVisualizer
# Instantiate the clustering model and visualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(1,12))
visualizer.fit(data[['Recency', 'Frequency', 'Monetory']]) # Fit the data to the visualizer
visualizer.show() # Finalize and render the figureИнерция в зависимости от количества кластеров — библиотека YellowBricksТеперь мы видим, что нам подходит использование разбивки на 4 кластера.Если бы кластеров было меньше, чем четыре, то в последующей интерпретации результатов мы бы отказались от части сегментов, которые озвучивали в части про формирование стратегии коммуникаций, и схлопнули существующие сегменты.Проведем моделирование алгоритмом k-средних и визуализируем результаты:result = rfm_kmeans(data[['user_id', 'Recency', 'Frequency', 'Monetory']], 
                    n_clusters=4)
rfm_aggr = result[['user_id', 'R_Cluster', 'F_Cluster']].\
                    groupby(['R_Cluster', 'F_Cluster']).user_id.nunique().reset_index()
rfm_pivot = rfm_aggr.pivot(values = 'user_id', index='R_Cluster', columns='F_Cluster')Количество пользователей в кластерах, построенных методом k-meansКак можно видеть, кластера распределены уже не так равномерно и красиво, как при моделировании при помощи квантильного подхода. В процентном соотношении также наблюдается смещение внутри определенных кластеров:rfm_aggr['users_share'] = rfm_aggr['user_id']*1.0/rfm_aggr['user_id'].sum()*100.0
sns.heatmap(rfm_aggr.pivot(values = 'users_share', index='R_Cluster', 
                           columns='F_Cluster'), annot=True, fmt=".3g")Доля пользователей в кластерах, построенных методом k-meansТаким образом k-средних обнаружил особенности в данных, которые не были учтены при квантильном подходе. Проверив вариант разбиения с тремя кластерами и убедившись в отсутствии переобучения модели, приходим к выводу, что можем использовать данную сегментацию в качестве базовой.Последующие шаги развития системы: куда и как двигаться?Как мы и проговорили изначально выше, RFM-сегментация — это довольно простой алгоритм, основные достоинства которого — простота реализации и довольно высокая для такого базового алгоритма эффективность, однако существуют и более эффективные модели, которые применяются на практике для повышения точности и полноты результатов:Модели оттокаКласс моделей оттока: Уточнение сегментации vможно проверсти путем создания модели оттока — в ней мы предсказываем вероятность покупки или её отсутствия через N дней и отправляем сообщение пользователям с наименьшей вероятностью покупки;Класс uplift-моделей:В процессе uplift-моделирования мы пытаемся максимизировать разницу между вероятностью покупки у пользователей при отсутствии коммуникации и в случае коммуникации с пользователем. Метрики Uplift рассчитываются по-разному, популярными и общепризнанно эффективными являются расчеты на основе дивергениции Кульбака-Лейбнера, о чем можно почитать подробнее в статьях про Uplift-моделирование.Сложная красота математики uplift-моделиЗаключениеВсе мы в рамках e-commerce сервисов стремимся к оптимизации cost’ов и повышению лояльности пользователей, применяя при этом затратные стратегии, к примеру, рассылки промо-кодов, и хотим делать это наиболее эффективно. Рассмотренные нами выше простые методики могут в этом и не займут много времени для реализации в качественной инфраструктуре. Пробуйте, экспериментируйте и добивайтесь более эффективных результатов.Данный метод ранее освещался в виде доклада на ряде конференций (EpicGrowth Meetup-2021, Нетология, митапы в рамках программы VK Study и Worki@Mail.Ru) и образовательных курсов и получил высокие оценки от участников в рамках программы повышения квалификации в VK Company (former Mail.Ru Group) и программ обучения студентов VK Образования в МГТУ им. Н.Э.Баумана, МГУ им. М.В.Ломоносова и других ВУЗов, поэтому надеемся, что он будет полезен и интересен более широкой аудитории.AcknowledgmentsFormer Delivery Club Tech Presents…Задумка и вдохновение на написание статьи появились достаточно давно, и вот, наконец, появилась возможность реализовать задуманное и передать опыт решения интересной и полезной задачи.За крутые задачи хочу особенно отметить команду 💚 Деливери Клаб:      своего потрясающего team-lead'а в Деливери Клаб Лёшу Пыжика,      своих непосредственных заказчиков           CMO, директора по маркетингу, директора по развитию крутейшего Олега Хаустова,               директора по CRM крутейшего Андрея Ленеца      и всех наших потрясающих маркетологов и CRM-менеджеров,           продактов и всех, всех, всех сопричастных коллег. Всем добра и Big Data :)      Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста. Кто Вы по профессии и специальности? 
            0%
           Разработчик, заинтересовался методикой 
            0
           
            0%
           Продуктовый/маркетинговый аналитик 
            0
           
            0%
           Продуктовый менеджер 
            0
           
            0%
           Маркетолог 
            0
           
            0%
           ML-разработчик 
            0
            
       Никто еще не голосовал. 

       Воздержавшихся нет. 
      Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста. Что Вас заинтересовало в статье? 
            0%
           Практическая ценность — внедряю данный метод в продукте для повышения эффективности 
            0
           
            0%
           Новый метод — изучаю существующие методологии и обучаюсь новому 
            0
           
            0%
           Научно-популярная статья — изучаю на досуге для развлечения 
            0
           
            0%
           Научно-популярная статья — изучаю на досуге 
            0
           
            0%
           Пытался реализовать метод ранее, возникли трудности и пришёл за рабочим кодом 
            0
            
       Никто еще не голосовал. 

       Воздержавшихся нет. 
    
--------------------------
Ссылка: https://habr.com/ru/articles/780216/
Как создать эффективную посадочную страницу в 2023: всего 3 фактора, от которых зависит конверсионность лендинга
estrizhak82  (https://habr.com/ru/users/estrizhak82/)
 Показываю, почему стандартные формулы для оформления лендингов работают не всегда и почему у сайта может быть мало конверсий. В конце — 6 советов о том, как увеличить отдачу от запуска рекламы на лендинг.  Евгения Стрижак, маркетолог-смысловикРаньше можно было набросать лендинг за один вечер буквально на коленке — без оффера, с размытыми смыслами, шаблонными выгодами, нечитабельными текстами. И получать заявки даже при скромных бюджетах. Для большинства бизнесов этот вариант был вполне рабочим.За последние два года ситуация сильно изменилась: доступных каналов продвижения стало меньше, трафик постоянно дорожает, стоимость привлечения клиентов из интернета растет. В текущих реалиях цена наспех собранного лендинга — слитый рекламный бюджет.Проговорим один важный момент:Конверсию лендинга можно оценивать только в связке с источником трафика Если на один и тот же сайт запустить контекст, таргет или настроить SEO-продвижение, то конверсия сайта будет разной. Даже отдельные инструменты внутри одной рекламной системы (например, Поиск, РСЯ и Мастер кампаний) дадут разную конверсию лендинга. Поэтому важна не конверсия сайта, а количество лидов и их стоимость.От каких трех факторов на самом деле зависит, насколько хорошо сайт будет превращать посещения в заявки, и как можно повысить эффективность рекламы, расскажу в этом материале:Фактор 1. Наполнение главного экрана и офферФактор 2. Структура сайтаФактор 3. Целевое действие: как выбрать CTAВ чем еще может быть проблема: что делать, если подготовили лендинг по правилам, но он не приносит заявок6 советов, как увеличить отдачу от запуска рекламы на лендингФактор 1. Наполнение главного экрана и оффер: что добавить и как убедиться, что мы решаем задачу пользователяЧто обязательно должно быть в шапке на главном экране:логотипдескрипторконтактные данныеЕсли вам важно подчеркнуть еще какие-то детали о своем бизнесе (например, членство в ассоциациях, наличие производства), то эту информацию тоже можно разместить в шапке.В центре экрана размещаем:УТП (ваш оффер)буллеты с ключевыми преимуществами и выгодамикнопку (или форму) с целевым действиемОбразец качественно оформленного главного экрана  Главный экран — ключевой элемент контакта с потенциальным клиентом, потому что его видит каждый посетитель. По моему опыту анализа лендингов, до второго экрана доходят не более 65% от всех посетителей. Если главный экран не вызвал доверия, а предложение не конкурентно, то с большой вероятностью сайт не будет приводить клиентов.  Как проверяем, будет ли работать главный экран?Посмотрите, отвечает ли он на 3 ключевых вопроса пользователя:туда ли я попал?это точно мне подходит?этому можно доверять?Если на все вопросы ответите утвердительно, то всё в порядке.Еще одна частая ошибка — отсутствие оффера. Если на главном экране написано, чем занимается компания, но нет конкретного предложения с деталями, значит, там нет оффера. Информация о вашем бизнесе сама по себе — это не оффер.Пример отсутствия оффера в заголовке  В оффере нужно показать конкретные условия и понятные выгоды, которые важны для вашей целевой аудитории. При поиске нужной информации человек открывает десятки сайтов из выдачи, поэтому лучше сразу сформулировать четкое предложение и подчеркнуть, почему стоит обратиться именно в вашу компанию.  Есть разные формулы создания оффера (или УТП):4U = Полезность + Уникальность + Специфичность + СрочностьPmPHS = Боль + Больше боли + Надежда + РешениеODC = Предложение + Ограничение + Призыв к действиюAIDA = Внимание + Интерес + Желание + ДействиеОднако эти формулы сложно применять тем, у кого нет навыков продающего копирайтинга. Поэтому мы рекомендуем использовать для платного трафика другую рабочую и простую для понимания формулу.Немаркетологам будет гораздо проще работать по такой формуле.  Пример оффера, составленного по формуле «Релевантное ключевое слово + выгода + выгода»  Также хорошо работает гиперсегментация   — когда ключевое слово из рекламы релевантно офферу. С помощью инструментов подмены заголовков можно сделать так, чтобы в заголовке вашего объявления пользователю показывался тот запрос, который он вводит. То есть в зависимости от входящего трафика будет меняться наполнение объявления. По моему опыту, подмена заголовков в некоторых нишах может повысить конверсию в целевое действие на несколько процентных пунктов (п. п.).  Пример того, как мы настраиваем подмену заголовков на проекте по интерьерным светильникам  Фактор 2. Структура лендинга  В самом простом варианте структура продающего лендинга выглядит так:что мы продаемсколько это стоитрезультаты нашей работыдоказательствакупи у нас, потому что это выгодногарантируем всё, что можно гарантироватькупи сейчас или проиграешьЕсли разбираться детальнее, то всё работает чуть сложнее. Структура сайта зависит от теплоты трафика, который приходит на лендинг.Лестница теплоты трафика  Вспомним лестницу Ханта. Человек приходит к покупке не сразу:  он проходит ступени от осознания проблемы до поиска способов решения и выбора поставщика.В контекстной рекламе рекламодатели обычно работают с последними ступенями, когда потенциальный клиент определяется с выбором поставщика: запросы с продающими добавками «купить», «заказать», «под ключ» и т. д. При таком подходе все рекламодатели попадают в «алый океан» с огромной конкуренцией, дорогими заявками из-за перегретого аукциона по горячим ключам и ограниченным спросом.Чтобы не проиграть в конкурентной борьбе, лендинг под горячий трафик должен быть максимально конкретным:Закрываем потребность целевой аудитории с помощью предложения (оффера) + показываем выгоды от покупки здесь через критерии принятия решения.Например, пользователь хочет найти замену европейским брендам светильников, ушедших с российского рынка. Мы закрываем эту потребность предложением купить светильники аналогичного качества от производителя из РФ с выгодами по срокам поставки и стоимостиПредлагаем расчет стоимости или консультациюПродумываем акцию, спецпредложение, эксклюзивные условия (это не всегда скидка)Основная задача лендинга, созданного для горячих клиентов — «продать» компанию как поставщика Если продукт чем-то отличается от того, что предлагает рынок, обязательно показываем его преимущества. Но актуализировать боль или потребность пользователя на этой ступени не нужно: клиент сам понимает, что произойдет (или не произойдет), если он не решит проблему сейчас.Под теплые запросы, когда пользователи еще выбирают между решениями, продуктами и поставщиками, можно добавить в структуру лендинга несколько догревающих блоков, которые будут актуализировать потребность и подталкивать человека к покупке.Что надо добавить на лендинг под теплый трафик:заголовок по формуле 4U ( Полезность + Уникальность + Специфичность + Срочность)больше информационных блоков о преимуществах продукта и компаниипредлагаем расчет цены/подбор варианта + обучающий контентзаводим пользователя в диалог с чат-ботом, на свой канал или «догоняем» лид-магнитомПодытожимНа сайтах под горячий и теплый трафик 2, 3, 4 экраны — это отстройка от конкурентов + доверие + выгода от компании/продукта.Пример 2 экрана с конкурентными преимуществами компании  Фактор 3. Целевое действие: как выбрать CTA Задача целевого действия на сайте — перевести аудиторию на следующий этап воронки, а не продать. Продавать на лендинге в одно касание можно только недорогие товары в пределах 5000–10 000 рублей. Всё остальное требует промежуточного этапа: общения с менеджером, консультации, расчета стоимости, составления коммерческого предложения, презентации и т. д. Покупка происходит на следующих этапах воронки продаж.Какой он — эффективный призыв к действию на посадочной странице:1. Нестрессовый. Покупка — это целевое действие, которое вызывает стресс. В некоторых нишах часто предлагают в качестве первого шага бесплатный расчет, замер, оценку. Но если разобраться, то это довольно стрессовый шаг: нужно потратить время, пообщаться со специалистом компании.2. Ценный. Сам по себе расчет стоимости продукта или услуги ценности не несет, но вы можете усилить ценность первого шага дополнительными полезными материалами (чек-листами, инструкциями, видео с решением проблемы клиента), консультацией.3. Понятный. Чтобы снизить стресс от контакта, важно в лид-форме в паре слов рассказать, как будет проходить консультация. Это простое действие снизит тревожность потенциального покупателя.Отлично, если на странице благодарности за заявку вы расскажете, что будет происходить дальше: кто и когда перезвонит, отправит коммерческое предложение. Можно даже указать телефон, с которого будет звонить менеджер, показать его фото. Это тоже снизит уровень тревоги, и конверсия в звонок будет выше.Пример целевого действия для горячей аудитории  На лендинге можно продумать разные целевые действия для аудиторий разной теплоты:  горячей аудитории показываем открытую форму обратной связи с предложением оставить контактыменее теплой ЦА предлагаем полезные материалы (лид-магниты), рассылку, подписку на Telegram-каналВ чем еще может быть проблема: что делать, если подготовили лендинг по правилам, но он не принес заявок Если вы уже протестировали разные офферы, структуры лендинга, призывы, тексты, но сайт не конвертирует, это говорит о том, что:перед разработкой лендинга вы не провели маркетинговое исследование или провели его некорректно — собрали недостоверные данныепродукт не закрывает потребности аудитории, которая приходит на страницупродукт плохо упакован: выгоды от применения продукта не очевидны, сложно понять преимущества предложениякомпания не вызвала доверия у потребителяВ этом случае мы возвращаемся на шаг назад и начинаем с маркетингового исследования. В ходе анализа важно:1. Найти и сформулировать ключевые смыслы бизнеса: кто мы, что продаем и кому, на каких условиях2. Сегментировать целевую аудиторию по потребностям3. Выяснить путь клиента (customer journey map)В результате маркетингового исследования нужно составить карту смыслов. В карте накапливаются результаты конкурентного анализа, анализа целевой аудитории, рынка в целом, продукта. Так мы находим те самые смыслы, которые важно показать на лендинге.Пример карты смыслов  Ключевая идея посадочной страницы:соединить потребности аудитории с предложением компаниипоказать, как мы решим проблему и закроем потребность клиентапоказать преимущества от сотрудничества с компанией через выгоды для клиента6 советов, как увеличить отдачу от запуска рекламы на лендингСовет 1. Ключевое слово, заголовок объявления и содержание сайта должны быть релевантны друг другу. Человек должен увидеть на сайте на первых 3 экранах примерно то же, что написано в объявлении, но уже более подробно.Чтобы обеспечить релевантность, добавьте связующее звено между прототипировщиком сайта и специалистом по трафику. Например, их взаимодействие может контролировать маркетолог. Если маркетолога в штате компании нет, прототипировщик может сам курировать запуск.Совет 2. Тестируйте разные офферы и гипотезы. Офферы формируются на этапе маркетингового исследования. Кроме офферов можно тестировать структуру сайта и различные целевые действия. На этом этапе не обойтись без просмотра визитов с помощью Вебвизора Яндекс Метрики.Совет 3. По возможности подключайте сквозную аналитику. Это действенный инструмент, с помощью которого можно отследить всю цепочку от клика до денег и понять, с каких каналов трафика приходят платежеспособные клиенты.Пример сквозной аналитики проекта Совет 4. Выигрывает тот, у кого больше сайтов. Большинство рекламодателей используют всего 1 лендинг, а крупные компании запускают много посадочных с разных аккаунтов, получая львиную долю лидов. Если ваш лендинг хорошо работает, просто масштабируйте трафик.Количество горячего трафика ограничено, и чтобы не попадать в ситуацию «алого океана», можно посмотреть на путь клиента и подумать, как зацепить тех, кто находится на более ранних ступенях лестницы Ханта (кто еще не готов к покупке).Какие типы сайтов можно использовать для быстрой лидогенерации на разных ступенях готовности к покупке:супербыстрые квизы: подходят для ниш со сформированным спросом — например, для так называемых замерных ниш (тех, в которых перед сделкой нужно делать индивидуальные замеры и расчеты)подогревающие квизы: пошаговые опросники, где по мере прохождения опроса можно продавать идею, обучать продукту, делать расчет на летусайты-одноэкранники, даже не просто одностраничники: например, те, на которых есть только оффер и ссылка для скачивания каталогаподписные страницы, где описаны варианты решения проблемы пользователя и размещено предложение подписаться на канал, паблик, рассылку или чат-бота.Пример квизаСовет 5. Адаптируйте сайт под мобильные устройства. Еще пару лет назад конверсия с мобильных устройств была ниже, чем с ПК, практически во всех нишах. Сейчас видим, что конверсия с мобильных практически не отличается:люди стали профессиональными пользователями мобильных устройствэргономика и удобство оформления заказа с мобильных повысилисьЧто учитываем при разработке мобильной версии сайта:как аудитория принимает решение на мобильных устройствахкак лучше расставить акценты в мобильной версиикакие блоки вырезатькакие элементы взаимодействия с сайтом из версии для ПК оставить, а какие заменитьСовет 6. Работайте над своей уникальностью. Не копируйте избитые фразы, блоки и преимущества с лендингов конкурентов. Люди уже устали от стандартных заголовков типа «Наши преимущества», «Почему мы». Ищите, в чем ваша уникальность, и показывайте ее на сайте.    
--------------------------
Ссылка: https://habr.com/ru/articles/780164/
Синхронизация в Ktor для самых маленьких
Scogun  (https://habr.com/ru/users/Scogun/)
 ВведениеЭто был теплый осенний вечер, когда передо мной встала задача "настроить серверное кэширование в сервисе". Казалось бы чего может быть проще, наверняка в Ktor это делается парой строчек кода.  Однако реальность оказалась не такой радужной: Ktor предлагает только Cache Headers plugin, который, например, не поддерживает распределенное кэширование. Что ж, вооружившись IDE, я начал ваять свой велосипед - Ktor Simple Cache.Первые шагиСначала я задался вопросом: "А как вообще что-то прикрутить к Ktor?". Оказалось, что Ktor расширяем через plugin'ы, о чем подробно описано в документации.Достаточно быстро определились основные части plugin'а:Первый кодПровайдер кэша:abstract class SimpleCacheProvider(config: Config) {

    val invalidateAt = config.invalidateAt

    abstract fun getCache(key: String): Any?

    abstract fun setCache(key: String, content: Any, invalidateAt: Duration?)

    open class Config protected constructor() {

         var invalidateAt: Duration = 5.minutes
    }
}Тело plugin'а с конфигураций:class SimpleCacheConfig {

    internal var provider: SimpleCacheProvider? = null
}

class SimpleCache(internal var config: SimpleCacheConfig) {

    companion object : BaseApplicationPlugin<Application, SimpleCacheConfig, SimpleCache> {
        override val key: AttributeKey<SimpleCache> = AttributeKey("SimpleCacheHolder")

        override fun install(pipeline: Application, configure: SimpleCacheConfig.() -> Unit): SimpleCache {
            return SimpleCache(SimpleCacheConfig().apply { configure })
        }
    }
}Ну и сам plugin с конфигурацией:class SimpleCachePluginConfig {

    var invalidateAt: Duration? = null
}

val SimpleCachePlugin = createRouteScopedPlugin(name = "SimpleCachePlugin", ::SimpleCachePluginConfig) {
    val provider = application.plugin(SimpleCache).config.provider  
    onCall {
        val cache = provider.getCache(it.request.uri)
        if (cache != null) {
            it.respond(cache)
        }
    }
    onCallRespond { call, body ->
        provider.setCache(call.request.uri, body, pluginConfig.invalidateAt)
    }
}Идея донельзя простая:Перехватываем запрос - используем OnCallЛезем в кэш по имени методаЕсли кэш найден - возвращаемЕсли нет - вызываем route метод, а ответ сохраняем в кэш внутри onCallRespond.Я предполагал, что если внутри OnCall сделать it.respond(cache) - onCallRespond вызываться не будет. Однако первый же unit test показал, что это не так. Итак, мне надо было как-то передать в OnCallRespond информацию о том, что данные взяты из кэша и повторно сохранять их не надо. Благо, разработчики Ktor предусмотрели Call State.Изначально plugin выглядел так:val SimpleCachePlugin = createRouteScopedPlugin(name = "SimpleCachePlugin", ::SimpleCachePluginConfig) {
    val provider = application.plugin(SimpleCache).config.provider
    val isResponseFromCacheKey = AttributeKey<Boolean>("isResponseFromCacheKey")
    onCall {
        val cache = provider.getCache(it.request.uri)
        if (cache != null) {
            it.attributes.put(isResponseFromCacheKey, true)
            it.respond(cache)
        }
    }
    onCallRespond { call, body ->
        if (!call.attributes.contains(isResponseFromCacheKey)) {
            provider.setCache(call.request.uri, body, pluginConfig.invalidateAt)
        }
    }
}Первая рабочая версия Решено было сразу сделать и два провайдера: Memory и Redis.Memory не вызвал особых проблем и в первом приближении выглядел так:SimpleMemoryCacheProvider.ktclass SimpleMemoryCacheProvider(config: Config) : SimpleCacheProvider(config) {

    private val cache = mutableMapOf<String, SimpleMemoryCacheObject>()

    override fun getCache(key: String): Any? {
        val `object` = cache[key]
        if (`object` == null || `object`.isExpired) {
            return null
        }

        return `object`.content
    }

    override fun setCache(key: String, content: Any, invalidateAt: Duration?) {
        cache[key] = SimpleMemoryCacheObject(content, invalidateAt ?: this.invalidateAt)
    }

    class Config internal constructor() : SimpleCacheProvider.Config()
}

private data class SimpleMemoryCacheObject(val content: Any, val duration: Duration, val start: LocalDateTime = LocalDateTime.now()) {

    val isExpired: Boolean
        get() = LocalDateTime.now().isAfter(start.plusSeconds(duration.inWholeSeconds))
}

fun SimpleCacheConfig.memoryCache(
    configure : SimpleMemoryCacheProvider.Config.() -> Unit
){
    provider = SimpleMemoryCacheProvider(SimpleMemoryCacheProvider.Config().apply(configure))
}Для удобства работы был объявлен SimpleMemoryCacheObject класс, который бы хранил как данные, так и время жизни кэша.Ну и, сам кэш - это просто mutableMapOf<String, SimpleMemoryCacheObject>() в котором данные и лежат.С Redis же пришлось немного повозиться. Для работы с ним я выбрал Jedis библиотеку. Так как ответ от сервера может быть не только текстовый, но и некие DTO объекты, пришлось вооружиться Gson'ом. В результате, SimpleRedisCacheObject стал выглядит так:private class SimpleRedisCacheObject(val type: String, val content: String) {

    override fun toString() = "$type%#%$content"

    companion object {

        fun fromObject(`object`: Any) = SimpleRedisCacheObject(`object`::class.java.name, Gson().toJson(`object`))

        fun fromCache(cache: String): Any {
            val data = cache.split("%#%")
            return Gson().fromJson(data.last(), Class.forName(data.first()))
        }
    }
}Ну и, сам провайдер:SimpleRedisCacheProvider.ktclass SimpleRedisCacheProvider(private val config: Config) : SimpleCacheProvider(config) {

    private val jedis: JedisPooled = JedisPooled(config.host, config.port, config.ssl)

    override fun getCache(key: String): Any? = if (jedis.exists(key)) SimpleRedisCacheObject.fromCache(jedis.get(key)) else null

    override fun setCache(key: String, content: Any, invalidateAt: Duration?) {
        val expired = (invalidateAt ?: config.invalidateAt).inWholeMilliseconds
        jedis.psetex(key, expired, SimpleRedisCacheObject.fromObject(content).toString())
    }

    class Config internal constructor(): SimpleCacheProvider.Config() {

        var host = "localhost"

        var port = 6379

        var ssl = false
    }
}Конечно, все это я покрыл тестами (включая использования Redis TestContainer) и начал использовать в работе.Так как изначально кэшировался только endpoint с одинаковым текстом для всех, то имени метода в качестве ключа хватала на ура. Однако быстро оценив перспективы библиотеки, было решено покрывать ей и другие endpoint'ы. А там уже ключ должен был учитывать query parameters. По итогу конфигурация пополнилась полем queryKeys: List<String> (добавил их для одного особого случая), ну и ключ теперь формировался так:private fun buildKey(request: ApplicationRequest, queryKeys: List<String>): String {
    val keys =
        if (queryKeys.isEmpty()) request.queryParameters else request.queryParameters.filter { key, _ -> key in queryKeys }
    val key = "${request.path()}?${
        keys.entries().sortedBy { it.key }
            .joinToString("&") { "${it.key}=${it.value.joinToString(",")}" }
    }"
    return key.trimEnd('?')
}Наверно, надо бы и value сортировать (значения параметров тоже должны быть отсортированы), но я пока параметры-списки не использую.Новые проблемыВ какой-то момент, проверяя телеметрию сервиса, я заметил, что иногда запросы к БД идут не через время протухания кэша, а сразу несколько подряд. Стало понятно, что я забыл о многопоточности. При нескольких одновременных запросах, пока кэш еще не добавлен, несколько потоков подряд делают это. Прежде всего, покрыл это тестом:Check cache is cocurrency    @Test
    fun `check cache is concurrency`() {
        testApplication {
            val jsonClient = client.config {
                install(ContentNegotiation) {
                    json()
                }
            }

            application(Application::testApplication)

            runBlocking {
                val totalThreads = 1000
                val deferred = (1..totalThreads).map {
                    async {
                        jsonClient.get("long")
                    }
                }

                val result = deferred.awaitAll().map { it.body<TestResponse>() }.groupBy { it.id }
                    .map { it.key to it.value.size }
                result.shouldBeSingleton {
                    it.second.shouldBe(totalThreads)
                }
            }
        }
    }Запускаем параллельно 1000 запросов. Ждем завершение всех и проверяем, что все они имеют один и тот же результат (TestResponse при создании записывает в id случайное значение).Новое решениеСначала пытался обыграть все это через synchronized. Не помогло. Потом мучил ReentrantReadWriteLock - тоже безрезультатно.Пришел к тому, что надо бы делать методы suspend и играться с coroutines. Вначале пытался использовать Mutex с withLock. Затем перешел на Channel. Все равно все первые запросы писали в кэш.Решил полностью разобраться, как именно происходят вызовы, и добавил кучу логирования в тест. Благодаря этому, смог проследить весь цикл запроса-ответа и нашел главную проблему: запросы на кэш должны блокироваться с первого запроса, а разблокироваться только после записи в кэш. А благодаря тому, что когда-то получал MS Certificate, где одним из вопросов был потокобезопасный singleton решение всплыло самим собой (Mutex таки подошел идеально):    private val mutex = Mutex()

    override suspend fun getCache(key: String): Any? {
        var `object` = cache[key]
        if (`object`?.isExpired != false) {
            mutex.lock()
            `object` = cache[key]
            if (`object`?.isExpired != false) {
                return null
            } else {
                mutex.unlock()
                return `object`.content
            }
        }

        return `object`.content
    }

    override suspend fun setCache(key: String, content: Any, invalidateAt: Duration?) {
        cache[key] = SimpleMemoryCacheObject(content, invalidateAt ?: this.invalidateAt)
        mutex.unlock()
    }Логика проста:Пытаемся получить кэш - если все хорошо, возвращаем.Нет - блокируем потоки и еще раз проверяем, нет ли кэша. Это проверка нужна, если мы проскочили первую, но какой-то поток уже добавил кэш.Если кэша все еще нет - значит этот поток первый. Возвращаем null, чтобы plugin дернул метод и затем вызвал setCache, который и отпустит lock'и.Если есть - разблокируем другие потоки и возвращаем данные.Что дальше?Внимательный читатель, наверно, спросит: "А почему любые ответы кэшируются? И даже от 400 и выше!", - и попадет в точку. Сейчас, если вдруг сервис первым вернул какую-то ошибку, она закэшируется. Решается это вроде легко:if (!call.attributes.contains(isResponseFromCacheKey) && (call.response.status() ?: HttpStatusCode.OK) < HttpStatusCode.BadRequest) {
    provider.setCache(buildKey(call.request, pluginConfig.queryKeys), body, pluginConfig.invalidateAt)
}Вот незадача! Если такое случилось в моём потокобезопасном решении, потоки никогда не разблокируются. Пришлось делать так:    onCallRespond { call, body ->
        if ((call.response.status() ?: HttpStatusCode.OK) >= HttpStatusCode.BadRequest) {
            provider.badResponse()
        }
        else if (!call.attributes.contains(isResponseFromCacheKey)) {
            provider.setCache(buildKey(call.request, pluginConfig.queryKeys), body, pluginConfig.invalidateAt)
        }
    }    override suspend fun badResponse() {
        mutex.unlock()
    }Но мне такое решение не очень нравиться, так что буду признателен за любые советы, как улучшить concurrency.В заключениеБиблиотеку собираюсь развивать и расширять. Пока не уверен, какие еще провайдеры могут понадобиться, так что если у кого-то есть идеи на этот счет - буду рад. Так же думаю над тем, что сейчас можно обернуть в cacheOutput не только get запросы, и насколько это правильно. В целом, проверить метод и ругнуться warning'ом, вроде дело несложное.Спасибо, что дочитали!    
--------------------------
Ссылка: https://habr.com/ru/articles/780204/
Mixtral 8x7B – Sparse Mixture of Experts от Mistral AI
high_fly  (https://habr.com/ru/users/high_fly/)
 11 декабря 2023 года Mistral AI, парижский ai-стартап, выпустил новую модель Mixtral 8x7B – high-quality sparse mixture of experts model (SMoE). Многие считают модели Mistral AI самыми крутыми из открытых llm-ок, я тоже так считаю, поэтому интерес к новой модели есть большой. В этой статье я хочу коротко пробежаться по тому, как устроена новая модель и какие у её архитектуры преимущества. На некоторых технических моментах я буду останавливаться более подробно, на некоторых – менее.Архитектура mixture of expertsЧто такое  mixture of experts? Это такая техника, в которой некое множество “экспертов”,иными словами специализированных сетей, применяются для того, чтоб решать комплексную проблему по частям. Это значит, что у нас есть несколько разных сетей, каждая из которых хорошо умеет решать свою узкую задачу, и когда в модель приходит запрос, то на этот запрос отвечают не все эксперты, а только некоторое их множество, а то и один.Основные этапы создания mixture of experts моделиПоделить входящую в сеть задачу на подзадачи. При этом подзадачи могут пересекаться.Простой пример: на картинке есть объекты, есть распределения цветов, есть формы, есть фон, и информацию о фоне эффективно обработает одна экспертная сетка, а информацию об объектах – другая.Не во всех случаях деление комплексной задачи такое прямое. В более сложных случаях, можно, например, делить входные сэпмлы или токены на кластеры на основе расстояния в пространстве, и представлять каждый кластер, как свою задачу. Есть и другие способы.Обучить экспертную сеть для решения каждой подзадачи.Использовать gating (routing) модель, что решить, какого эксперта мы используем. Gating model принимает поданный в модель контекст, на основе него оценивает все экспертные предсказания и выбирает, какой эксперт / эксперты будет давать итоговый ответ пользователю.Объединение предсказаний экспертов. Можно выбрать одного эксперта, который даст ответ, а можно выбрать нескольких и совместить их ответы. Источник: https://machinelearningmastery.com/mixture-of-experts/Sparse Mixture of ExpertsНа картинке выше видно, что каждый эксперт получает input и каждый эксперт даёт предсказание. Это может быть довольно долго.Но ведь мы можем сразу отсекать "неподходящих" экспертов.  На картинке ниже изображён вариант, когда gating модель выбирает экспертов, к которым стоит обратиться, исходя из полученного контекста. Остальные эксперты не получат входных данных и будут тихонько стоять в стороне, то есть они не будут работать и что-то считать, и время ответа сократится.Источник: https://arxiv.org/abs/1701.06538Ребята из Mistral пишут, что для каждого входного токена роутер (построитель маршрутов к экспертам) выбирает двух экспертов, которые обработают этот токен; их аутпуты затем складываются. Стоит пояснить, что использовать не всех экспертов придумали изначально не они. Для наглядности картинка из блога гугла. В примере с этой картинки роутер выбирает для каждого токена топ-1 эксперта (оценка схожести токена и "домена" эксперта самая высокая) и направляет этот токен именно этому эксперту. Остальные не получают этот токен.Источник: https://blog.research.google/2022/11/mixture-of-experts-with-expert-choice.html?m=1Зачем это нужно ?Не секрет, что большие языковые модели используют transformer-based архитектуру. Если сильно упрощать, то большие языковые модели – это поставленные друг на друга блоки трансформеров. И в этом трансформере есть такой feed-forward слой – он голубой на картинке ниже. В этом слое, как говорят, хранятся выученные "знания" модели. Для больших моделей этот слой может содержать сумасшедшее количество нейронов.Источник: всем известная Attention is all you need от GoogleМы же дробим этот FNN слой на набор экспертов, и таким образом, мы можем использовать не все нейроны и не все знания этого слоя, а выбирать только те, которые нам нужны. Отсюда расшифровка слова Sparse в аббревиатуре SMoE – на смену обычным feed-forward слоям пришли разреженные слои. Разреженность, "разнесённость" знаний по набору экспертов, позволяет нам запускать только части сложной системы, и наши вычисления заметно ускоряются.Источник: Mixture of Experts Explained, HuggingFaceМинусы – мы всё равно должны хранить все веса в памяти, так как потенциально мы можем выбрать любого эксперта, однако, как я уже неоднократно упомянула, использовать мы будем не все веса. Это позволяет Mixtral 8*7B, которая по факту содержит 42B параметров, работать с той же скоростью, что и LLaMa 7B, которая, собственно, содержит только 7B параметров.Метрики MixtralMixtral сравнивалась с семейством Llama 2 и базовой моделью GPT-3.5. Mixtral оказалсь не хуже, а на некоторых бенчмарках даже победила ламу и гпт.Источник: https://mistral.ai/news/mixtral-of-experts/А ещё по сравнению с ламой Mixtral меньше галлюцинирует и более корректна в высказываниях.Сейчас Mixtral доступна на платформе Mistral AI через api в бета-версии, также выложена на huggingface. Перед запуском нужно иметь в виду, что, например, в 16ГБ такая модель, вероятно, не влезет. И в 32, думаю, тоже. А ещё mixtral можно выбрать в качестве отвечающей модели в чате perplexity, так что попробовать её можно прямо из браузера. Интересно, что в блогпосте Mistral AI среди языков Mixtral русский не упомянается, тем не менее, она на нём говорит.Скин из чата perplexityМожно ожидать, что в виду The AI Act, принятого в Евросоюзе пару дней назад, прогресс в выкладывании open-source llm-ок притормозится, но будем ждать и надеяться!Источники:Пост Mistral AI о выходе моделиПодробный пост с формулами Mixture of Experts Explained от HFДоступно написанная Mixture-of-Experts with Expert Choice Routing от GoogleОбщее объяснение концепции Mixture of experts    
--------------------------
Ссылка: https://habr.com/ru/articles/780200/
Эмуляция бэкенда: как разрабатывать изолированный фронтенд с помощью Mock Service Worker
AlexGriss  (https://habr.com/ru/users/AlexGriss/)
 Всем привет! Сегодня я хочу рассказать о Mock Service Worker — технологии, которая позволяет эмулировать поведение бэкенда в ситуациях, когда по каким-то причинам невозможно использовать реальный бэкенд для полноценной разработки фронтенда, а также когда необходимо изолированно протестировать различные пользовательские сценарии.Эта технология подойдёт в следующих случаях:если в вашей команде фронтенд должен разрабатываться параллельно или даже раньше, чем бэкенд на основе контрактов;если технически невозможно использовать реальный API бэкенда в условиях локальной разработки — такая ситуация возникла в моём рабочем проекте в банке, где бэкенд функционирует только в боевом контуре и доступен через виртуальную машину;если вы пишите end-to-end тесты, в которых проверяете работу критических пользовательских сценариев.Для всех выше перечисленных задач отлично подойдёт технология Mock Service Worker, которую я давно и успешно применяю в реально работающих приложениях. Тем более, что совсем недавно вышла новая мажорная версия соответствующей библиотеки msw, и в ней достаточно много важных обновлений.Что такое API Mocking и при чём тут Service WorkersMock — это модель, основанная на реальных данных, а API Mocking — это технология обработки таких моделируемых данных. Функцию API Mocking сервиса может выполнять отдельный сервер на Node.js, который запускается локально и обрабатывает запросы к API. Однако существует и другой способ обработки запросов, который не требует использования отдельного сервера, при этом отлично эмулирует сетевое поведение и предоставляет мощные инструменты для тестирования, изолированной разработки и дебага различных сетевых сценариев. Это технология Mock Service Worker (далее MSW).MSW предоставляет файл сервис-воркера, который подключается и регистрируется на этапе инициализации приложения, слушает события отправки XHR-запросов и перехватывает и обрабатывает их по заданным правилам для конкретных эндпоинтов. На продакшене Service Worker не будет активирован, и приложение автоматически переключится на работу с реальным API.Подробнее про Service Worker API можно почитать тут: https://developer.mozilla.org/ru/docs/Web/API/Service_Worker_API.MSW позволяет использовать привычные сервисы для отправки запросов к API (такие как встроенный fetch, библиотеку axios и др.) и запрашивать те же эндпоинты, которые работают с реальным бэкендом. Таким образом, не появляется необходимость поддерживать две конфигурации и два набора эндпоинтов для разработки и продакшена.Как начать работать с Mock Service WorkerДля того чтобы настроить Mock Service Worker понадобится соответствующая библиотека msw, информацию о которой можно найти на официальном сайте: https://mswjs.io.Установка и инициализация MSWЧтобы установить библиотеку msw, необходимо ввести следующую команду в терминале, находясь в папке проекта:npm install msw@latest --save-dev
Для работы с последней версией библиотеки понадобится версия Node.js не меньше 18.0.0. Если на проекте используется TypeScript, то его версию придётся поднять как минимум до версии 4.7.Далее необходимо инициализировать Mock Service Worker и создать необходимые файлы для его работы. Это можно сделать с помощью следующей команды:npx msw init ./public --save
Данная команда создаст файл с воркером mockServiceWorker.js в папке ./public, в которой хранятся публичные статические файлы проекта. Параметр --save позволит сохранить путь до папки в package.json проекта для будущих обновлений скрипта воркера.Создание обработчиков запросовСледующий шаг — это создание обработчиков запросов, которые будут перехватывать запросы к реальному бэкенду и эмулировать его работу.В ранних версиях msw синтаксис работы с запросами повторял синтаксис обработчиков маршрутов на сервере Node.js, например, в Express. Однако с выпуском версии 2.0 вся логика работы была адаптирована к нативному стандарту Fetch API. Это важное изменение, позволяющее полноценно эмулировать работу с запросами с использованием современного браузерного стандарта.Обработка запросов в MSW реализована двумя концепциями:request handler — это обработчик запросов, привязанный к конкретному URL, который перехватывает запрос и запускает функцию-резолвер;response resolver — это функция-резолвер, которая имеет доступ к параметрам запроса и возвращает ответ, эмулирующий ответ от реального бэкенда.Давайте рассмотрим эти концепции на практике, а заодно создадим обработчик GET-запроса на эндпоинт /api/posts.Для начала необходимо создать директорию src/mocks, в которой будет храниться всё, что относится к моковым данным. Далее создадим в ней файл handlers.js, в котором будут описаны все обработчики моковых запросов.Начнём с импорта модуля http, который позволяет перехватывать и обрабатывать REST-запросы:import { http } from 'msw';
Следом напишем заготовку под обработчик:// request handler
const postsHandler = http.get("/api/posts", postsResolver);
Модуль http позволяет обработать как вызов конкретного стандартного REST-метода (GET, POST, PATCH и др.), так и вызовы всех методов сразу на один и тот же URL — для этого предназначен метод модуля http.all(predicate, resolver).Подробнее про методы модуля http можно почитать в документации msw: https://mswjs.io/docs/api/http.Кроме REST-запросов, msw может также эмулировать запросы к GraphQL с помощью модуля graphql. Дополнительную информацию об этом можно найти в документации: https://mswjs.io/docs/network-behavior/graphql.Первый параметр в request handler называется predicate (далее предикат) и отражает правила, по которым проверяется URL запроса. Предикат может быть как обычной строкой, так и регулярным выражением. Запрос будет обработан функцией-резолвером, переданной в параметре resolver, только если URL запроса совпадёт с правилом, заданным в предикате. В нашем случае предикатом является строка с относительным URL api/posts. Теперь при вызове любого GET-запроса на данный URL из клиентского приложения с работающим Mock Service Worker запустится обработчик вызова postsHandler.Подробнее о правилах формирования предиката можно почитать в документации: https://mswjs.io/docs/basics/intercepting-requests#http-request-matching, а пока что пойдём дальше и напишем код функции-резолвера:// response resolver
const postsResolver = ({ request, params, cookies }) => {
  return HttpResponse.json([
    {
      title:
        "Что такое генераторы статических сайтов и почему Astro — лучший фреймворк для разработки лендингов",
      url: "https://habr.com/ru/articles/779428/",
      author: "@AlexGriss",
    },
    {
      title: "Как использовать html-элемент <dialog>?",
      url: "https://habr.com/ru/articles/778542/",
      author: "@AlexGriss",
    },
  ]);
};
Response resolver предоставляет доступ к единственному аргументу в виде объекта, в котором содержится информация о перехваченном запросе.В поле request будет доступна реализация нативного интерфейса Request из Fetch API, так что вы сможете использовать все стандартные методы и свойства соответствующего класса. В поле params будут доступны параметры пути, такие как, к примеру, postId для URL вида api/posts/:postId. Наконец, поле cookies содержит все установленные при запросе куки в виде строковых пар ключ-значение.Функция-резолвер должна возвращать инструкцию о том, что нужно сделать при перехвате запроса. Чаще всего это будет ответ в виде mock-данных, и для этого в msw используется инстанс нативного класса Response из всё того же Fetch API. Вы можете использовать класс Response напрямую, но разработчики msw рекомендуют работать с библиотечным классом HttpResponse, который является более продвинутой надстройкой над Response. Например, он позволяет мокать установку cookies и дополняет стандартные методы класса Response удобными методами для отправки ответа с различными Content-Type.О классе HttpResponse можно почитать подробнее в документации библиотеки msw: https://mswjs.io/docs/api/http-response.Кроме обычных текстовых данных в виде plain text, json, xml или formData, библиотека msw позволяет возвращать стримы ReadableStream. Подробнее об этом можно почитать в соответствующем разделе документации: https://mswjs.io/docs/recipes/streamingВ приведённом примере вызывается метод HttpResponse.json(), в который передаётся моковая структура данных — в данном случае это коллекция постов на Хабре. Далее она будет отправлена на клиент при успешной обработке вызова.Давайте посмотрим на то, как должен выглядеть итоговый файл handlers.js:// src/mocks/handlers.js

import { HttpResponse, http } from "msw";

// response resolver
const postsResolver = () => {
  return HttpResponse.json([
    {
      title:
        "Что такое генераторы статических сайтов и почему Astro — лучший фреймворк для разработки лендингов",
      url: "https://habr.com/ru/articles/779428/",
      author: "@AlexGriss",
    },
    {
      title: "Как использовать html-элемент <dialog>?",
      url: "https://habr.com/ru/articles/778542/",
      author: "@AlexGriss",
    },
  ]);
};

// request resolver
const postsHandler = http.get("/api/posts", postsResolver);

export const handlers = [postsHandler];
В конце необходимо экспортировать переменную handlers, содержащую массив со всеми обработчиками вызовов.Настройка Mock Service Worker для работы в браузереДалее в папке src/mocks создадим файл browser.js, в котором нужно будет настроить работу воркера с ранее добавленными обработчиками вызовов:// src/mocks/browser.js
import { setupWorker } from "msw/browser";

import { handlers } from "./handlers";

export const worker = setupWorker(...handlers);
Функция setupWorker принимает список обработчиков и подготавливает канал связи между клиентом и воркером mockServiceWorker.js, который мы сгенерировали на первом этапе.Далее мы готовы активировать работу MSW через вызов метода воркера worker.start(). Ниже показан пример активации воркера во входной точке React-приложения:// src/index.jsx
import React from 'react';
import ReactDOM from 'react-dom';

import { App } from './App';

async function enableMocking() {
  if (process.env.NODE_ENV === "development") {
    const { worker } = await import("./mocks/browser");

    return worker.start();
  }
}

const rootElement = ReactDOM.createRoot(document.getElementById("root"));

enableMocking().then(() => {
  rootElement.render(<App />);
});
Активация MSW должна происходить только в режиме разработки. Результатом выполнения метода воркера worker.start() будет являться промис, при резолве которого необходимо рендерить клиентское приложение. Такая последовательность задач необходима для того, чтобы избежать гонки состояний между событием регистрации воркера и запросами, которое делает приложение на старте работы.Для приложения на ванильном JS достаточно просто запустить метод worker.start() на этапе инициализации приложения.Если всё сделано правильно, то при запуске приложения в консоли браузера можно будет увидеть сообщение:[MSW] Mocking enabled.
MSW умеет работать не только в браузерной среде, но и в связке с Node.js и React Native. Настроить моки для данных окружений помогут соответствующие разделы в документации библиотеки: https://mswjs.io/docs/integrations/node и https://mswjs.io/docs/integrations/react-native.Проверка обработки запросовТеперь если в клиентском приложении сделать запрос на URL /api/posts, Mock Service Worker должен его перехватить и вернуть mock-данные.Давайте рассмотрим это на примере компонента Posts в рамках React-приложения:// src/components/Posts.jsx
import { useEffect, useState } from "react";

import { Post } from "./Post";

export const Posts = () => {
  const [posts, setPosts] = useState(null);
  const [isLoading, setIsLoading] = useState(true);

  useEffect(() => {
    fetch(`/api/posts`)
      .then((response) => response.json())
      .then((posts) => {
        setPosts(posts);
      })
      .finally(() => {
        setIsLoading(false);
      });
  }, []);

  return isLoading
    ? "Loading..."
    : posts.map((post) => (
        <Post key={post.title} post={post} />
      ));
};
Теперь при загрузке React-компонента на вкладке Network в Developer Tools браузера отобразится XHR-запрос на URL api/posts с ответом в виде mock-данных:Запрос на /api/postsMSW будет перехватывать любой запрос, если его URL соответствует предикату пути в обработчике запроса. Кроме отображения во вкладке Network каждый перехваченный запрос выведет дополнительную информацию в консоль браузера.В заключениеТехнология Mock Service Worker является не просто средой для работы с mock-данными, она позволяет буквально эмулировать сетевое поведение без развёртывания отдельного сервера. MSW предоставляет широкие возможности для повторения логики бэкенда на клиенте, тестирования сложных пользовательских сценариев и полноценной разработки фронтенда в условиях, когда бэкенд не готов. Точная эмуляция сетевого поведения позволяет автоматически переключаться на работу с реальными данными и реальными API, как только приложение запускается в продакшене.MSW умеет эмулировать работу как с обычными REST-запросами, так и с GraphQL и даже возвращать стримы в качестве ответа на запрос. Библиотека msw позволяет настроить интеграцию сервис-воркера с различными средами, такими как браузер, Node.js и React Native. Эмуляция реального сетевого поведения стала ещё более доступной в новой версии MSW с введением интерфейсов обработчиков запросов, которые повторяют реализацию нативных интерфейсов Fetch API, таких как Request и Response.Мы изучили основы работы, подключения и настройки технологии Mock Service Worker, но на этом её возможности не заканчиваются. Предлагаю попробовать поэкспериментировать с MSW самостоятельно, повторив руководство, представленное в этой статье. Продолжить изучение технологии можно на сайте с документацией к библиотеке msw: https://mswjs.io/docs/getting-started.Приглашаю вас подписаться на мой телеграм-канал: https://t.me/alexgriss, в котором я пишу о фронтенд-разработке, публикую полезные материалы, делюсь своим профессиональным мнением и рассматриваю темы, важные для карьеры разработчика.      Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста. Какие технологии для мокинга API вы использовали? 
            66.67%
           Mock Service Worker 
            2
           
            0%
           Nock 
            0
           
            0%
           JSON Server 
            0
           
            0%
           Mirage 
            0
           
            0%
           Cypress 
            0
           
            0%
           Playwright 
            0
           
            0%
           Другое чужое решение (напишу в комментариях) 
            0
           
            0%
           Самописное решение 
            0
           
            0%
           Не использовал(а) никакие технологии для мокинга API 
            0
           
            33.33%
           Хочу просто посмотреть ответы 
            1
            
       Проголосовали 3 пользователя.  

       Воздержались 2 пользователя. 
    
--------------------------
Ссылка: https://habr.com/ru/articles/780198/
Деградация современного ПО
ShapitoS999  (https://habr.com/ru/users/ShapitoS999/)
 В погоне за коммерческой выгодой современные отделы тестирования и обеспечения качества IT-компаний стали забывать о реальном положении вещей в индустрии.Жадность и желание получить максимальную прибыль за разработанный продукт стали максимальным приоритетом для владельцев, в ущерб качеству.В данной статье я буду приводить примеры кейсов, на которые стоит обратить внимание при реализации жизненного цикла ПО с учетом QA.Начнем с простейшего вопроса - это ресурсы.Каждый человек в современном мире, живущий в мегаполисе, обладает такими девайсами как ПК/Ноутбук и мобильный телефон. Как правило (чаще всего, нужное подчеркнуть) ПК/Ноут в большинстве случаев на OS Windows, а мобильный телефон - это смартфон на OS Android.И вот здесь я сейчас буду конкретизировать совершенно стандартные жизненные ситуации при использовании программного обеспечения, которые позволят читателям понять действительную деградацию современного обеспечения качества.Телефон и его стоимостьНи для кого не секрет, что мобас на андроиде в широких пределах стоит от 100 до ~бесконечной суммы в долларах.Всё зависит от стоимости камеры + начинка по железу по оперативной памяти, CPU и размера жесткого диска для хранения фоток и видосов.Большинство приложений для андроида пишутся на языках семейства Java - сама джава, kotlin  ну и вот это вот всё.А теперь я расскажу реальный пример из жизни.Один раз несколько лет назад я разбил свой телефон на андроиде и решил купить новый. Денег на новый тогда было достаточно мало, поэтому я купил новый за $150.По хардвэйр компонентам - это 2 ядра по 1.2Hz на 4 потока, 3GB оперативки и 32 GB диск для фоток не с самой мегапиксленой камерой.И всем наверняка знаком кейс - телефон начинает тормозить через полгода/год эксплуатации.И все знают 2 способа решения этого вопроса - купить новый или гуглить. Я выбрал второй - и моментально нашел, что есть такой пункт меню в настройках "Обслуживание устройства", в котором есть волшебная кнопка с очисткой памяти.И вот на этом абзаце начинается самая интересная часть этой статьи, а именно деградация системы QA.При нажатии на кнопку пункта меню я вижу, что у меня общее кол-во ОЗУ 3GB, зарезервировано 300MB, что-то на OS и предлагается закрыть фоновые приложения, чтобы очистить память.И конечно я жму - и что получаю как неожидаемый результат - памяти стало меньше и мобас начинает сильнее глючить.Повторяю процедуру  - и вижу, что вычисленное значение свободной ОЗУ считается неправильно, неверное округление.и вот тут я задумался.Тезисы:Разработчики на Java и этом семействе языков в компании Гугл совершенно не понимают про типы данных и их преобразованияБизнесу совершенно неинтересны кейсы про "тормоза" на устройствахQA совершенно забыли про негативные кейсы, тестируют только позитивныеВ современном мире очень удобно стало скейлить сервисы на kubernetes, покупать новые девайсы вместо старых тормозных и ссылаться на техническую сложность при разработке, писать многопоточку и абстрагироваться от хорошего кода с помощью SOLID, DRY итд.А по факту даже в самых крупных и крутых IT-гигантах уровень разработки и тестирвания просто упал.А теперь скрины - нет, это не с телефона, это просто косяки самого хабра Мне за современное IT просто становится стыдно.    
--------------------------
Ссылка: https://habr.com/ru/companies/communicanomics/articles/780192/
Накатим по авиационному. Фреймворк «Ситуационная осведомленность»
vshumovsky  (https://habr.com/ru/users/vshumovsky/)
 Условия, в которых принимаются решения,для команды сложного проекта и экипажа воздушного судна почти идентичны.С 2017 года я помогаю технологическим компаниям создавать лучшие практики управления эффективностью коммуникаций, оптимизировать процессы принятия решений в командах и повышать качество отношений с заинтересованными сторонами. До 2020 года для меня это был своего рода «пет-проект», параллельный основной работе – управлению портфелем проектов инновационного развития Авиакомпании «Россия». Основная часть задач, само собой, относилась к технологиям, но мне всегда были интересны управление продуктами и процессами. Поэтому я никогда не упускал возможности получить новые знания и практический опыт именно в этом. Когда ситуация на рынке авиаперевозок заставила компанию сменить фокус, я перешел с этими знаниями и опытом в частную практику.Поскольку за время работы в авиации часто приходилось глубоко погружаться в специфические методики и технологии организации совместной работы, а за плечами уже был большой опыт управления в коммерческом секторе – быстро выяснил, что довольно много хорошо проработанных и проверенных жизнью авиационных фреймворков (если их слегка адаптировать) могут быть весьма полезными для «неавиационного» менеджмента. Особенно при разработке и внедрении IT-продуктов. Тем более, что в зарубежных источниках, ориентированных на технологических предпринимателей, можно обнаружить довольно много различных отсылок к самой распространенной в авиации технологии управления коммуникациями и командной работой – Crew Recourse Management (CRM, управление ресурсами экипажа).В результате появилась «Коммуниканомика» – набор практик, объединяющий наработки из авиации с методологиями управления продуктовыми и проектными командами. К примеру, тренинг «Экипаж», в основе которого – переработанные российские методики на основе CRM.В своем дебюте на Хабре я хочу поделиться описанием проверенного практикой фреймворка, который позволяет более эффективно управлять вниманием руководителя проекта или лидера продукта в ситуациях, где максимально высока цена ошибок и несогласованности. Этот фреймворк называется «Ситуационная осведомленность». Короткая предыстория.Понятие ситуационной осведомленности появилось не в авиации, но примерно одновременно с ней – во времена Первой мировой. В отдельный аспект теории и практики принятия решений его выделили специалисты американских ВВС, анализируя опыт воздушных боев в Корее и Вьетнаме. С началом эпохи массового применения сверхзвуковой авиации влияние скорости принятия верных решений кратно выросло.Максимальная скорость основного американского истребителей в период междуконцом II Мировой и началом Корейской войны выросла в 1,8 раза  В гражданской авиации методология закрепилась в 80-х годах прошлого века, когда международные авиаперевозки сильно повысили плотность загрузки воздушного пространства.Сегодня большинство распространенных определений этого понятия основаны на словосочетании «понимание ситуации», но для нас больше подойдет формулировка из одной научной статьи: «Ситуационная осведомленность — это ментальная модель состояния окружающей среды, имеющаяся у людей, принимающих решение». Она хороша тем, что связывает принятие решений не с абстрактным «пониманием», а с картиной реальности, которая в определенный момент времени складывается в голове у тех, от кого эти решения ожидаются. К тому же позволяет несколько отдалить себя от «аналитического паралича» - для субъективного «понимания» можно до бесконечности вытаскивать информацию из внешнего мира, ничего не предпринимая. Тогда как архитектурой и параметрами ментальной модели можно оперировать достаточно гибко. Но это семантический нюанс из категории «я автор, я так вижу» и фокусировать на нем внимание не предлагаю.Внимание стоит сфокусировать на том, что для регулярных задач фреймворк ситуационной осведомленности, чаще всего, избыточен (если вы не control freak). Однако очень полезен как основа подготовки к ситуациям экстремальным. Например, к тестированию сложной информационной системы на серверах заказчика. Или на первых этапах внедрения изменений.Следуя изложенному в этой статье «мануалу» можно более качественно подготовиться к работе в экстремальном режиме, в том числе согласовать приоритетность источников информации и распределить ответственность за мониторинг и реагирование.  В прикладной конфигурации ситуационная осведомленность образуется тремя составляющими: System Awareness, Environmental Awareness и Anticipation. System Awareness. Осведомленность о состоянии (статусе) систем.Для пилотов чуть ли не каждое значение любого показателя на приборах описано в инструкциях по принципу «если … то делай раз, делай два». Для большинства IT-процессов это, пожалуй, не обязательно, однако будет очень полезно обсудить заблаговременно в команде: кто отслеживает какой поток информации, с какой периодичностью и главное – с какой целью. Чтобы не получилось в самый ответственный момент как в старом анекдоте – «Петрович, приборы? – Восемь! – Что «восемь»!? – А что «приборы»?». Еще полезно в принципе определить какие данные необходимы, а какие можно временно проигнорировать. В типичной ситуации постоянно следить за температурой в офисе может, особой необходимости и нет, но, если в ключевом процессе участвует человек с гипертонией, который должен быть за монитором/клавиатурой постоянно – разумно будет кого-то из второстепенных (в данный момент) сотрудников этим озадачить. Когда я провожу командные сессии – у меня на ноутбуке, как правило, висит стикер с надписью «Т-О2». Каждый раз, переключая слайд сценария, я натыкаюсь на него глазами. Зачем? Я модератор увлекающийся. Работаю, как правило, со сложными задачами, в которых важна максимальная вовлеченность группы. Без дополнительного напоминания легко забыть, что люди в помещении теплокровные и дышат, а когда жарко и душно – качество совместной работы резко снижается и потом долго не восстанавливается.  Как один из элементов управления вниманием можно использовать список из пяти ключевых элементов ситуации – его я тоже взял из авиационных гайдов, упростив и чуть докрутив для адекватного применения: полет, пилот, летательный аппарат, окружающая среда и тип операции. Первое – про инструкции, регламенты, описание процедур и прочие известные заранее нормы, правила и установки. Второе – индивидуальные особенности ключевых участников, их типичные реакции в различных ситуациях, уровень знаний и индивидуальные шаблоны принятия решений. Третье – технические параметры и возможности задействованных программных и аппаратных средств. Четвертое – внешние условия, в которых принимаются решения и пятое – насколько то, что будет делать команда привычно или непривычно, просто (с учетом имеющихся компетенций) или сложно, завязано только на команду или будут задействованы другие люди и т.п.На этапе моделирования команде будет полезно согласовать порядок действий при определенных значениях и/или динамике показателей. В каком диапазоне каждый из участников принимает решение сам; в каком – согласовывает свои действия с лидером, а в каком – сообщает о ситуации «наверх» и складывает ручки на коленках до особых распоряжений. Последний вариант не совсем шутка, поскольку множество проблемных ситуаций разворачивалось до критических масштабов как раз из-за того, что человек просто продолжал делать то же самое, что и прежде, но в изменившейся ситуации. Именно это и становилось причиной распространения каскада усиливающих друг друга ошибок. Для отдельных ситуаций можно (а иногда и нужно) вводить «голосовую сигнализацию». Почти все действия КВС (командира воздушного судна, в авиации официально нет понятия «первый пилот») и второго пилота дублируются голосом – сначала объявляешь «делаю то-то», а только потом совершаешь действие. Кто-то сейчас ухмыльнулся, представив, но я и мои клиенты на своем опыте прочувствовали, насколько сокращает издержки коммуникации эта простая процедура. Сначала делаешь звонок коллеге из технического отела – «отправляю начальнику департамента обновленный расчет по проекту», и только потом нажимаешь кнопку «Отправить». Коллега готов к тому, что через час-полтора шеф позвонит ему с вопросами и уточнениями, а он успеет посмотреть историю нашей с ним переписки и, скорее всего, уже не окажется в момент звонка где-нибудь в ангаре в активном диалоге с техниками. В итоге следующий шаг в реализации проекта произойдет сразу, а не через день-два-неделю.Environmental Awareness / Осведомленность о текущих условиях Этот параметр включает информацию об окружающей среде в широком смысле. Что происходит в реальности и как осуществляется взаимодействие с внешними ресурсами. Для пилота, к примеру, сюда входят диспетчер, состояние облачности на эшелоне, положение находящихся поблизости других воздушных судов и т.д. Очевидно, что команда проекта также может заблаговременно определить круг ключевых участников и аспекты ситуации, которые будут влиять на ход процесса. Чтобы не отвлекаться на то, что не влияет.Как и в первом разделе – при оценке текущей ситуации важно не забывать делится информацией с другими. Какой и зачем – вопрос, не имеющий универсального ответа, но элементарно проясняющийся процессе моделирования.Anticipation / Ожидание (прогнозирование)Поскольку основная идея этой статьи – управление вниманием, я намеренно не стану углубляться в тонкости процесса принятия решений как такового, хотя в авиации для этого также есть специализированные и весьма полезные фреймворки под общим названием Aeronautical Decision Making. Под прогнозированием понимается проекция будущего состояния, то есть возможность предсказать, что произойдет дальше, на основе данных, полученных на предыдущих шагах. Здесь лидеру команды и основным экспертам необходимо понимать, что в определении из первого абзаца под «оператором» можно понимать как одного человека (если для ситуации установлен авторитарный порядок управления) так и группу. Например, создать штаб на время особых обстоятельств. Моделирование позволяет более гибко конструировать сочетание ключевых процессов, определяющих эффективность прогнозирования: применение системных знаний для выявления тенденций; экстраполяция имеющихся данных на перспективу; выявление возможных /будущих проблем и проработка стратегий на случай непредвиденных обстоятельств.Очевидно, что лидер, скорее всего, не будет обладать всем объемом узких системных знаний, необходимых для прогнозирования всех возможных ситуаций. Из чего возникает коммуникационная задача – построить взаимодействие команды на особый период так, чтобы в нужный момент можно было быстро задать вопрос тому, кто этими знаниями обладает и максимально быстро получить ответ. Крайний и неэффективный (как минимум экономически) вариант – собрать узко специализированных экспертов в одном месте где-то поблизости и пусть сидят «на всякий случай». Значительно эффективнее заранее провести «штабную игру», на которой зафиксировать в каком диапазоне «нормальности» должна развиваться ситуация в штатном режиме, каковы наиболее вероятные варианты ее нештатного развития и что в этом случае следует делать, чтобы минимизировать ущерб. Непосредственно в рабочем процессе возможность в необходимый момент подключить эксперта значительно повышается благодаря уже упомянутой «голосовой сигнализации». Например, можно договориться, что перед запуском какой-то из процедур сначала поступает сообщение в чат команды «готовы запускать …», но сам запуск не производится, пока эксперт не проведет «проверку готовности»: «Такие-то параметры проверили? Такие-то действия уже выполнили? Ок, запускайте». Дальше эксперт по предварительной договоренности может находится в режиме «готовности номер ноль» до момента, пока в тот же чат не поступит сообщение о том, что все нормально запустилось.    Необходимо понимать, что мониторинг состояния системы, оценка обстановки, прогнозирование и принятие решений — все это процессы, оказывающие взаимное влияние в непрерывном цикле, который может быть разорван посторонними факторами. Поэтому моделирование по алгоритму ситуационной осведомленности не имеет смысла применять формально, «для галочки». Только в случае, если действительно есть намерение сконструировать ситуацию, в которой:все, кто влияет на ситуацию, в курсе того, что происходит;каждый понимает какую информацию и в каком порядке получает и что эта информация означает;можно достоверно предположить, что эта информация будет значить в будущем.Поэтому в практике стоит учитывать факторы, которые снижают ценность осведомленности:1. Туннельное внимание. Фиксация на одном ограниченном наборе информации в ущерб другим данным приводит к ошибкам в оценке контекста.2. Завышенные ожидания относительно рабочей памяти членов команды. Надежда, что в момент принятия решения всю необходимую информацию можно будет удержать «в голове», никогда не оправдывается.3. Тревожность, усталость и другие психоэмоциональные факторы, снижающие возможность человека адекватно обрабатывать информацию. Здесь понятно, но многие это обстоятельство игнорируют. 4. Перегрузка информацией. Слишком большое количество информации уменьшает осведомленность. Также стоит учитывать «считываемость» данных.5. Попытки оперировать желательным, а не объективным состоянием системы. На выходе - ложная трактовка событий и неверные прогнозы.Необходимо подчеркнуть, что на стороне лидера определяющим фактором осведомленности служит умение учитывать не только свое видение ситуации. Это подразумевает умение и, что важнее, намерение быстро собирать варианты, оценивать их и выбирать наиболее правильный, даже если это не твой. Дополнение. Принцип «чистой кабины».В завершение поделюсь еще одной полезностью. Это уже не часть фреймворка, а простая, но эффективная техника. Самые напряженные этапы полета, требующие от летного экипажа максимальной концентрации, это (само собой) взлет и посадка. На этих этапах действует принцип «чистой кабины» - никаких отвлекающих факторов. IT-команда может применять этот принцип довольно творчески. Например, в одном из кейсов по моему совету со всеми представителями заказчика, кроме трех действительно нужных (а их в проект было вовлечено что-то около полутора десятков) заранее договорились о том, что на время особой ситуации команда отправляет их телефоны в «черный список». На связи для них остается только один из ЗГД, не участвующий в процессе непосредственно. Раз в час операционный лидер получал от него информацию о настроениях стейкхолдеров и отгружал им актуальный статус. При этом заранее было согласовано какие сообщения будут передаваться сразу, в обход «фильтра». Остальные «парковались» до очередного «сеанса связи».Спасибо, что дочитали. Для меня эта статья своего рода эксперимент. Если он окажется удачным – буду и дальше делиться подобными наработками, поскольку «Коммуниканомика» это объемный пакет нестандартных решений для разработки адаптивных бизнес-стратегий, развития команд и лидеров изменений, а также повышения экономической эффективности управленческих коммуникаций в целом.Мы умеем:делать бизнес‑анализ систем принятия решений по адаптивному набору метрик;разрабатывать и проводить стратегические сессии в техниках сценарного моделирования и группового исследования;проводить командные сессии и тренинги, в том числе с включением методологий подготовки авиационных экипажей;поддерживать индивидуальное развитие менеджеров уровней B/C на основе профиля коммуникативно компетентного лидера;создавать и развивать высокоэффективные команды по принципу ролей, основанных на сильных сторонах.В декабре 2023 года на платформе Литрес опубликована моя книга «Коммуниканомика. Рецепты национальной кухни управления изменениями». Читатели Хабр могут получить электронную версию до (и после) официальной публикации просто так, под добровольную договоренность о развернутой обратной связи.Для этого напишите мне в Телеграм - @vshumovsky.    
--------------------------
Ссылка: https://habr.com/ru/articles/780184/
Зыбучие пески или установка песочницы CAPE
ap_security  (https://habr.com/ru/users/ap_security/)
 Привет, Хабр, на связи лаборатория кибербезопасности AP Security. В данной статье предлагаем разобрать процесс установки и первоначальной настройки такой песочницы, как CAPE.Зачем взрослым песочница?В такой сфере, как компьютерная безопасность песочница (Sandbox) - это виртуальная среда, в которой вредоносное ПО может безопасно запускаться и анализироваться, не причиняя вреда хост-системе. Иными словами песочница запускает файл в изолированной виртуальной среде, анализирует действия, которые он совершает в системе, и выдает вердикт о том, безопасен этот файл или нет.  Если говорить совсем просто, то наша цель — запустить неизвестное и ненадежное приложение или файл в изолированной среде и получите информация о том, что он делает.Песочница для анализа вредоносных программ — это практическое применение динамического анализа: вместо статического анализа двоичного файла он выполняется и отслеживается в режиме реального времени. Очевидно, что у данного подхода есть плюсы и минусы, но это ценный метод для получения дополнительной информации о вредоносном ПО, например о его поведении в сети. Поэтому рекомендуется выполнять как статический, так и динамический анализ подозрительных файлов, чтобы получить более глубокое представление о них.Прежде чем приступить к установке, настройке и использованию песочницы,следует подумать о том, как и чего вы хотите добиться.Некоторые вопросы, которые стоит задать себе:Какие файлы я хочу проанализировать?Какую платформу я хочу использовать для проведения анализа?Какую информацию я хочу получить о файле?Создание изолированной среды (виртуальной машины) и её настройка, вероятно, является самой важной частью развертывания песочницы: это следует делать аккуратно и с планированием каждого этапа установки.После выбора продукта виртуализации, следует построить план проектирования, который определяет:Какую операционную систему, язык и уровень исправлений использовать.Какое программное обеспечение устанавливать и какие версии (особенно важно при анализе эксплойтов).Учтите, что автоматический анализ вредоносного ПО не является детерминированным, и его успех может зависеть от миллиона факторов: вы пытаетесь запустить вредоносное ПО в виртуализированной системе так же, как и в собственной, что бывает очень сложно. Вашей целью должно быть создание системы, способной удовлетворить все требования, необходимые вам, а также постарайтесь сделать систему для анализа как можно более реалистичной.Закончив проектирование и подготовку прототипа желаемой системы, можете приступать к созданию и развертыванию песочницы и виртуальной среды. Вы всегда успеете изменить что-то или немного исправить, но помните, что хорошее планирование всегда влечет меньше проблем в долгосрочной перспективе.КАПЕ? Может быть МЫС? А нет, скорее всего ПЛАЩ!CAPE (Malware Configuration And Payload Extraction) — это автоматизированная система анализа вредоносного ПО с открытым исходным кодом.Песочница используется для автоматического запуска и анализа файлов, а также для сбора полной информации. Результаты анализа показывают, что делает вредоносное ПО во время работы внутри изолированной операционной системы (в основном ОС Windows).CAPE может получить следующие типы результатов:Следы вызовов Win32 API, которые выполнялись всеми процессами, порожденными вредоносным ПО.Файлы, которые были созданы, удалены и загружены вредоносной программой во время ее выполнения.Дампы памяти процессов вредоносного ПО.Трассировка сетевого трафика в формате PCAP.Снимки экрана рабочего стола Windows, сделанные во время работы вредоносной программы.Полные дампы памяти виртуальных машин.CAPE является "выходцем" из одной достаточно популярной песочницы Cuckoo Sandbox и  и предназначен для использования как в качестве автономного приложения, так и в качестве интегрированного решения в более крупные структуры благодаря своей модульной конструкции.Что можно анализировать:Общие исполняемые файлы WindowsDLL-файлыPDF-документыДокументы Microsoft OfficeURL-адреса и HTML-файлыPHP-скриптыCPL-файлыСценарии Visual Basic (VB)ZIP-файлыJava-JAR-файл - Файлы PythonПочти все остальноеCAPE обладает мощными возможностями, которые благодаря модульности архитектуры позволяет создавать неограниченное количество различных сценариев.На чём стоит мысCAPE Sandbox состоит из программного обеспечения для централизованного управления, которое обрабатывает образцы, выполнение и анализ.Каждый анализ запускается на новой изолированной виртуальной машине, либо из состояния снимка одной ВМ (snapshot). Инфраструктура CAPE состоит из хост-машины (управляющей машины) и ряда гостевых машин (виртуальных машин для анализа). Хост запускает основной компонент песочницы, который управляет всем процессом анализа, в то время как "гости" представляют собой изолированную среду, где образцы вредоносного ПО безопасно выполняются и анализируются. На следующем рисунке объясняется основная архитектура CAPE:Подготовка и настройка хостаВ данной статье конфигурация песочницы будет представлять собой совокупность Ubuntu 22.04 LTS в качестве хост-системы и виртуальной машины на операционной системе Windows 7 Pro. Такая сборка предлагается и разработчиками CAPE, поскольку позволяет максимально задействовать возможности песочницы и обладает хорошей устойчивостью к обнаружению вредоносным ПО. Не запрещается использовать и другие решения, экспериментировать с конфигурацией хоста и гостя.Скачать CAPE на хост можно из официального репозитория git, где находится стабильная и упакованная версия продукта.Рекомендуется устанавливать гипервизор KVM, хотя можно использовать и любой другой. Для его скачивания в директории CAPE находится скрипт установки всего необходимого kvm-qemu.sh.Перед выполнением сценария вам следует заменить вхождения <WOOT> в самом сценарии реальными шаблонами оборудования. Вы можете использовать acpidump в Linux и acpiextract в Windows для получения таких шаблонов, как указано в самом скрипте.Для создания простой лабораторной среды достаточно заменить <WOOT> в коде скрипта на 4 любых символа.Делаем файл скрипта исполняемым:cmod a+x kvm-qemu.shИ устанавливаем сам KVM. выполняя следующую команду:sudo ./kvm-qemu.sh all <username> | tee kvm-qemu.log
заменяя <username> своим фактическим именем пользователя.После установки обязательно необходимо перезагрузиться.Если необходимо установить Virtual Machine Manager (virt-manager), выполните следующую команду:sudo ./kvm-qemu.sh virtmanager <username> | tee kvm-qemu-virt-manager.logзаменяя <username> своим фактическим именем пользователя.После установки обязательно необходимо перезагрузиться.Далее для установки самого CAPE необходимо сделать исполняемым скрипт cape2.sh:chmod a+x cape2.shЧтобы установить CAPE со всеми оптимизациями, используйте одну из следующих команд:sudo ./cape2.sh base cape | tee cape.log
sudo ./cape2.sh all cape | tee cape.logНе забудьте перезагрузиться после установки.Рекомендуется использовать Poetry (это инструмент для управления проектами на Python) для установки зависимостей. Поскольку все службы настроены на использование poetry и лучше справляются с конфликтами зависимостей. Для установки традиционным способом:pip3 install -r requirements.txtЧтобы установить зависимости с помощью poetry, выполните следующую команду (из основного рабочего каталога CAPE, обычно /opt/CAPEv2/):poetry installПосле завершения установки вы можете подтвердить создание виртуальной среды с помощью:poetry env listВывод должен быть похож на:poetry env list
capev2-t2x27zRb-py3.10 (Activated)С этого момента вам придется запускать CAPE в виртуальной среде Poetry. Для этого вам нужно всего лишь написать команду poetry run <command>. Например:sudo -u cape poetry run python3 cuckoo.pyДанная команда, в будущем, будет основной командой запуска CAPE.ВНИМАНИЕ! Только установочные скрипты и некоторые утилиты типа rooter.pyдолжен быть выполнены с sudo, остальные конфигурационные файлы и программы ДОЛЖНЫ выполняться под пользователем cape, который создается в системе после выполнения cape2.sh.По умолчанию у пользователя cape нет логина. Чтобы использовать терминал от его имени, вы можете выполнить следующую команду:sudo su - cape -c /bin/bashПодготовка гостяНа этом этапе хост является практически настроенным, в дальнейшем мы вернемся к его конфигурации. Сейчас пришло время создать виртуальную машину и настроить её.Запускаем Менеджер виртуальных машин, который устанавливался ранее и создаем новую ВМ. Как упоминалось выше в качестве ОС будет использоваться рекомендованная Windows 7 Pro.При создании виртуальной машины и настройке её параметров CAPE не требует каких-либо особых требований к конфигурация. Выбирайте варианты, которые лучше всего соответствуют вашим потребностям. В любом случае, при необходимости, каждый параметр в будущем(разве что кроме размера диска) можно будет изменить.Для правильной работы CAPE в виртуальной машине следует установить необходимое программное обеспечение и библиотеки.Строгим требованием является установка Python, в настоящее время для Windows поддерживаются только 32-разрядные (x86) версии Python3 из-за особенностей взаимодействия анализатора с низкоуровневыми библиотеками. Для других ОС версия Python может быть 64-битной (x64).Скачать установщик Windows / Linux можно с официального сайта . Предпочтительная версия Python > 3.6.При установке Python рекомендуется выбрать параметр "Добавить Python<версии> в PATH"После завершения установки следует проверить, правильно ли указан Python в  переменной среды PATH с помощью ввода в командной строке команды : python --versionСледующие библиотеки Python не являются строго обязательными для CAPE, но рекомендуется установить их, если вы хотите иметь доступ ко всем доступным функциям.python -m pip install --upgrade pip
python -m pip install Pillow==9.5.0ВАЖНО!!! Необходимо отключить функцию "Автообновления" или "Проверку обновлений". Также стоит установить дополнительное программное обеспечение, такое как браузеры, офисные пакеты. Не рекомендуется использовать версии Office более поздние, чем 2016, из-за отсутствия доказанной совместимости как с Maldocs, так и с CAPE.Пришло время настроить некоторые параметры сети виртуальной машины. Двумя наиболее важными настройками являются отключение брандмауэра Windows и Автоматических обновлений . Причина этого в том, что эти особенности могут повлиять на поведение вредоносных программ в обычных условиях и могут загрязнять сетевой анализ, выполняемый CAPE, путем удаления соединений или включения ненужных запросов.Следующий шаг - отключение автоматических обновлений. Для этого в Панели управления открываем Администрирование (Administrative Tools), затем открываем Службы (Services). Ищем Центр обновления Windows (Windows Update). Устанавливаем тип запуска Отключено и останавливаем работу службы.Теперь вам нужно решить, хотите ли вы, чтобы ваша виртуальная машина имела доступ к Интернету. или вашей локальной сети. Чтобы сеть виртуальной машины работала правильно, вам необходимо настроить сети, чтобы Хост и Гость могли общаться.Предупреждение: Диапазон 192.168.122.0/24— это диапазон по умолчанию для первого интерфейса KVM (обычно virbr01) и его можно использовать в качестве проверки ANTI VM .При настройке статического IP-адреса (объяснять как это делается не имеет смысла) в качестве шлюза необходимо указать IP-адрес интерфейса virbr<номер интерфейса> (по умолчанию virbr1)Проверка доступа к сети путем проверки связи гостя и хоста является хорошей практикой, чтобы убедиться, что виртуальная сеть настроена правильно.Используйте только статические IP-адреса для своих гостей, поскольку CAPE не поддерживает DHCP (по крайней мере, на момент написания этой статьи).Предлагаемая разработчиками настройка — использование изолированной сети (hostonly) для вашей виртуальной машины. В этом случае будет отсутствовать подключение к сети Интернет. Для этого вы можете следовать приведенным ниже инструкциям, если вы используете KVM и virt-manager (менеджер виртуальных машин).Cначала в графическом интерфейсе диспетчера виртуальных машин нажмите «Правка» -> «Сведения о подключении» .Если после создания новой изолированной сети вы уже создали виртуальную машину, вы можете выбрать ее в диспетчере виртуальных машин.Следующее — проверка того, что новый интерфейс действительно создан и виртуальная машина действительно его использует. На своем хосте выполните следующую команду из командной строки: ip a. Должен быть интерфейс с IP-адресом, который вы указали при его создании.Гостевая виртуальная машина и хост должны иметь соединение между собой. Чтобы проверить это, вы можете использовать такие инструменты, как pingили telnet.При успешной конфигурации внутренней сети между виртуальной машиной и хостом можно переходить к установке агента. Агент CAPE разработан как кроссплатформенный, поэтому вы имете возможность использовать его как в  Windows, так и в Linux.На хосте в каталоге аgent/ вы найдете файл аgent.py , просто скопируйте его  в гостевую операционную систему (любым способом, возможно, в временную общую папку, загрузив ее с хост-веб-сервера или монтирование компакт-диска с файлом аgent.py ) и запустите его. Предлагаю запустить HTTP-сервер из директории /opt/CAPEv2/agent c помощью команды :python3 -m http.serverДалее заходим в браузер по адресу 192.168.122.1:8000 на виртуальной машине и копируем нужный файл agent.py.Предупреждение: Рекомендуется использовать агент в его безоконной версии ( .pyw расширение) учитывая, что открытие cmd определенно будет мешать работе human.py , вызывая несколько проблем, таких как блокировка agent.py.Не забудьте протестировать агент перед сохранением снимка. Вы можете сделать это, перейдя к VM_IP:8000с помощью браузера с вашего хоста или выполните: curl VM_IP:8000. Вы должны увидеть вывод, аналогичный следующему:Если вы хотите, чтобы скрипт запускался при загрузке Windows, поместите файл в папке автозагрузки администратора. Чтобы получить доступ к этой папке, откройте приложение с помощью Win+R и найдите «shell:common Startup», который откроет нужную папку (обычно C:\ProgramData\Microsoft\Windows\Start Menu\Programs\StartUp). Не помещайте агент в папку автозагрузки пользователя (обычно C:\Users\<Username>\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Startup), так как он запустит агент без прав администратора, что приводит к неправильной работе агента.Редактирование conf-файловСледующим шагом в нашей настройке будет редактирование конфигурационных файлов которые находятся в директории /opt/CAPEv2/confВАЖНО!!! Редактирование conf-файлов обязательно нужно производить от пользователя capeПройдемся по основным настройкам необходимым для первоначального запуска песочницы и её безошибочной работы. Первым делом открываем файл cuckoo.conf и обращаем внимание на параметры:machinery = kvm (либо название используемого вами гипервизора)memory_dump = off ( либо on, если вам необходим полный дамп памяти машины после завершения анализа)ip = 192.168.122.1 (в данном параметре указываем ip-адрес используемого интерфейса хостовой системы, который мы указывали в качестве шлюза при настройке сети)port = 2042 (порт, используемый CAPE по умолчанию)Следующий конфигурируемый файл - kvm.conf (при условии, что в качестве гипервизора вы использовали рекомендованный KVM):machines = win7 (указываем наименования созданной нами виртуальной машины)interface = virbr1 (используемый для внутренней сети интерфейс)label = win7 (название ВМ)platform = windows (операционная система ВМ)ip = 192.168.122.191 (адрес ВМ)arch = x64 (архитектура используемой ВМ)После открываем файл qemu.conf. Параметры его настройки аналогичны параметрам из предыдущих conf-файлов:Если виртуальная машина имеет доступ в Интернет, то в файле routing.conf обязательно следует указать параметр route = internetНазначение и параметры настройки остальных файлов вы можете прочитать в официальной документации. В основном, благодаря конфигурации других файлов, можно сделать песочницу достаточно гибкой и адаптивной системой анализа, включить необходимые функции или отключить то, что мешает или ненужно, активировать vpn или настроить использование tor. Также существует несколько файлов для настройки CAPE при наличии других гипервизоров.Запуск песочницыПредупреждение: Не забудьте сделать снимок готовой к работе виртуальной машины (snapshot). При использовании песочницы ВМ всегда будет запускаться из состояния снимка, а после выполнения анализа обратно возвращаться к нему.Чтобы создать снимок ВМ, необходимо в Менеджере виртуальных машин (virt-manger) в окне "Управление снимками" слева внизу нажать на кнопку "Создать снимок"Запуск CAPE производится из корневой папки /opt/CAPEv2 под пользователем cape.Для запуска CAPE используйте команду:python3 cuckoo.pyНо, поскольку для установки зависимостей мы использовали poetry, следует запускать CAPE с помощью следующей команды:sudo -u cape poetry run python3 cuckoo.pyи с помощью командыpoetry run python3 cuckoo.pyесли терминал уже используется пользователем cape.Примерный вывод:CAPE ждёт загрузки необходимого файла для проведения анализа.Иногда, после перезапуска работы песочницы используемый порт не освобождается, для устранения этого достаточно прописать следующую команду: sudo fuser -n tcp -k 2042 (2042 - порт по умолчанию)CAPE предоставляет полноценный веб-интерфейс в виде приложения Django. Этот интерфейс позволит вам отправлять файлы, а также просматривать отчеты по всем результатам анализа.Для доступа к нему (после запуска cuckoo.py в терминале) в браузере переходим по адресу localhost:8000:Загрузку подозрительного файла можно выполнить с помощью вкладки "Submit" :Выбираем нужный файл, выставляем необходимые настройки и запускаем анализ. Необходимо некоторое время, чтобы CAPE запустила созданную виртуальную машину и провела в ней все необходимые тесты и процедуры анализа. По окончании работы песочницы во вкладке "Recent" можно наблюдать результат проведенного анализа:Подводя итог, следует сказать, что в данной статье рассмотрен один из вариантов конфигурации для установки CAPE, которая позволяет экспериментировать, гибко настраивать и выбирать конфигурацию песочницы в зависимости от своих целей. Если в процессе установки возникли вопросы, нерассмотренные в статье, вы можете обратиться к официальной документации разработчиков по детальной установке и настройке.    
--------------------------
Ссылка: https://habr.com/ru/articles/780172/
Инструкция: как поднять GitLab CI/CD на GoLang-проекте
comerc  (https://habr.com/ru/users/comerc/)
 В продолжение к заметке Инструкция: как быстро настроить GitLab CI/CD на Flutter-проекте.Больше спасибо автору, всё получилось относительно легко. Я усложнил задачу: поднял GitLab локально на Хакинтоше, прикрутил executor = "docker" вместо "shell". И началось веселье.Docker DesktopНе повторяйте мою ошибку: сначала сохраните данные, если есть наработки в Docker-образах. Установливаю с официального сайта, через скачанный Docker.dmg v4.25.2:$ docker -v
Docker version 24.0.6, build ed223bc
$ docker-compose version
Docker Compose version v2.23.0-desktop.1
Локальная установка GitLab в Docker# docker-compose.yml
version: '3'
services:
  web:
    image: 'gitlab/gitlab-ce:latest'
    restart: always
    hostname: 'localhost'
    environment:
      GITLAB_OMNIBUS_CONFIG: |
        external_url 'http://localhost'
    ports:
      - '80:80'
      # - '443:443'
      - '22:22'
    volumes:
      - '~/.gitlab/config:/etc/gitlab'
      - '~/.gitlab/logs:/var/log/gitlab'
      - '~/.gitlab/data:/var/opt/gitlab'
$ docker-compose up -d
Пошуршит пару минут, потом доступен по адресу http://localhost. Сбрасываю пароль:Если пароль был сохранён ранее для входа в git через VSCode, тоже можно сбросить:$ git credential-osxkeychain erase host=localhost protocol=http
DOCKER_AUTH_CONFIGПроверяю доступ к аккаунту в docker.io:$ docker logout
$ docker login
Login Succeeded
Генерирую хэш:$ printf "my_username:my_password" | openssl base64 -A
# Example output to copy
bXlfdXNlcm5hbWU6bXlfcGFzc3dvcmQ=
Добавляю хэш в конфиг:$ nano ~/.docker/config.json
{
  "auths": {
    "docker.io": {
      "auth": "bXlfdXNlcm5hbWU6bXlfcGFzc3dvcmQ="
    }
  },
  "credsStore": "desktop",
  "currentContext": "desktop-linux",
  "plugins": {
    "-x-cli-hints": {
      "enabled": "true"
    }
  }
}
Прописываю DOCKER_AUTH_CONFIG:{
  "auths": {
    "docker.io": {
      "auth": "bXlfdXNlcm5hbWU6bXlfcGFzc3dvcmQ="
    }
  }
}
Рыба проектаСоздаю новый проект в интерфейсе GitLab и клонирую его к себе:$ git clone http://localhost/root/my-project.git
$ cd my-project
Пора добавить божественный main.go:$ go mod init my-project
$ nano main.go
package main

import (
	"log"
	"time"
)

func main() {
	for {
		time.Sleep(time.Second)
		log.Println("OK")
	}
}
Мне нужен Dockerfile для локальной сборки:$ nano DockerfileFROM golang:onbuild AS build
WORKDIR /build
COPY . .
RUN CGO_ENABLED=0 GOARCH=amd64 GOOS=linux go build -o ./app

FROM ubuntu:20.04 AS ubuntu
RUN apt-get update
RUN DEBIAN_FRONTEND=noninteractive apt-get install -y upx
RUN apt-get clean && \
    rm -rf /var/lib/apt/lists/*
COPY --from=build /build/app /build/app
RUN upx /build/app

FROM scratch
COPY --from=ubuntu /build/app /build/app

ENTRYPOINT ["/build/app"]
Вся эта суета, чтобы получить образ (image) на 577KB:ubuntu применяет волшебный upx;scratch тоже сокращает итоговый размер.И для порядка:$ nano .dockerignore
Содержимое:.git
Вишенка на торте:$ nano docker-compose.ymlversion: '3'
name: 'my-project'
services:
  app:
    build: .Пробую запустить в докере:$ docker-compose up -d --build
gitlab-runnerУстановка:$ brew install gitlab-runner
$ brew services start gitlab-runner
Добавляю теги 'ci, cd':И получаю токен:Выполняю регистрацию:$ gitlab-runner register \
 --url "http://localhost" \
 --description "via docker" \
 --docker-image "alpine" \
 --executor "docker" \
 --docker-privileged \
 --docker-volumes "/certs/client" \
 --docker-network-mode host \
 --docker-helper-image "registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-e0218c92"
Открываю на редактирование config.toml:$ nano ~/.gitlab-runner/config.toml
Удаляю значение для ключа image:[[runners]]
  name = "via docker"
  # ...
  [runners.docker]
    image = ""
    # ...
.gitlab-ci.yml$ nano .gitlab-ci.yml
Я: Как перевести Dockerfile в формат .gitlab-ci.yml?FROM golang:onbuild AS build
WORKDIR /build
COPY . .
RUN CGO_ENABLED=0 GOARCH=amd64 GOOS=linux go build -o ./app
ChatGPT:stages:
  - build

variables:
  GOARCH: amd64
  GOOS: linux
  CGO_ENABLED: 0

build:
  stage: build
  image: golang:onbuild
  script:
    - go build -o ./app
  artifacts:
    paths:
      - ./app
Добавляю от себя:  tags:
    - cd

workflow:
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: always
Всё, можно коммитить. Прямо ветку main, как мы любим. Потом делаю новую ветку с каким-либо изменением и оформляю "merge request".Запущенный job выполнен успешно, можно скачать результат:UPD: решил вопросы с запуском gitlab-runner и отказался от Docker Desktop 4.26.0.    
--------------------------
Ссылка: https://habr.com/ru/articles/780170/
Никто никогда не учит писать качественный софт
PatientZero  (https://habr.com/ru/users/PatientZero/)
 ВведениеВы когда-нибудь участвовали в проекте разработки ПО, в котором отсутствовали жизненно необходимые меры по обеспечению качества? Вы в этом не одиноки. Такое случается в потрясающе огромном проценте компаний и проектов. Даже если компании знают о существовании такого понятия, как QA, и что его нужно выполнять, все усилия обычно приводят лишь к большому спринту QA прямо перед релизом. Это стрессовый период, в который мы пытаемся заставить ПО хотя бы немного работать. Разумеется, весь этот хаос повторяется на следующем цикле релиза без малейших улучшений.Чему нас учат в вузахПроблема в том, что при изучении computer science вас не учат, как обеспечить стандарты качества ПО. Основную часть времени тратят на изучение алгоритмов, принципов работы компьютера, историю каких-то языков и концепций и так далее. Кроме того, по крайней мере, в моей учёбе, был семестр, посвящённый методикам управления проектами и Scrum. Всё это замечательно, но тут совершенно отсутствует QA. Пренебрежение QA — это огромная потеря, потому что больше 90% всех студентов после завершения учёбы работает в контексте компаний. Они должны будут выпускать ПО вовремя и без багов.Почему компании едва успевают выпускать ПО вовремяЯ сталкивался с этим бесконечное количество раз. Стандарты и меры обеспечения QA одними из первых попадают под нож, потому что для проектов важнее всего бюджет. Зачастую они планируются на конец проекта, но если разработка затягивается (а такое бывает часто) или происходит разрастание фич (что бывает всегда), на QA больше не остаётся времени. В конечном итоге мы получаем абсолютный минимум неструктурированного тестирования и выпускаем цифровой карточный домик с шатающимися стенами.Увеличение масштабов и набора функций приводит к повышению трудозатрат на тестированиеВ некоторых компаниях и командах применяются некие стандарты QA. Обычно их внедрением занимается сотрудник сеньор-уровня. Чаще всего он на своём горьком опыте знает, что без QA работа постепенно становится всё более сложной. К сожалению, даже наличия стандартов недостаточно. Очень часто команды просто пишут тесты, чтобы удовлетворить метрикам управления проектом.Как сойти с этой каруселиМне потребовались долгие годы опыта, чтобы набраться уверенности и начать говорить об отсутствии мер обеспечения QA в проектах. До этого мне приходилось спорить с менеджерами, ночевать на работе перед релизами, иметь дело с разваливающимися в продакшене системами и искать отсутствующий мониторинг. Всё это было невесело. Эти ситуации схожи с другими улучшениями кодовой базы или проекта, которые незаметны менеджерам, например, с рефакторингом. Но в случае с QA это было особенно сложно, потому что если мы не внедряли никаких мер его обеспечения, мы так и не учились делать это правильно.Что бывает, когда юнит-тесты успешны, а интеграционные тесты нет.Только высказывая своё мнение и поднимая это на обсуждение снова и снова, мы сможем сделать первые шаги, чтобы выйти из этого порочного круга.Поговорим о деньгахВ какой-то момент я осознал, что использую неподходящие аргументы. Если говорить, что ПО будет «стабильнее» или что оно «сильно упростит поддержку», то эти доводы не будут иметь особого смысла для тех, кто не работает над самой кодовой базой. Нужно говорить о деньгах. Мы, разработчики, должны говорить о том, сколько стоит отсутствие QA. Это язык бизнеса и руководства в целом. Сейчас я всегда стараюсь объяснить меры обеспечения QA на таких примерах: «Если не заняться этим сейчас, то трудозатраты на разработку (а значит, и затраты) через четыре месяца увеличатся на 15%» или «Нам нужно реализовать юнит-тесты для всех фич, или наши фазы стабилизации релизов с каждым разом будут становиться всё длиннее. Это напрямую относится ко всем создаваемым нами фичам, потому что нам приходится каждый раз вручную тестировать все побочные эффекты. В результате при каждом релизе наш прогресс будет всё меньше».По моему опыту, переход на такие примеры помогает донести мысль. В конечном итоге, ваши усилия улучшат жизнь каждого, только пока это не все понимают.Минимальная эффективная дозаЧтобы быть реалистичными, важно не оверинжинирить меры обеспечения QA, тратя на них предварительно большие суммы. Мы не должны мешать развитию проекта в целом; к тому же такой подход понравится не всем стейкхолдерам. Я всегда советую выделить самые жизненно важные части приложения. Обычно существует определённый сценарий использования, фича или нечто иное, на чём построено всё приложение. Это какая-то базовая функциональность, которая обязана работать корректно, чтобы ПО имело ценность для клиента или пользователя. Вот её-то и нужно тестировать. Придумайте меры и способы обеспечить её работу так, как это задумано.Мне нравится термин «минимальная эффективная доза» (МЭД). Это минимальная доза, дающая желаемый результат. В области QA это может быть план ручного тестирования, автоматизированные тесты в конвейере или что-то другое. Вот с этого и стоит начинать. Если функциональность базовых фич обеспечена, можно постепенно расширять стабильность, например, добавлять для всех новых фич юнит-тесты. Кроме того, подумайте об источниках информации, которые вы не можете контролировать, например, о внешних API или вводимых пользователем данных. Находите способы валидировать и их тоже, потому что это очевидные точки, в которых ваше ПО может вылетать из-за неправильного использования. Проводите итерации и работайте инкрементно. Это относится и к QA.Что я ищуВ каждом новом проекте, который я начинаю или продолжаю, я ищу концепцию QA. Какой бы маленькой она ни была, команде нужно подумать о ней.Что мы выпускаем?Что нам нужно, чтобы это работало?Как это сделать?Какие из мер мы намеренно исключим и почему?Наличие письменной документации плюс, возможно, плана тестирования — отличный фундамент, на котором может развиваться ПО. Они показывают, что мы, команда, подумали, как двигаться дальше. Подумали с точки зрения МЭД. Кроме того, я рекомендую регулярно пересматривать выбранные методики, допустим, раз в квартал.При написании кода я не пользуюсь TDD, но крайне рекомендую писать тесты параллельно с написанием ПО. Это подходящее время для написания теста. Если вы пишете тесты в процессе реализации фичи, то получите дополнительное преимущество: код будет структурирован так, что его действительно можно тестировать. При написании тестов для уже готового ПО часто выясняется, что в его коде слишком много взаимных зависимостей или что нарушены принципы единственной ответственности. Благодаря тестам вы показываете, что поняли желаемое поведение и гарантировали, что всё работает, как ожидается. Это даже можно назвать документацией к коду.Преимущества для проектаКогда вы заводите об этом разговор, окружающие вас люди понимают, что вас заботит проект. Поднимая вопрос качества и предлагая возможные решения, вы расширяете свою сферу влияния как разработчика. Это может принести пользу и вам, и проекту. Качество жизни разработчиков и менеджеров повысится. Это определённо заметят.Только при наличии мер обеспечения QA проект может расти здоровыми темпами.Меняем проекты к лучшемуВы уже используете меры обеспечения QA в своих проектах или в них всё очень шатко? Хотите расширить свои навыки разработчика и быть известным как человек, пишущий качественное ПО?Начните с малого. Подумайте о МЭД своего проекта. Возьмите на себя ответственность и станьте в своей команде тем, кто меняет всё к лучшему. Не всем нужно быть проповедниками QA, но вы можете учить людей необходимым методикам, показывая их на своём примере. Станьте тем, кто начнёт это обсуждение.    
--------------------------
Ссылка: https://habr.com/ru/companies/raft/articles/780166/
Создание чат-бота для конференции с GPT Engineer за 2 часа
Squirrelfm  (https://habr.com/ru/users/Squirrelfm/)
 Изображение сгенерировано с помощью AI, ни один человек в процессе не пострадалПару месяцев назад мы решили принять участие в недавней конференции Highload, и нам потребовалось что-то интересное и интерактивное, чтобы привлечь людей к нашему стенду. После некоторых раздумий выбор пал на создание чат-бота. Но совсем не типичного: основная его цель - общение с пользователями в игровом формате. Бот показывает изображение, сгенерированное AI, и предлагает составить промт, который бы максимально точно воссоздал это изображение. Довольно занимательно, правда?Сроки поджимали, до конференции оставалось всего два дня, и, поскольку все остальные были заняты, я вызвался разработать  Telegram бота. Честно признаться, такой опыт был у меня впервые, но я верил в силу всемогущего искусственного интеллекта, и принялся за дело!  Для тех, кто не знаком, GPT Engineer - это инструмент, схожий с Auto-GPT. Он способен автономно генерировать код и создавать целые приложения, основываясь лишь на описании. По крайней мере, такова теория. На практике все немного сложнее: да, он может генерировать код, но успешно запустить его - уже совсем другая история. Это хорошая отправная точка, однако для достижения желаемого результата вам, скорее всего, придется дорабатывать и корректировать ее. Об этом, и о том как скоро AI заменит программистов - в конце статьи.Чтобы использовать GPT Engineer, нужно склонировать репозиторий и настроить его, следуя инструкциям в прилагаемом файле README. По сути, это набор скриптов на Python, поэтому процесс настройки относительно прост. Когда все будет готово, опишите, что вы хотите сделать в файле 'main_prompt', который вам нужно будет создать в папке projects. Затем запустите инструмент и подождите, пока он сгенерирует код.Когда дело доходит до описания того что вы хотите получить, точность имеет ключевое значение. Нужно как можно подробнее описать желаемый результат и стек, которые вы хотите использовать. Кроме того, желательно использовать последнюю версию API OpenAI (я использовал версию 4), так как результаты значительно отличаются в зависимости от версии. Вот мой гениальный промт:Bot design promptOverviewThis document outlines the design for a game where players guess the textual prompts used to generate images by DALLE-3. The game will be implemented using the OpenAI API, Python programming language, and Telegram API. It is a guessing game where players are shown an image generated by DALLE-3 and must guess the prompt used to create it.Game FlowStart: The player starts the game through a Telegram bot.Image Presentation: A DALLE-3 generated image is presented to the player.Guessing Phase: The player has 3 attempts to guess the prompt.Submission: After 3 attempts, the player submits their final guess.Data Storage: The game stores the player’s guess and other relevant data.Technical ComponentsOpenAI APIPurpose: To generate images based on textual prompts.Integration: Python scripts will interact with the OpenAI API to request image generation.Python BackendFunctionality:Communicate with the OpenAI API.Handle Telegram bot interactions.Manage game logic and state.Store and retrieve data.Libraries:python-telegram-bot: For Telegram bot interactions.requests: For making API calls to OpenAI.os, json: For file handling and configuration management.Telegram APIPurpose: To provide a platform for users to play the game.Integration:The game will be accessible through a Telegram bot.The bot will handle user inputs and display images and messages.Game LogicStarting Image:Read an image name from the config file, load it from the root folder.User Interaction:Present the image to the user via Telegram bot. Receive guesses from the user.After each guess - use OpenAI API to generate an image from that prompt and show this image to the user.Allow up to 3 attempts per image.Data Handling:After 3 attempts or final submission, save the user’s guess.Store the image, prompt, user’s guess, Telegram username, and account info.Data StorageLocation: user_data folder in the bot’s root directory.Structure:Subfolders named after the user’s Telegram username.Filder contains info.txt file with all user data and an image generated by the user.Configuration FileFile Name: bot_config.jsonContents:API keys.Game settings (e.g., time limits, number of attempts).Other configurable parameters.  Internationalizationgettext should be used for all text messagesКак видите, довольно подробно. На основе этого тула сгенерировала проект, который я попытался запустить - безуспешно. Оказалось, что данные, на которых обучался OpenAI, были устаревшими. Telegram API значительно изменился несколько месяцев назад, и модель не знала об этом. Поэтому, покопавшись в документации Telegram (параллельно про себя проклиная Дурова и всю его команду за то, что они так часто меняет API), я смог настроить код.Но прежде чем его запустить, нам нужно зарегистрировать бота в Telegram. Это можно сделать, обратившись к специальному боту Telegram под названием BotFather.BotFather - это крестный отец официальный бот Telegram, который позволяет создавать и управлять ботами. Чтобы начать, найдите '@BotFather' в строке поиска Telegram и начните с ним чат. Далее:1. Создайте нового бота: отправьте BotFather команду /newbot. После этого BotFather предложит вам выбрать имя и юзернейм для вашего бота. Имя - это то, что пользователи будут видеть в беседах, а юзернейм - то, как ваш бот будет найден в Telegram. Юзернейм должен заканчиваться на "bot", например "examplebot".2. Получите токен: После того как вы укажите боту имя, BotFather даст его токен. Это уникальный идентификатор для вашего бота, который используется для аутентификации ваших запросов к Telegram Bot API.3. Настройте бота (необязательный пункт): С помощью BotFather вы можете настроить картинку профиля бота, его описание, информацию о нем и многое другое. Эти команды необязательны, но помогут сделать ваш бот более привлекательным.Теперь, когда вы зарегистрировали своего бота, давайте посмотрим на код.Здесь находится класс main.py - загрузчик игры, с помощью которого мы будем запускать бота:from bot_config import BotConfig
from telegram.ext import Application

def main():           
   # Load our bot configuration
   config = BotConfig.load_config()
   application = Application.builder().token(token=config.telegram_token).build()

   # Instantiate the bot
   game = Game(config)
   game.start(application)
   print("Bot has started")Application - это класс Telegram API, который управляет приложением Telegram.Основная логика игры будет находиться в классе Game:def start(self, application: Application):
   self.app = application

   # Add command handlers
   application.add_handler(CommandHandler("start", self.handle_start))
   application.add_handler(CommandHandler("rules", self.handle_print_rules))
   application.add_handler(MessageHandler(Filters.TEXT & ~Filters.COMMAND, self.handle_message))

   # Start the bot
   application.run_polling(allowed_updates=Update.ALL_TYPES)Здесь мы определяем типы команд/сообщений, на которые будет реагировать наш бот. У нас будет 3 типа: команда start - для запуска бота, команда rules, которая показывает правила игры пользователю, и любой рандомный message, обычно являющийся описанием изображения, созданного пользователем.Поскольку одновременно может играть множество пользователей, нам необходимо отслеживать каждую сессию. Для этого в конструкторе игры мы создадим словарь под названием sessions.class Game:
   def __init__(self, config):
       self.sessions = {}
       self.config = config
       self.app = NoneКогда мы получаем сообщение от пользователя, мы решаем, является ли он вернувшимся пользователем - в этом случае мы загружаем его сессию из папки user_data (из файла info.txt), или новым пользователем, в этом случае мы создаем новую сессию:def get_user_session(self, update: Update):
   user_id = update.message.from_user.id

   if user_id not in self.sessions:
       user = User(user_id, update.message.from_user.full_name, update.message.from_user.username)
       session = self.load_user_session(user)
       self.sessions[user_id] = session
   return self.sessions[user_id]

  
def load_user_session(self, user):
   user_folder = f'user_data/{user.telegram_name}'
   if not os.path.exists(user_folder):
       os.makedirs(user_folder)

   info_path = os.path.join(user_folder, 'info.txt')

   if not os.path.exists(info_path):
       user.image_submitted = ''
       user.attempts_left = self.config.max_attempts
       return GameSession(user, self.config)

   with open(info_path, 'r') as info_file:
       info_file.readline()
       info_file.readline()
       info_file.readline()
       user.image_submitted = info_file.readline().split(':')[1].strip()
       user.attempts_left = int(info_file.readline().split(':')[1].strip())
       user.last_prompt = info_file.readline().split(':')[1].strip()
       user.position = info_file.readline().split(':')[1].strip()
       user.company = info_file.readline().split(':')[1].strip()

   session = GameSession(user, self.config)
   return sessionИ наконец, мы хотим определить хендлеры для всех команд:async def handle_start(self, update: Update, context: CallbackContext):
   if update.message.from_user is None:
       return

   session = self.get_user_session(update)
   await session.start(update, context)
    
async def handle_message(self, update: Update, context: CallbackContext):
   session = self.get_user_session(update)
   await session.handle_message(update, context)
  
async def handle_print_rules(self, update: Update, context: CallbackContext):
   session = self.get_user_session(update)
   await session.handle_print_rules(update, context)Как вы можете видеть, мы делегируем обработку всех команд классу пользовательской сессии, потому что сессия будет иметь необходимый контекст для этого. Здесь находится класс GameSession, в котором и происходит вся реальная работа:class GameSession:
   STATE_NOT_STARTED = 'not_started'
   STATE_WAITING_FOR_POSITION = 'waiting_for_position'
   STATE_WAITING_FOR_COMPANY = 'waiting_for_company'
   STATE_WAITING_FOR_PROMPT = 'waiting_for_prompt'
   STATE_WAITING_FOR_CONFIRMATION = 'waiting_for_confirmation'
    
   def __init__(self, user, config, is_running = False):
       self.config = config
       self.user = user
       self.dalle_service = DalleService(config.openai_api_key)
       self.last_image = None
       self.attempts_left = user.attempts_left
       self.state = self.STATE_NOT_STARTED if not is_running else self.STATE_WAITING_FOR_PROMPTКак видите, у игры могут быть разные статусы, которыми нам нужно управлять. Это может быть ожидание промта пользователя для создания изображения или ожидание подтверждения для определенных действий. Она также отслеживает оставшиеся попытки и текущее состояние дел. В самом начале игра отправляет пользователю правила игры и запрашивает начальное изображение:async def start(self, update: Update, context: CallbackContext):
   if self.state != self.STATE_NOT_STARTED:
       if (self.attempts_left > 0):
           await update.message.reply_text(_("Game is already running. Please enter a prompt for the image:"))
       else:
           await update.message.reply_text(_("You have no attempts left. Please come back tomorrow."))
       return

   self.state = self.STATE_WAITING_FOR_PROMPT
   await update.message.reply_text(_("Welcome to the 'Culture Code' game! Here are the rules..."))
   await update.message.reply_text(_("Game Rules"))
    
   # Send the predefined image
   with self.get_predefined_image() as image:
       await update.message.reply_photo(photo=image)

   await update.message.reply_text(_("Please enter a prompt for the image:"))Как только пользователь ответит, будет вызван message handler. Он использует запрос пользователя для генерации изображения с помощью OpenAI Dall-e API, чтобы сгенерировать изображение и показать его пользователю. После этого игра спросит, хочет ли пользователь отправить изображение судьям или хочет попробовать еще раз (если у пользователя остались попытки):async def handle_message(self, update: Update, context: CallbackContext):
 if self.state == self.STATE_WAITING_FOR_PROMPT:
     # Check if the user has already submitted an image today and have attempts
     if self.user.image_submitted:
         await update.message.reply_text(_("You have already submitted a result today. Please come back tomorrow."))
         return 

     if not self.has_attempts_left():
         await update.message.reply_text(_("You have no attempts left. Please come back tomorrow."))
         return 

     prompt = update.message.text
    
     # Generate image from DALLE-3
     try:
         await update.message.reply_text(_("Generating image..."))
         self.last_image = self.dalle_service.generate_image(prompt)
         self.user.last_prompt = prompt
         await update.message.reply_photo(photo=self.last_image.image_data)

         self.state = self.STATE_WAITING_FOR_CONFIRMATION
         if (self.attempts_left == 1):
             await update.message.reply_text(_("This is your last attempt. Do you want to submit this image for validation? (yes/no)"))
         elif (self.attempts_left == 2):
             await update.message.reply_text(_("You have 2 attempts left. Do you want to submit this image for validation? (yes/no)"))
         else:
             await update.message.reply_text(_("Do you want to submit this image for validation? (yes/no)"))
     except Exception as e:
         await update.message.reply_text(str(e))
 elif self.state == self.STATE_WAITING_FOR_CONFIRMATION:
     if update.message.text.lower() == 'yes':
         if self.submit_attempt(self.last_image):
             await update.message.reply_text(_("Congratulations, the image was submitted!"))
             if not self.user.position:
                 await update.message.reply_text(_("Btw, what is your position in your company?"))
                 self.state = self.STATE_WAITING_FOR_POSITION
                 return
         else:
             await update.message.reply_text(_("Something went wrong! Please enter another prompt:"))
         self.state = self.STATE_WAITING_FOR_PROMPT
     elif update.message.text.lower() == 'no':
         self.attempts_left -= 1
         self.save_user_info()
         if self.has_attempts_left():
             await update.message.reply_text(_("Allright. Attempts left: ") + str(self.attempts_left) + ". " + _("Please enter another prompt:"))
         else:
             await update.message.reply_text(_("You have no attempts left. Please come back tomorrow."))
         self.state = self.STATE_WAITING_FOR_PROMPT
     else:
         await update.message.reply_text(_("Please answer with 'yes' or 'no'"))
 elif self.state == self.STATE_WAITING_FOR_POSITION:
     self.user.position = update.message.text[:100]
     self.save_user_info()
     await update.message.reply_text(_("How interesting! And what is the name of your company?"))
     self.state = self.STATE_WAITING_FOR_COMPANY
 elif self.state == self.STATE_WAITING_FOR_COMPANY:
     self.user.company = update.message.text[:100]
     self.save_user_info()
     await update.message.reply_text(_("Understood. We will validate your image and send you the results soon. Thank you for participating! Please come back tomorrow \xF0\x9F\x98\x89."))
     self.state = self.STATE_WAITING_FOR_PROMPTЕсли пользователь решает отправить изображение - игра заканчивается, а изображение сохраняется для последующей оценки жюри:def submit_attempt(self, image):
   if self.attempts_left > 0 and not image is None and not self.user.image_submitted:
       self.attempts_left = 0
       self.save_user_image(image)
       self.user.image_submitted = image.image_name
       self.save_user_info()
       return True
   return False

def save_user_image(self, image):
   user_folder = f'user_data/{self.user.telegram_name}'
   if not os.path.exists(user_folder):
       os.makedirs(user_folder)

   image_path = os.path.join(user_folder, f'{len(os.listdir(user_folder)) + 1}.jpg')
   with open(image_path, 'wb') as image_file:
       image_file.write(image.image_data)

   return image_path

def save_user_info(self):
   user_folder = f'user_data/{self.user.telegram_name}'
   if not os.path.exists(user_folder):
       os.makedirs(user_folder)

   info_path = os.path.join(user_folder, 'info.txt')
   with open(info_path, 'w') as info_file:
       info_file.write(f"Name: {self.user.user_name}\n")
       info_file.write(f"Id: {self.user.user_id}\n")
       info_file.write(f"Telegram account: {self.user.telegram_name}\n")
       info_file.write(f"Image submitted: {self.user.image_submitted}\n")
       info_file.write(f"Attempts left: {self.attempts_left}\n")    
       info_file.write(f"Last prompt: {self.user.last_prompt}\n")
       info_file.write(f"Position: {self.user.position}\n")  
       info_file.write(f"Company: {self.user.company}\n")       Вот, собственно, и весь код игры! Есть еще несколько типовых классов для обработки вызовов API и хранения данных о пользователе и изображениях.Теперь о качестве кода и моих впечатлениях:Пока что программисты могут вздохнуть спокойно - полноценный проект AI создать не способен, и этому есть несколько причин:API и фреймворки обновляются быстрее чем современные модели, дообучать их просто не успевают. Соотвественно, модель генерирует устаревший код, и, что еще хуже, смесь из кода разной степени устаревания.Модель галлюцинирует - додумывает методы и свойства которых нет.Если в рамках одного модуля уже получается довольно связанный код - в рамках всего проекта бывает по разному: генерируются нигде не используемые классы, классы-пустышки, ненужные методы и тп.С ростом сложности проекта - все вышеперечиселнные проблеммы нарастают.Качество кода не идеальное с точки зрения стилистики и styleguides: не всегда указывает типы переменных, может не обрабатывать эксепшены и тп.Самый попобольный момент: модель вретДоктор Хаус был прав…а теперь и не только human Модель натурально обманула меня один раз, сказав то, что я хотел услышать, а не то, что было правдой! Тут я вспомнил одного разработчика в мою бытность лидом... В общем, совсем как человек. Скоро начнёт просить грейд повыше и смузи...В общем, будьте осторожны. Этот момент касается не конкретно GPT Engineer, а Chat GPT, который я использовал для доделок.Тем не менее, я считаю, что с поставленной задачей AI справился хорошо. Для простого бота сложный код с замудрённой архитектурой не только не нужен, но и только мешал бы. Простое, в лоб решение было сгенерировано относительно неплохо, и даже стилистически в целом модель генерирует логичный и понятный код (что даже не нужны комментарии). Такое может далеко лишь не каждый джун, мало кто из них способен (с). Учитывая, что это в принципе было невозможно всего год назад - тут есть над чем задуматься.Код можно найти здесь: https://github.com/vsemogutor/culture_code_pulbicHave fun!    
--------------------------
Ссылка: https://habr.com/ru/articles/780162/
Держись, Маша! Ты, ведь, наша! Продолжение разбора книги «Цифровая схемотехника и архитектура компьютера»
lws0954  (https://habr.com/ru/users/lws0954/)
 Упомянутая в заглавии книга (далее H&H) - это про железо [15]. Я - про программирование, но на базе "железной модели" конечного автомата. И там и там математическая основа одна. Все это, действительно, крутая железная концепция, помогающая поставить не только синтез цифровых схем, но и программирование на совершенно другие рельсы, определяющие его будущее. Параллелизм у программистов нынче в моде. Но, видимо, они (программисты) совсем не в курсе, что разработчики железа давным-давно погружены в эту тему. А потому им (я все про программистов) есть у кого поучиться. Но, похоже, некие амбиции-заскоки этому  мешают. Но, если вы этим не страдаете, то прочитайте книгу H&H и дойдите, ну, хотя бы до 4-й главы. Попробуйте реализовать одно-два упражнения, используя свой, программистский инструментарий - всякие там корутины, потоки и весь сопутствующий этому террариум. Убедитесь в его полном бессилии. И тогда, может, это заставит кое-что пересмотреть, переосмыслить. Только представьте: логический элемент - отдельный процесс, десятки, сотни, тысячи элементов - множество параллельных процессов, и все это в вашей ладошке (это я про смартфон) и даже работает!Но пришло время исполнять обещанное (см. предыдущую часть темы в [1]). И пусть количество "плюсов" пока не достигло заданной планки, но ... если каждый "минус" считать за два "плюса", то это уже более чем ... ;) Так что спасибо всем, давшим положительную оценку - нет, не автору, а затронутой теме. Области знаний, от которой многое сейчас зависит.  Это те слова, которые мы вправе сказать в адрес теории, посвященной  синтезу цифровых схем, в адрес тех, кто занимался и занимается ее развитием, становлением и внедрением в практику. И не надо забывать, что именно наши ученые, особенно советской школы, внесли в ее развитие свой значимый вклад. И, может, следовало бы дополнить книгу хотя бы небольшим обзором, отражающим этот факт. А так может создаться впечатление, что мы к этому имеем лишь косвенное отношение и даже в большей степени, как потребители. Ведь, даже учебник для себя сами написать не можем. Хотя это совсем не так...    Были и есть учебники, был и есть вклад... Просто все немного подзабылось.В основе проектирования цифровых схем лежит теория, базирующаяся на двух моделях цифровых схем - комбинационной и последовательностной. В первой части мы рассмотрели комбинационные схемы, эту же часть посвятим больше последовательностной модели. Эти же две темы определяют основу книги H&H, т.к. именно  на них базируется рассмотрение всего остального - от приемов и подходов к реализации схем до языков описания аппаратуры. СветофорКомбинационная модель - грубая модель, которая не учитывает временных свойств цифровых элементов. Но она проста и легко аппроксимируема понятиями теории булевых функций [2]. Это позволяет выполнять процедуры оптимизации схем, что достаточно подробно описано в научной литературе. Особенно в советской. И еще давно. А были, ведь, "богатыри"!Но легкость манипулирования булевыми функциями создает ложное представления о модели логических элементов. Формируются даже определенные мифы. И об одном из них мы ниже подробно поговорим.  Ну, а пока перейдем к задаче, которая в книге Н&Н приведена для знакомства со второй моделью цифровых схем - последовательностной. Речь пойдет о задаче проектирования системы управления (СУ) простого светофора.Создание СУ было поручено уже знакомому нам по книге студенту Бену Битдидлу, которой незамедлительно взялся за дело. Что послужило причиной такого задания подробно описано в разделе 3.4.1 книги H&H. Исходные данные для проектирования, место расположения светофоров и структурная модель системы управления показаны на рис. 1. В соответствии с заданием Бен установил на перекрестке два датчика - Ta и Tb. Они фиксируют движение по улицам. Управлять сигналами светофоров предполагается двумя сигналами La и Lb. Для формирования пауз между переключениями Бен решил тактировать СУ импульсами раз в 5 сек. По переднему фронту импульса светофоры должны  гореть в зависимости от состояния датчиков движения. Не была забыта и кнопка сброса светофора.Граф модели СУ (в книге он почему-то назван таблицей), который нарисовал Бен, показан на рис. 2. И сразу потекли вопросы. Возьмем форму графа. Если уж мы привлекли теорию автоматов, то желательно использовать описание, отвечающее принятым в ней нормам. Как это может выглядеть показано на рис. 3 и рис. 4. Здесь показана модель в двух классических вариантах - модели автомата Мура (рис.3), которая напрямую соответствует модели на рис. 2,  и эквивалентная ей модель автомата Мили (рис. 4).Рис. 1. Исходные данные на проектирование светофора Рис. 2. Автоматная модель автомата Мура, нарисованная БеномРазницу между моделями на рис. 2 и рис. 3 можно описать, как разницу между эскизом детали и конструкторским чертежом. Во-первых, что это за стрелка, направленная откуда-то сверху в состояние S0? Во-вторых, где у автомата начальное состояние?  Или этой стрелкой обозначено именно оно? Судя по всему, так оно и есть. Но "конструкторская документация" предусматривает для обозначения этого другие средства. Например, используя иное изображение начального состояния. На рис. 3, 4 начальным состоянием является состояние, имеющее двойные границы. Все выше сказанное можно посчитать за придирку, но в той или иной форме начальное состояние должно быть обязательно, а не обозначать в его качестве некий "прилет из безвоздушного пространства". Рис. 3. Автоматная модель светофора в форме автомата МураРис. 4. Автоматная модель светофора в форме автомата МилиБезусловно, модель автомата будет включать в себя те или иные особенности, связанные с реализацией. На рис. 2 это сигнал сброса Reset, устанавливающий автомат в начальное состояние. Но тогда, если следовать теории, такие же стрелки-переходы нужно направить из каждого состояния в состояние сброса - S0. На практике сброс - это фактически обязательная стандартная процедура, которой не следует "захламлять" модель. В схеме, реализующей автомат, это элементарный сигнал сброса регистра состояний модели (см. в [2] рис. 3.26 (с)). К примеру, в реализации автоматов на ПЛК функция сброса модели реализована программно. В каждом программном модуле она представлена соответствующей цепью  (см. в [3] цепь 1 на рис. 5). В рамках среды ВКПа (что это за среда см. [4]) функция сброса возвращает любой автомат в начальное состояние или в состояние, которое указано в методе FResetAction() (см. ниже код реализации автомата на С++ в ВКпа).Специфику программной реализации включают и модели на рис. 3, 4. Ей соответствует петля в начальном состоянии st, помеченная предикатом x12 и действием y12. Здесь предикат x12 выполняет проверку условий, обеспечивающих работу автомата, а отвечающее за инициализацию действие y12 создает ссылки на входные/выходные переменные автомата, выполняет инициализацию значений и т.п. Автомат покинет начальное состояние st, когда будут выполнены все процедуры инициализации.Поясним роль сигнала bType в моделях на рис. 3 и рис. 4.  В программной реализации автоматы, изображенные отдельно, объединены в один автомат (для этого достаточно "склеить" их начальные состояния), режим работы которого определяется данной переменной: в режиме автомата Мура bType=0,  в режиме автомата Мили bType=1. В книге H&H приведена аппаратная реализация модели. Программная реализация имеет свою специфику, но и она должна быть максимально приближена к формальной модели. При прочих равных условиях программная реализация многие вопросы решает все же гибче и проще. Единственно в чем она проигрывает - в скорости, но это дело наживное. Выше было сказано про объединение двух автоматов в один. Программная реализация показывает насколько просто это можно сделать. Листинг программы светофора на языке С++ в среде ВКПа, объединяющий две модели автоматов Мили и Мура, выглядит следующим образом: Hidden text#include "lfsaappl.h"

class FTraficLights :
    public LFsaAppl
{
    enum {enNone, enRed, enYellow, enGreen};
public:
    void MooreAction();
    void FResetActions();
    LFsaAppl* Create(CVarFSA *pCVF) { Q_UNUSED(pCVF)return new FTraficLights(nameFsa); }
    bool FCreationOfLinksForVariables();
    FTraficLights(string strNam);
    virtual ~FTraficLights(void);
    CVar *pVarStrNameTa;// имя входного датчика Ta
    CVar *pVarStrNameTb;// имя входного датчика Tb
    CVar *pVarTypeOfFSM;// тип автомата
    CVar *pVarTa;		//
    CVar *pVarTb;		//
    CVar *pVarLa;		//
    CVar *pVarLb;		//
protected:
    int x1(); int x2(); int x3(); int x12();
    void y1(); void y2(); void y3(); void y4(); void y5(); void y6(); void y7();
    void y8(); void y9(); void y10(); void y12();
};

#include "stdafx.h"
#include "FTraficLights.h"

static LArc TBL_TraficLights[] = {
    LArc("st",		"st","^x12","y12"),		
    LArc("st",		"S0","x12^x3","y9"),	
    LArc("st",		"S00","x12x3","y4y6y10"),	
// Moore's automaton
    LArc("S0",		"S1","^x1", "--"),	
    LArc("S1",		"S2","--",  "--"),	
    LArc("S2",		"S3","^x2", "--"),	
    LArc("S3",		"S0","--",  "--"),	
// Mealy`s automaton
    LArc("S00",		"S10","^x1","y3"),		
    LArc("S10",		"S20","--", "y2y8"),	
    LArc("S20",		"S30","^x2","y7"),		
    LArc("S30",		"S00","--", "y4y6"),	
    LArc()
};

FTraficLights::FTraficLights(string strNam): LFsaAppl(TBL_TraficLights, strNam)
{ FSetMoore(); }

FTraficLights::~FTraficLights(void) { }

void FTraficLights::FResetActions() { LFsaAppl::FResetActions(); }

void FTraficLights::MooreAction()
{
    string strState = FGetState();
    if (strState=="st")     { y1(); y5(); }		// none, none
    else if (strState=="S0"){ y4(); y6(); }		// La=green, Lb=red
    else if (strState=="S1"){ y3(); }           // La=yellow
    else if (strState=="S2"){ y2(); y8(); }		// La=red, Lb=green
    else if (strState=="S3"){ y7(); }           // Lb=yellow
}

bool FTraficLights::FCreationOfLinksForVariables() {
    CVar *pVar=nullptr;
    pVarTypeOfFSM = CreateLocVar("bTypeOfFSM", CLocVar::vtBool, "");
    pVarTa = CreateLocVar("bTa", CLocVar::vtBool, "bTa");
    pVarTb = CreateLocVar("bTb", CLocVar::vtBool, "bTb");
    string str;
    pVarStrNameTa = CreateLocVar("strTa", CLocVar::vtString, "Ta");
    str = pVarStrNameTa->strGetDataSrc();
    if (str  != "") { pVar = pTAppCore->GetAddressVar(str.c_str(), this);
        if (pVar) pVarTa = pVar;
    }
    pVarStrNameTb = CreateLocVar("strTb", CLocVar::vtString, "Tb");
    str = pVarStrNameTb->strGetDataSrc();
    if (str  != "") {
        pVar = pTAppCore->GetAddressVar(str.c_str(), this);
        if (pVar) pVarTb = pVar;
    }
    pVarLa = CreateLocVar("nLa", CLocVar::vtInteger, "nLa");
    pVarLb = CreateLocVar("nLb", CLocVar::vtInteger, "nLb");
    return true;
}

int FTraficLights::x1() { return static_cast<int>(pVarTa->GetDataSrc()); }
int FTraficLights::x2() { return static_cast<int>(pVarTb->GetDataSrc()); }
int FTraficLights::x3() { return static_cast<int>(pVarTypeOfFSM->GetDataSrc()); }
int FTraficLights::x12() { return pVarTa != nullptr && pVarTb; }

void FTraficLights::y1() { pVarLa->SetDataSrc(this, enNone); }
void FTraficLights::y2() { pVarLa->SetDataSrc(this, enRed); }
void FTraficLights::y3() { pVarLa->SetDataSrc(this, enYellow); }
void FTraficLights::y4() { pVarLa->SetDataSrc(this, enGreen); }

void FTraficLights::y5() { pVarLb->SetDataSrc(this, enNone); }
void FTraficLights::y6() { pVarLb->SetDataSrc(this, enRed); }
void FTraficLights::y7() { pVarLb->SetDataSrc(this, enYellow); }
void FTraficLights::y8() { pVarLb->SetDataSrc(this, enGreen); }
void FTraficLights::y9() { FSetMoore(true); }
void FTraficLights::y10() { FSetMoore(false); }

void FTraficLights::y12() { FInit(); }
Листинг 1. Программная реализация модели светофора на С++Миф об автоматах Мили и МураЦитата из H&H. "Существует два основных класса конечных автоматов, которые отличаются своими функциональными описаниями. В  автомате Мура  выходные значения зависят лишь от текущего состояния, в то время как в  автомате Мили  выход зависит как от текущего состояния, так и от входных данных." С этим можно было бы согласиться, но...Процитируем другого классика из классиков[3]. "... Для задания функций выходов (обычной или сдвинутой) ребра графа (стрелки) обозначаются не только входными, но и соответствующими им выходными сигналами. Если обозначенная входным сигналом xi стрелка соединяет вершину aj с вершиной ak, то в случае автоматов первого рода ей приписывается выходной сигнал λ1(aj, xi), а в случае автоматов второго рода - выходной сигнал λ2(ak, xi), где λ1 и λ2 - соответственно обычная и сдвинутая функции выходов автомата. В случае автоматов Мура все стрелки, входящие в одну и ту же вершину aµ, должны быть обозначены одним и тем же выходным сигналом. Поэтому принято обозначать выходными сигналами не стрелки, а вершины, в которые эти ребра входят".И это уже ближе к истине: сигналы с дуг убираются и приписываются вершине, в которую они направлены. И если посмотреть на рис. 2, то, вроде бы, противоречий-то и нет. Но они есть, т.к. утверждение, что выходной сигнал автомата Мура зависит только от состояния, которым он помечен, не верно.  На самом деле зависит, как и любой сигнал, выдаваемый автоматом Мили. Просто, повторимся, одинаковые выходные сигналы всех переходов, ведущих в одно состояние, приписываются этому состоянию, которое станет текущим только на следующем такте.  Т.е. мы имеем скрытую зависимость рассматриваемого выходного сигнала автомата Мура от текущего состояния и тех входных ситуаций, которые вызывают соответствующие переходы. Просто мы схитрили, упростив запись функции выходов автомата Мили, "обозвав" его попутно автоматом Мура. А автомат Мура - это лишь упрощенная запись эквивалентного ему автомата Мили.В силу сказанного выше мы можем "по щелчку" для любого автомата Мура создать эквивалентный автомат Мили, перенеся сигнал с вершины, на ведущие в нее дуги. Но мы наталкиваемся на проблемы, пытаясь найти эквивалентный автомату Мили автомат Мура. Все это объяснить это можно хотя бы тем, что, рассматривая некий автомат, как "черный ящик", в общем случае невозможно дать ответ о форме его внутреннего представления - это автомат Мили или Мура. Тестирование программной реализации на С++ в рамках ВКПа, представленной на листинге 1, в этом также убеждает: результаты абсолютно одинаковы, а тип автомата можно понять только по значению переменной bType.Аргументом в пользу развенчания мифа может служить и сравнение результатов программного моделирования схемной реализации автомата на рис.2 (представленна в H&H на рис. 3.26, стр. 318), с программной реализацией автоматов в разных формах (см. листинг 1). Опять же (по крайней мере в рамках ВКПа) их функционирование неотличимо. Рис. 22 в H&H создает "ложное" представление о независимости сигналов автомата Мура от входных сигналов автомата, т.к., как ни крути, переход в то или иное состояние (кроме начального)  у автомата Мура определяется только его текущим состоянием и текущей входной ситуацией на входах (смю выше цитату из книги H&H). Не было бы этих переходов не было бы и последующего состояния и соответственно сигналов автомата Мура.  Тем не менее, форма автоматов Мура имеет свои плюсы даже в сравнении со своим прародителем - автоматом Мили. Например у Мура вместо указания сигнала на каждом переходе он обозначается один раз при состоянии. Это повышает надежность проектирования, т.к. вероятность пропустить/забыть сигнал на переходе, особенно когда их достаточно много, гораздо выше, чем при состоянии. А, например для ПЛК использование автоматов Мура дает даже двойную выгоду. Так, привязав выходной сигнал командой OUT к состоянию, мы автоматически устанавливаем его при входе в это состояние и автоматически же сбрасываем при переходе в другое состояние. Удобно, наглядно, надежно.Плюсы аппаратной реализации автоматов Мура в сравнении с автоматами Мили (см. рис. 3.22(a)) тоже достаточно давно известны и понятны. Это стабильность выходных сигналов, на которые, действительно, не влияет нестабильностm входных сигналов, формируемых, если следовать терминологии H&H, некой "обезьяной" (см. стр. 348). Последняя в неконтролируемые по отношению к фронту тактового сигнала моменты времени с произвольной частотой изменяет входной сигнал, порождая так называемую метастабильность. Но, еще раз, это совсем не означает, что выходные сигналы автомата Мура не зависят от ситуации, вызвавшей его переход в то или иное состояние.Надуманность разделения автоматов на две форме можно показать и на других примерах. Например, в достаточно объемной монографии Марвина Минского "Вычисления и автоматы" при активном использовании автоматов почему-то эти две формы не упоминаются [6]. При этом, исходя из времени ее издания, сложно предположить, что он о них не знал. Еще навскидку - книга Дж фон Неймана по самовоспроизводящимся автоматам[7]. Или другая - про клеточные автоматы [8]. В них используются просто автоматы в своей единственной, определяемой формальным описанием, форме. C формальной точки зрения в автоматах Мура необходимости нет. Они всего лишь подмножество автоматов Мили и покрываются последними. А как же быть с их ролью в схемах с обратными связями (см. [3])? Ведь элемент памяти - классический пример автомата Мура. А никак! В них в этом случае нет нужды, т.к. их заменяют эквивалентные им и, кстати, неотличимые от них автоматы Мили. Правда, для этого необходимо изменить закон функционирования последних, убрав мгновенную зависимость выходных сигналов от входных. Или, говоря другими словами, использовать инерционную форму функции переходов автомата (инерционный закон функционирования автоматов подробно рассмотрен в [4]). Кстати в этом случае исчезает во многом необходимость включать автоматы в разрывы обратных связей.Сама по себе форма автоматов Мура достаточно удобна, т.к. повышает надежность проектирования, чем в определенных ситуациях ее применение оправдано. Но повторимся, это всего лишь модифицированная форма автомата Мили. А общем случае - просто автомата. Автомата, который известен, как совмещенная форма автоматов Мили и Мура. В теории проектирования цифровых схем ее еще часто называют С-автоматом [9]. Для книги H&H это сводится к совмещению двух структурных схем реализации автоматов в одну схему - совмещение схем (a) и (b) на рис. 3.22.О форме описания автоматовЕсли сравнить автоматы Мура на рис. 2 и рис. 3, то внимательный глаз увидит, что у модели на рис. 3 отсутствуют петли. В них просто нет нужды.  От слова совсем. Просто на рис. 3 используется минимизированная форма автомата - дизъюнктивная нормальная форма структурных автоматов (ДНФ СКА), описанная в статье [4]. Более того, синтез цифровой схемы по ДНФ СКА может порождать логические функции, требующие меньших шагов для их минимизации. Например, в нашем случае можно построить следующую таблицу переходов (Табл. 1) (подробнее про таблицы переходов см. [5]), по которой затем провести синтез комбинационной схемы.                                                                                                                                                             Табл.1  Ta,   TbS0(00)S1(01)S2(10)S3(11)^x1^x2S1(01)S2(10)S3(11)S0(00)^x1x2S1(01)S2(10)S3(11)S0(00)x1^x2S0(00)S2(10)S3(11)S0(00)x1x2S0(00)S2(10)S2(10)S0(00) Выражения, построенные по данной таблице, и шаги их минимизации будут выглядеть так:S0` = ^S1^S0^x1^x2 + ^S1^S0^x1x2 + S1^S0^x1^x2 + S1^S0^x1^x2 = ^S1^S0^x1 + S1^S0^x2S1` = ^S1S0^x1^x2 + ^S1S0^x1x2 + ^S1S0x1^x2 + ^S1S0x1x2 + S1^S0^x1^x2 + S1^S0^x1x2 ++ S1^S0x1^x2 + S1^S0x1x2 = ^S1S0^x1 + ^S1S0x1 + S1^S0^x1 + S1^S0x1 = ^S1S0 + S1^S0Они в точности совпадают с выражениями (3.2) из книги H&H.Рис. 5. Параллельный тест трех разных форм моделей светофораНа рис. 5 показаны результаты тестирования трех автоматных моделей управления светофоров: в форме автоматов Мура, Мили и цифровой схемы, реализующей автомат Бена. На карте кампуса для каждой модели выделена пара светофоров. Можно видеть, что их поведение синхронно и идентично.По поводу частоты синхронизации. Возможно, как частное решение, предложенное к тому же студентом, демонстрирующее особенности работы автоматов в дискретном времени, оно и имеет право на жизнь. Но, на мой взгляд, лучшим решением будет выбрать частоту синхронизации максимально возможной, а заданные моменты времени отсчитывать таймером. Это будет ближе к реальной практике, да и гибче с точки зрения развития алгоритма светофора. В свете подобной синхронизации вызывают возражение временные диаграммы сигналов, представленные на рис. 3.27 в H&H. Наклонные фронты сигналов при такой частоте выглядят весьма неправдоподобными. Декомпозиция автоматовСинтез цифровой схемы, реализующий автомат - есть декомпозиция на множество взаимодействующих автоматов, каждый из которых  представляет модель элементарного логического элемента. При этом простейшая автоматная модель любого логического элемента в рамках модели двоичных сигналов - это автомат с двумя состояниями (по числу состояний выхода). Таким образом, для схемы светофора на рис. 3.26 результат синтеза исходного автомата светофора на рис. 3 - это декомпозиция на девять взаимодействующих автоматов, каждый из которых имеет два состояния. Обратная операция по отношению к операции декомпозиции - операция композиции автоматов. С ее помощью для некоторого множества автоматов (такое множество в теории автоматов называется сетью автоматов или просто сетью) можно найти эквивалентный однокомпонентный автомат. В нашем случае это должен быть автомат на рис. 3. Немного подробнее о данных операциях (особенно про операцию композиции), которые часто ассоциируются с известными операциями деления и умножения, говорится в статье [4].Синтез цифровой схемы автомата - это интуитивно найденный подход к декомпозиции исходной модели на некое множество простейших автоматов. Посмотрим, что мы можем получить, если проведем декомпозицию исходного автомата на некое другое множество автоматов, имеющих в общем случае произвольное число состояний. Во-первых, сразу возникает вопрос -  какое минимальное число автоматов может входить в автоматную сеть, представляющую декомпозицию. Их число определяется достаточно просто - пространство состояний такой сети - произведение состояний компонентных автоматов. Оно должно покрывать, т.е. быть равно или больше, числу состояний исходного автомата. Для светофора с его четырьмя состояниями достаточно двух автоматов по два состояния. Но если хотите разобраться в деталях алгебры автоматов, включающих упомянутые операции, то лучших книг (да не обидятся наши бывшие теперь "не-друзья-партнеры") на тему декомпозиции/композиции автоматов, чем книги советских ученых С.И.Баранова[9] и А.Н.Мелихова [10],  я не знаю.Формально декомпозиция может включать любое число автоматов, как в случае сети, эквивалентной схеме автомата светофора - девять автоматов по два состояния. Здесь важно понимать, что взаимодействие компонентных автоматов сети в конечном итоге определит множество состояний, которое содержит эквивалентный сети автомат. Минуя теоретические препоны, приведем результат декомпозиции автомата на рис. 3 на два автомата. Пусть это будут автоматы A и B и, как уже сказано, содержащие по два состояния. Такая сеть приведена на рис. 6.Рис. 6. Результат декомпозиция автомата светофора (рис. 3) на два автоматаОтличие данной декомпозиции от стандартной в том, что последняя использует типовые логические элементы, а здесь компонентные автоматы нужно еще создать. Программно - нет проблем, а с аппаратной - нужно будет попотеть. Фактически это будет проблема, сравнимая по сложности с реализацией исходного автомата. В этом смысле ПЛМ хороши тем, что фактически содержат набор стандартных логических элементов, из которых, путем их соединения, создается сеть/схема, реализующая поставленную задачу.К чему мы вообще затронули тему декомпозиции/композиции автоматов? Как минимум, для информации. Чтобы на рассматриваемом примере продемонстрировать действия под "автоматным капотом": какие на самом деле с точки зрения теории автоматов решаются проблемы, когда выполняется процедура синтеза цифровой схемы автомата. Другими словами, синтез - синтезом, а об алгебре автоматов, хотя бы общее представление, необходимо иметь. Авторы книги H&H эти вопросы, вольно или нет, пропустили.  Для уровня техникума это еще как-то объяснимо и даже где-то оправдано, но когда речь идет о "вышке", то, на мой взгляд, это уже недостаток. Уж упомянуть-то надо было!Упражнение 3.29В процессе обсуждения предыдущей части статьи мы вышли на обсуждение упражнения 3.29 из книги. Меня в нем зацепил довольно нестандартная на мой взгляд постановка - синтез автомата по диаграмме сигналов. Я не припомню, чтобы когда-либо я сталкивался с подобным  в литературе по автоматам, которую я штудировал когда-то очень давно. Захотелось попробовать свои силы на решении хотя бы этой задачи. Попробовать, так сказать, пройти собеседование... Получилось, прямо скажем, не так чтобы успешно. Как минимум по времени, затраченного на ее решение. Наверное, собеседование я провалил... :(Неожиданно обсуждение прошло не без пользы и за это спасибо тем, кто в нем участвовал. Не сразу, но мы все же пришли к определенному решению, хотя недоговоренности остались. Но что-то мне подсказывает, что к итогу нас вывело решение, прилагаемое к книге. И  было бы хорошо, чтобы "решебник" был доступен, как и книга, хотя бы в оригинале. Но, с другой стороны, если решение было бы известно, то, скорее всего, самого обсуждения не получилось бы. А так ... все прошло, прямо скажем, достаточно живенько.Напомним упражнение:Диаграмма, приложенная к нему, следующая:Рис. 6. Диаграмма сигналов к упражнению 3.29 из книги H&HДля меня просто очевидно, что решение нужно начинать с конца, т.е. с проектирования автомата. Если есть автомат, то ответы на остальные вопросы будут уже очевидны. Для создания такого автомата у нас почти все есть, а то чего нет мы имеем право додумать.   А нет явно одного - не оговорено начальное состояние сигнала A. И это сразу вызывает "наивные вопросы" к авторам упражнения...Начинаем додумывать: мы можем ввести начальное состояние и из него, в зависимости от текущего значения сигнала A, перейти в состояние "1"  или "0", чтобы зафиксировать его значение. Далее совсем просто: мы переключаемся из состояния, которое отражает предыдущее значение сигнала A, в состояние, которое соответствует текущему значению сигнала A, а в процессе переходов выдаем сигнал Z в соответствии с логическими  уравнениями. Полученный в результате таких размышлений автомат показан на рис. 7.Рис. 7. Автомат для упражнения 3.29Автомат создан и, казалось бы, что еще надо? Подаем на входы автомата сигналы A и B и смотрим на выходе сигнал Z. Но не тут-то было...Какая связь между автоматом на рис. 7, да, и вообще любым другим предполагаемым автоматом, синхросигнала CLK? Да, ни какой (но это только мое мнение)! Но, ведь, зачем-то диаграмма тактового сигнала включена в общую диаграмму. Зачем? И это уже опять вопрос к авторам книги... Но именно диаграмма сигнала CLK осложнила выработку общего решения и внесла диссонанс в процесс обсуждения.Тем не менее, удалось получить доступ к авторскому решению. Автомат из него приведен на рис. 8. Не станем его комментировать, но зато есть на что обратить внимание. В этом автомате, что примечательно, отсутствует какая-либо привязка к сигналу CLK. Поэтому повторяем вопрос - зачем он присутствует в условии на диаграмме?  Думаю, что такое острое внимание к синхросигналу послужило бы причиной отрицательного решения по результату моего собеседования :(   А я бы не отстал ;)  Рис. 8. Авторское решение для упражнения 3.29Тем не менее, мы можем ввести синхросигнал в автомат и результат такого решения представлен на рис. 9. Диаграммы сигналов, приведенные на рис. 10, показывают его влияние на сигнал Z.  Заметим, что "жесткой" установкой сигнала CLK (точнее - предиката x3) в единицу (CLK = 1) мы приводим автомат на рис. 9 к автомату на рис. 7, а значение CLK = YES - к режиму работы автомата на рис. 9.Рис. 9. Автомат, учитывающий сигнал CLK Рис. 10. Влияние сигнала CLK на вид диаграммы сигнала ZНужно также обратить внимание на петли у автоматов на рис. 7 и рис. 9. Они здесь обязательны, т.к. есть переходы с установкой прямо противоположного значения сигнала Z. Это сигнал y2 на переходе в состояние "1" и сигнал y1 на переходе в состояние "0". Если бы их не было, то петли можно было бы без проблем исключить из описания автоматов. 6. ЗаключениеМожет, я ослабил контроль за автоматами, особенно в программировании, но пролистав статьи, в первую очередь на Хабре, обнаружил достаточно заметное число статей на тему автоматов даже за прошедший год. Явно есть большой интерес к данной теме. И мне это понятно, т.к. для меня альтернативы автоматам в программировании давно уже просто нет. Но, на мой взгляд, все или в основном почти все статьи и комментарии к ним отражают весьма простой, если не сказать - примитивный уровень проникновения в автоматы. Но, однако, давайте "полистаем" ...Вот статьи одного автора [11, 12]. Это вводный материал в тему минимального начального уровня. Чувствуется практический интерес к автоматам. Но... SWICH-технология? Даже много SWICH-технологии. Однако, на дворе 2023 год и вот-вот будет 2024-й, а это совсем не 1998 год - год издания книги А.А.Шалыто. Нынче она (SWITCH-технология) имеет отношение к автоматам, как лето к зиме: времена года, но общего мало. Тем не менее, спасибо А.А.Шалыто за подвижническую деятельность и активную рекламу автоматного программирования. Нынче у SWTCH-технологии общее с автоматным программированием только название (см. подробнее статью [4]). В статье [11], кстати, рассмотрена реализация светофора. Поэтому есть с чем сравнить. А статья [12] содержит включения из книги H&H. Приятная встреча! Правда, без ссылок на первоисточник, но мы этот непорядок исправляем. Есть в ней, кстати, даже ссылка на мою статью. А это еще приятнее, т.к. мой труд, похоже, не пропадает даром :) Но моя критика SWITCH-технологии автором явно не понята. В этом смысле его совет изучить SWICH-технологию только расстраивает... Статья [13]. Дает краткое представление о применении теории автоматов. Она заточена под оптимизацию цифровых схем, полученных процедурой синтеза автомата. Весьма специфичная тема и, думаю, будет интересна только математикам, занимающихся теми же генетическими алгоритмами. Может ошибаюсь, но полагаться на результаты подобной оптимизации, думаю, будет сложно. По мне так больше интересны комментарии к ней в части обсуждения проблем параллелизма. Какая же каша в головах!А вот пример большого разговора на тему автоматов на YouTube [14]. Лично я в этом видео мало что понял. Отстал, наверное, или текучка заела.  Но как инфа данное видео может кому-то   понравится, а что-то (правда, признаюсь, я не знаю что) даже пригодится. Да, чуть не забыл, книга А.А.Шалыто на одном из слайдов доклада все же промелькнула и здесь.Но в заключение вернем к нашим "баранам"... Посмотрите на рис. 5. Это программный проект, содержащий более 50-ти параллельных процессов. Не однотипных, а весьма  разных, взаимодействующих. Из них почти 40 - прикладные. Сюда входят СУ светофора в форме автомата Мили и автомата Мура, логические элементы, объединенные в схему реализации светофора, элементы графического интерфейса и т.д. и т.п. Все это обслуживает с той или иной степенью активности чуть больше десятка процессов ядра ВКПа. И все это - чистые автоматы. Они работают согласованно и дружно решают поставленную задачу/задачи. Это и есть автоматное программирование без изъятий, но здесь ... и близко нет SWITCH-технологии.  Подумайте, прочитайте в конце концов, если не читали, статью[4]. Не поняли - перечитайте ее еще раз, еще раз, может, даже еще раз, но включите наконец-то мозги... А то все switch, switch иногда case, if else... Это все и всегда будет про блок-схемы, а не про автоматы...Литература1.     Не спеши, Маша! Разбор примеров из книги Харрисон Д.М., Хариссон С.Л. Цифровая схемотехника и архитектура компьютера. 2.     Проектирование цифровых вычислительных машин. Под ред. С.А. Майорова. Учебное пособие для студентов вузов. - М.: «Высшая школа», 1972. – 344 с.3.     Глушков В.М. Синтез цифровых автоматов. М.: Физматгиз, 1962.4.     Автоматное программирование: определение, модель, реализация. [Электронный ресурс], Режим  доступа: https://habr.com/ru/articles/682422/ свободный. Яз. рус. (дата обращения 21.11.2023).5.     Проектирование цифровых вычислительных машин. Под ред. С.А. Майорова. Учебное пособие для студентов вузов. - М.: «Высшая школа», 1972. – 344 с.6.     Минский М. Вычисления и автоматы. М.: Мир, 1971. – 364 с.7.     Джон фон Нейман. Теория самовоспроизводящихся автоматов. М.: Мир, 1971 – 382с.8.     Тоффоли Т.,  Марголус Н. Машины клеточных автоматов: Пер. с англ. – М.: Мир, 1991. – 280 с.9.     Баранов С.И. Синтез микропрограммных автоматов. -Л.: Энергия, 1979. -232с.10.  Мелихов А.Н. Ориентированные графы и конечные автоматы. – М.: Наука, 1971. – 416 с.11.  OldFashionedEngineer. На полпути к конечному автомату для Arduino. Однопроходные функции и фиксация событий программы с помощью флагов. https://habr.com/ru/companies/timeweb/articles/719376/12.  OldFashionedEngineer. С чем едят конечный автомат. https://habr.com/ru/companies/timeweb/articles/717628/13.  Efi-fi. Оптимизация цифрового автомата (FSM). https://habr.com/ru/articles/530414/14.  Кирилл Мокевнин, Конечные автоматы как способ значительно упростить логику и понимание кода. https://www.youtube.com/watch?v=knoVv2ncwVI15.  Хэррис Д.М., Хэррис С.М. Цифровая схемотехника и архитектура компьютера. - 2-е изд. PSКогда уже практически закончил свой мини-обзор, то на YouTube обнаружил канал - Школа синтеза цифровых схем (https://www.youtube.com/@chipdesignschool). Что сказать? - Молодцы и спасибо за выложенные видео! Есть, что изучать, с чем знакомиться и, возможно, даже ... критиковать :)    
--------------------------
Ссылка: https://habr.com/ru/articles/780160/
Самый лучший тренажёр клавиатуры
oleg_rico  (https://habr.com/ru/users/oleg_rico/)
 Меню вызывается клавищей escБомбочка в виде буквы О падает. Нужно успеть сбить. Громкий заголовок, но оно того стоит. Умение печатать сейчас значит больше, чем уметь написать что-то от руки. Впрочем, слышал, что в некоторых странах уже и не учат писать, только печатные буквы:)К теме - много лет назад думалось, что вот-вот, когда компы станут обыденностью, компьютерные игрушки изменятся кардинально, и бОльшая их часть будет не развлекательными, а обучающими. В игровой форме позволяющими получить полезные для жизни или работы навыки. Понятно, что люди, так думающие, ошибались. Но попытки создать подобные проги предпринимались. Одна из таких, старых-престарых прог, называется просто - TRK. Или, по-простому, тренажер клавиатуры. Да-да, скажите сейчас вы, этих тренажеров как говна за баней, и будете правы. Но мы же говорим о самом-самом. Итак, у вас есть клавиатура, на которой надо разместить свои руки и начинается все с "фыва олдж", как и положено. Но главная фенечка тренажера, что каждая буква, которую нужно нажать, это падающая с неба бомбочка, а нажатие на клавишу сбивает её и позволяет продолжить игру.  В промежутках между бомбёжками идут упражения, и когда уровень ваших неправильных нажатий превышает заданный предел, игра предлагает вам отдохнуть. То есть в игровой форме, а я уверен, что в игру, где надо сражаться с пришельцами, сбивая падующие сверху бомбочки, играли многие, тренажер помогает освоить умение печатать быстро и ненапряжено.Понятное дело, что постепенно добавляются новые буквы, а потом и цифры и знаки препинания. И всё. Включается моторный навык и вы уже освоили так называемый "десятипальцевый слепой метод" печати, на обучение которому во времена печатных машинок уходили недели или даже месяцы. А здесь буквально дни. Я лично научился печатать за три дня, точнее, освоил русскую клавиатуру, а после освоение латинской раскладки заняло буквально часы - ну чего там учить после наших 33 трех букв?Потом применял этот тренажер для обучения сотрудников и всегда получались отличные результаты. Увы, сейчас прога устарела и требует для использования "плясок с бубном", так как надо установить эмулятор DOS и уже под ним запускать игру.  Но даже так раскладка в знаках препинания на русском не совпадает с раскладкой виндоус и поэтому уже не подходит для обучения. Жаль, что тема игрового обучения полезным навыкам так и не получило должного распространения. Надеюсь, было интересно узнать о такой игрушке...       Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста. Как вы научились печатать "вслепую"? 
            28.13%
           Просто учился на компе 
            18
           
            3.13%
           Еще в школе на печатной машинке 
            2
           
            0%
           На таком же тренажере 
            0
           
            40.63%
           На другом тренажере 
            26
           
            0%
           Играл в клавогонки 
            0
           
            28.13%
           Печатаю двумя пальцами и мне хватает 
            18
            
       Проголосовали 64 пользователя.  

       Воздержались 7 пользователей. 
    
--------------------------
Ссылка: https://habr.com/ru/articles/780158/
Онто: От идеи до реализации
Ontonet  (https://habr.com/ru/users/Ontonet/)
 Привет, с вами на связи команда Онто. Продолжая и детализируя нашу предыдущую статью, я хочу поделиться с вами, как мы в Онто превращаем идеи в реальность.Онто - это платформа, где сочетаются элементы Excel и Miro, созданная для того, чтобы сделать моделирование доступным для всех. Мы предоставляем возможность "мыслить вместе", используя простоту Excel и визуальные возможности безграничных досок Miro. Платформа включена в реестр российского ПО Минцифры, а мы, как команда, являемся финалистами ФРИИ и участниками ТОП-1000 сильных идей нового времени.Как владелец продукта, я глубоко убежден в важности внимания к каждой идее и предложению. Предложение - это любая идея, вопрос или замечание от пользователей или команды, которое может улучшить наш продукт или процесс работы. Я верю, что успешные инновации возможны благодаря диалогу и обмену мнениями. Понимание потребностей и проблем наших пользователей - ключ к развитию продукта, который помогает людям в их работе.  Для меня, как владельца продукта Онто и человека, стоящего за каждым решением, важность слушать каждую идею и предложение по улучшению  от пользователей, со стороны или от команды глубоко укоренена в моих убеждениях. Я верю, что именно в диалоге и обмене мнениями кроется секрет успешных инноваций. Каждая идея, какой бы незначительной она не казалась на первый взгляд, может стать ключом к следующему большому прорыву в нашем продукте. Слышать предложения от наших пользователей и команды - это не просто часть работы, это возможность для меня, как для профессионала, расти и развиваться. Это позволяет мне видеть Онто глазами наших пользователей, понимать их потребности и проблемы. Только так мы можем развивать продукт, который действительно помогает людям, делает их работу проще и эффективнее.  Передавая эти мысли команде, я стремлюсь создать среду, где каждый член команды чувствует свою значимость и влияние на конечный продукт. Это способствует не только развитию продукта, но и формированию крепкого и синергичного коллектива. Каждый в команде знает, что его мнение важно и что он может внести свой вклад в улучшение Онто.  Таким образом, процесс принятия и реализации предложений в Онто для меня - это больше, чем просто управление. Это возможность для совместного творчества, для построения чего-то ценного и важного, что будет служить не только нашей компании, но и всему сообществу пользователей Онто.  Как же мы управляем потоком идей?  Мы используем диаграммы и шаблоны Онто для визуализации и структурирования идей. Диаграммы позволяют нам наглядно представить каждую идею, а шаблоны помогают нам присваивать идеям атрибуты и свойства. Каждое предложение проходит оценку на соответствие нашей концепции и философии, после чего классифицируется как «обработано» или «отклонено». Отклоненные идеи сохраняются в нашей базе знаний.  Управляемость этого процесса крайне важна для нас и позволяет не только отслеживать каждую идею, но и гарантирует, что каждое предложение будет рассмотрено и оценено с должным вниманием. Мы стремимся к тому, чтобы каждая идея способствовала развитию Онто и приносила ценность нашим пользователям.  Пользовательская история формируется на основе каждого предложения. Эти истории затем используются для разработки новых функций. Мы стараемся вовлечь всю команду в процесс формулирования историй, чтобы учесть разные точки зрения. Например, запрос на поиск объекта не только по имени, но и по комментариям или значениям полей, может превратиться в несколько пользовательских историй.  Пользовательская история«Хочу искать объект не только по имени, но и по комментарию, и по значениям полей» Как мы видим из такого предложения можно сформулировать как минимум 3 истории: «Я, как пользователь, использующий сайдбар для навигации и поиска, хочу иметь возможность производить поиск элемента по содержимому поля "комментарий", чтобы находить объекты на основе комментария к ним.» «Я, как пользователь, использующий сайдбар для навигации и поиска, хочу иметь возможность производить поиск элемента по содержимому шаблонного поля, чтобы находить объекты на основе шаблонного поля к ним.» «Я, как пользователь, использующий сайдбар для навигации и поиска, хочу иметь возможность производить поиск элемента по содержимому поля объекта, чтобы находить объекты на основе полей к ним.» Связи в Онто позволяют создавать сложные структуры и отношения между объектами. Это помогает нам в организации и управлении разработкой. Результатами планирования у нас всегда остаются не просто цветные картинки, но связанная модель объектов с возможностями навигации по ней. Что же такое связи в Онто?Связи позволяют создать сложные структуры и отношения между объектами.  Как итог, все одобренные предложения имеют свои пользовательские истории, и чтобы не потерять цепочку, мы используем связи.  В нашем случае это будет выглядеть так:  После того как пользовательская история сформулирована, мы её отправляем на нашу User Story Map. У каждой пользовательской истории есть свой путь развития, но об этом мы расскажем в другой статье.  А расставляем приоритеты мы всегда совместно с командой, вместе выбираем пользовательские истории для работы. На основе этих историй формулируются “возможности” для пользователей и цели у спринтов. Мы работаем в спринтах, что позволяет нам быстро адаптироваться к изменениям и эффективно реализовывать новые идеи.  Продолжая наш пример мы формулируем “возможность пользователя”  На основе выбранной пользовательской мы можем сформулировать “возможность пользователя”, реализацию которой хотим взять в работу. Давайте рассмотрим один из примеров:  «Я, как пользователь, использующий сайдбар для навигации и поиска, хочу иметь возможность производить поиск элемента по содержимому поля "комментарий", чтобы находить объекты на основе комментария к ним» формирует “Возможность” «Производить поиск по содержимому поля "комментарий"».  Цель спринтаКаждая “Возможность” сопровождается целью. В чём же разница? “Возможность” - это функциональность или серия связанных функций, которые предоставляют пользователю способ что-то сделать, тогда как цель — это шаг или шаги к реализации. Цель мы можем поставить перед командой на спринт и идти к её реализации.  *Что же такое спринт в Онто?Мы Agile-команда, и выбор ведения разработки в спринтах для нас очевиден – это временной интервал, в течение которого команда выполняет заданный объем работ.  В нашем случае всегда есть 2 цели: первая цель — это передать в тест и следующая цель —  релиз. Да, это - в идеале, и не всегда получается так, как задумывалось. Но мы работаем в гибкой системе, и поэтому можем подстроиться под меняющиеся обстоятельства - сформулировать еще одну или несколько промежуточных целей.  В результате, у нас есть четкая и гибкая модель разработки, которая позволяет нам реализовывать идеи с минимальными рисками и максимальной пользой для продукта.  Перед началом каждого спринта мы проводим планирование, чтобы определить, какие функции и задачи будут реализованы.  Архитектурный Скетч или Дизайн Решение  Каждая пользовательская история декомпозируется на задачи. На этапе аналитической работы и создания скетча активно участвуют наши аналитики и архитекторы. Архитектурный скетч - это визуальное представление того, как функции будут интегрированы в Онто, показывая взаимодействие между различными частями системы. Это помогает команде разработчиков, дизайнеров UI/UX и тестировщиков видеть общую картину и понимать свои роли и задачи в процессе разработки. Скетчу я уделил много внимания в предыдущей статье. Это один из наиболее важных элементов планирования, тут возникают задачи.  На этом этапе задачи для команды Онто сформулированы, и зачастую мы готовы брать “Возможность” в работу.  Планирование СпринтаПосле формулирования задач и создания архитектурного скетча мы переходим к планированию спринта. Здесь мы определяем, какие возможности  будут разработаны и какие цели должны быть достигнуты в ближайшем спринте. Спринт в Онто - это временной интервал, в течение которого команда выполняет заданный объем работ, направленный на реализацию выбранных пользовательских историй.  Реализация и ТестированиеКогда спринт запланирован, команда приступает к реализации функций. После разработки функции она передается в тестирование, где тестировщики проверяют её на соответствие пользовательским историям и требованиям. Тестировщики используют архитектурный скетч как руководство для понимания того, как функции должны взаимодействовать и какие изменения были внесены.  ЗаключениеНа этом этапе наша разработка приобретает конкретные формы, и мы видим, как идеи превращаются в реальные функции и возможности для пользователей Онто. Этот процесс не только помогает нам быстро и эффективно реализовывать новые идеи, но и обеспечивает гибкость и адаптивность нашего процесса разработки. Наша цель - постоянно улучшать Онто, делая его более интуитивным, функциональным и удобным для всех пользователей.  Спасибо, что остаетесь с нами и следите за нашими новостями и развитием. Подписывайтесь на наш канал Telegram для получения последних обновлений.  Добро пожаловать в мир Онто, приходите и оставайтесь с нами!    
--------------------------
Ссылка: https://habr.com/ru/articles/779926/
Что использовать вместо Jira и Confluence
YusmpGroup  (https://habr.com/ru/users/YusmpGroup/)
 Продукты австралийской Atlassian — Jira и Confluence были удобными и надежными инструментами для многих российских команд. Но с 2022 года компания стала сворачивать деятельность в России, а к осени этого года доступ к продуктам для российских аккаунтов полностью заблокирован.Чем заменить Jira? По такому запросу в поисковой строке много сравнительных обзоров разной степени подробности, но мало пользовательских мнений. В статье взяли популярные российские альтернативы Jira (отечественные — чтобы точно не надумали уйти вслед за Atlassian). Дали емкий обзор каждому инструменту, привели комментарии от пользователей и, конечно, сделали выводы.Yandex TrackerЯндекс Трекер — первый сервис, который упоминают как российскую альтернативу Jira. В этой системе каждая задача имеет свою страницу с полной информацией: от описания и дедлайнов до ответственных лиц и связанных задач. Трекер подходит для разнообразных проектов, будь то разработка приложений, рекламные кампании или обработка клиентских запросов. В основе инструмента лежит управление задачами и очередями: задачи определяют, что нужно сделать, а очереди – как именно это делать. Кроме того, есть дополнительные инструменты, такие как компоненты, доски задач и информационные панели, которые помогают отслеживать статус задач и общие результаты работы. Также предлагается аналог Confluence в одной экосистеме, а API дает возможность для интеграций. Специалисты обещают помочь с миграцией данных.Тарифы. Можно пользоваться бесплатно, если добавите не больше 5 пользователей. Далее цены начинаются от 360 рублей в месяц за каждого сотрудника.Егорпроджект-менеджер из YuSMP Group                                                   Я.Трекер ощущается как Jira сильно ограниченная по функционалу. Также у них достаточно неочевидные настройки задач и воркфлоу, которые применяются к задачам.Это очень непривычно – их уникальные решения выглядят не как переосмысленный инструмент, а скорее как уникальность ради уникальности. Хороший пример — «Очереди». Презентуются они как новый функционал для настройки задач по всем проектам в одном месте.Но на деле это сильно урезанные бизнес-процессы из Jira. И когда я пользовался — очередь нельзя было применять в другой доске и смысла в ней не было никакого. Из плюсов — создание почтовых отбивок и прочие системные настройки достаточно простые, хорошо работает интеграция с инфраструктурой Яндекса. Интерфейс не перегружен, почти всё работает через drag’n’drop.Также когда я пользовался — было много багов, подозреваю, что сейчас их поправили и мой негатив не актуален. Из российских альтернатив я бы перешел на Трекер и не стал бы искать другие отечественные варианты.EvaTeamЭкосистема инструментов, которая стремится полностью закрыть функционал ушедшей Atlassian.EvaProject: альтернатива Jira для управления проектами, интегрируется с GitLab/GitHub и Active Directory и предлагает аналоги любимых плагинов.EvaWiki — замена Confluence. Здесь вы получаете полную свободу действий с вашими документами, создаете базы знаний, управляете версиями и делитесь ими, как вам угодно.EvaTest предлагает инструменты для ручного и автоматического тестирования — подобно плагину Zephyr для Jira.EvaServiceDesk отечественный аналог Jira Service Management, который помогает обрабатывать заявки из разных источников в одном месте.EvaGit предполагает замену Bitbucket.EvaCI — аналог Bambo.Еще Eva.team облегчает миграцию данных из Jira. Тарифы начинаются от 573 рублей в месяц за одного пользователя и сильно отличаются в зависимости от “начинки” подписки. Таблица сравнения EvaProject и Jira SoftwareКонстантин, QA из Solit Clouds сделал емкий, но полезный обзор EvaTeam. Правда, в личном общении признался, что в работе пользуется все-таки другим инструментом. Приводим цитату из материала автора.Конечно, я не в состоянии полноценно оценить заявленную разработчиками EvaTeam 100% гарантию замещения всех функций Atlassian. Здесь уже нужно оценивать со стороны специалистов по внедрению в вашей компании. Но, как обычный пользователь, я действительно не заметил никакой разницы в возможностях и увидел лишь косметические отличия на любителя.KaitenKaiten представляет собой инструмент для управления проектами, предлагая функционал, который может быть полезен для разных команд. Объединяет в себе проекты, задачи, документы, переписки, отчеты, заявки.Чем примечателен сервис:Адаптация под разные методологии: Kaiten может быть настроен для работы с различными методами управления проектами, такими как Agile и Scrum, Канбан, что дает гибкость в применении. Есть Диаграмма Ганта.Интеграция с другими инструментами: Github, GitLab, Telegram, Slack и календарями. Имеет открытый API для подключения сторонних интеграций.Отчетность и аналитика, учет времени.Индивидуальные настройки: предлагает возможность настроить рабочие процессы и задачи в соответствии с конкретными требованиями и предпочтениями команды.Также есть Автоматический перенос задач из Jira и Trello.Из минусов пользователи отмечают частые сбои в работе и слабую техподдержку, которая не решает оперативно вопросы.По ценам. Есть бесплатная версия с урезанным функционалом. Платные тарифы стартуют от 420 рублей в месяц за сотрудника.Александр, исполнительный директор веб-студии Пиробайт, в своей статье поделился опытом перехода на Kaiten. Я выбирал между Kaiten и еще двумя продуктами: YouTrack, Toggl. YouTrack — мегакрут. И для нас оказался слишком перегружен. Из удобного: логирование времени из IDE, то есть разработчику даже не надо переключаться между сервисами, достаточно сносные диаграммы Ганта. Отчеты не удовлетворяли наш запрос, поэтому после двух спринтов разработки решили отказаться от этого сервиса. Возможно, когда-то к нему вернемся :)Toggl — достаточно простой сервис для учета времени. Нажал на кнопочку и погнали. Также можно логировать уже затраченное время. Не было адекватной доски задач. Отказались. Но позже к нему вернулись и использовали связку Trello + Togll, пока не наступил исход иностранных сервисов.Сразу скажу, что мы не собрали все вещи и переехали в новый инструмент всей компанией. Сначала решили протестировать это с командой добровольцев, перенеся их работу в Kaiten. Когда убедились, что ребята могут использовать нужные функции и работают с комфортом, мы подтянули все остальные команды.Сегодня для каждого нового проекта у нас формируется отдельная проектная группа, которую ведет менеджер. Но, что важно, менеджер может вести одновременно 4–5 проектов.Почему так сложно повторить Jira Jira легко приспосабливается под любые потребности и методологии умеет работать с кучей других инструментов, и поэтому легко впишется в любую рабочую экосистему. Неважно, маленькая команда или целая корпорация — Jira подходит всем, масштабируясь под любые задачи и объемы работы. Сложность: Jira — это целая вселенная функций и опций. Создать что-то подобное — это как построить свой космический корабль. Нужно много времени, усилий и ресурсов.Интеграции — это искусство: Jira умеет работать с множеством других систем. Это требует технической изобретательности и постоянной работы над новыми «мостиками».Atlassian не дремлет, постоянно улучшая Jira. Так что, копируя её, придется бежать за постоянно ускоряющимся поездом обновлений.Безопасность и надежность на высоте: держать высокую планку в области безопасности и надежности для корпоративных клиентов — задача не из легких.Да, кому-то Jira могла показаться перегруженной, но если бизнес растет и развиваются, появляются новые потребности в управлении и Джира их отлично закрывала.Сейчас компаниям приходится искать что-то конкретное под текущие процессы: простую канбан-доску и отчеты могут закрыть многие отечественные продукты. Для работы с крупными проектами (особенно если их сразу несколько) лучше сразу искать решения, в которых есть возможность масштабироваться.В процессе подготовки материала мы пересмотрели множество отзывов на разные сервисы и поняли, что сложнее всего адекватно переехать и привыкнуть к новому продукту, при этом еще приходится сталкиваться с багами и общаться с поддержкой чаще, чем хотелось бы. Хотелось бы верить, что последнее — абсолютно временное явление.А в целом, кажется, большинство команд точно найдут достойную замену Atlassian среди отечественных аналогов.    
--------------------------
Ссылка: https://habr.com/ru/articles/780156/
Создаём графический информер на PHP
alexeyinkin  (https://habr.com/ru/users/alexeyinkin/)
 Это моя первая статья от 29 сентября 2004 года, ценность в основном историческая. Публикую её здесь, чтобы она не потерялась, потому что оригинальный сайт удалён. Подтверждение даты здесь.Картинка, видимо, от Евгения Жданова, редактора журнала "Протоплекс" -- прим. автора, 2023 г.Глядя на счётчик посещений mail.ru, или на какой-либо другой информер, вы наверняка задумывались о том, как бы сделать такой самому. Эта статья поможет вам осуществить ваши планы. Всё, что для этого понадобится — хостинг с поддержкой PHP и знание основ этого языка. Если же вы сомневаетесь в необходимости информера, значит, у вас, наверное, индекс цитирования и посещаемость — соответственно пяти- и семизначные числа. Потому что даже Яндекс уделяет таким вещам внимание. Ведь это, по сути, бесплатная реклама, которая при этом не злит посетителей!Почему именно PHP? Он значительно проще Perl´а, особенно в отладке, программы на этом языке менее капризные и практически не зависят от набора установленных дополнительных программ.Функции PHP для работы с графикой находятся в отдельной библиотеке GD. Скорее всего, она входит в дистрибутив вашей версии языка, однако не помешает выполнить следующий сценарий, чтобы это проверить:<?php if (function_exists('imageline')) print 'есть'; else print 'нет' ?>function_exists возвращает true, если функция с заданным именем определена, в данном случае мы проверяем наличие одной из тех самых графических функций, которые нам понадобятся. Если сценарий показал, что библиотека отсутствует, вам нужно будет скачать её отсюда: http://www.boutell.com/gd/. Вернёмся к нашей теме. Я распишу процесс создания информера на двух примерах: счётчик загрузок программ и информер оценки программы в форуме (оба используются у меня на GetSoft.ru). Начнём с первого из них. Нам понадобится изображение-заготовка, на котором будем рисовать числа. Предполагается, что у нас уже есть картинка с размерами 88x31 пиксел (стандартный формат для таких вещей). У себя я использовал формат PNG, хотя вы можете выбрать какой-нибудь другой. Библиотека поддерживает работу с GIF, JPEG, PNG, BMP и ещё несколькими. Кстати, поддержка GIF и PNG в некоторых версиях может отсутствовать, поэтому рекомендуется перед выбором формата проверить, будет ли библиотека с ним работать. Используйте сценарий, показанный выше, заменив название функции на imagecreatefrompng или imagecreatefromgif.В нижней части этой картинки мы будем выводить числа. Создайте сценарий следующего содержания (у меня он называется counterimage.php):<?php

$hImage = imagecreatefrompng('image.png');
header('Content-type: image/png');
imagepng($hImage);

?>Первая строка загружает изображение в память, возвращая дескриптор, по которому с ним можно будет работать. Вторая посылает браузеру заголовок о том, что далее будет следовать PNG-рисунок. Третья строка преобразует изображение с указанным дескриптором в выбранный формат и посылает его клиенту. Между первой и третьей строками с загруженной картинкой можно делать всё, что угодно, в частности мы будем рисовать на ней цифры. Есть несколько вариантов использованных нами графических функций, отличающихся окончаниями, например, imagecreatefromjpeg или imagewbmp. Справку по ним вы можете найти в руководстве по PHP, задав поиск фразы “image functions”.Если у вас установлен и настроен вебсервер, вы можете уже сейчас проверить работоспособность этого сценария. Для этого поместите его в папку с документами сервера и наберите в браузере:http://localhost/counterimage.phpЕсли всё сделано правильно, вы должны увидеть ваш рисунок. Если нет, то причин может быть множество. Проверьте, работает ли вообще сервер, набрав http://localhost/, проверьте, куда вы положили скрипт и рисунок (в нашем примере они должны лежать в одной папке). А вы вообще проверяли, установлена ли GD :) ?Библиотека GD позволяет печатать текст на рисунке, однако, чтобы не зависеть от различных шрифтов, мы будем выводить цифры вручную, используя функции imagesetpixel и imageline.int imagesetpixel (resource image, int x, int y, int color)
int imageline (resource image, int x1, int y1, int x2, int y2, int color)Первым аргументом обеих функций должен быть дескриптор изображения (который был возвращён imagecreatefrompng), последним — цвет, которым будем рисовать. Способ его задания будет различным в зависимости от того, как цвет представляется в изображении. Если у нас 24-битный рисунок, то color интерпретируется так:Синий + Зелёный * 256 + Красный * 256 * 256.Если в изображении 8 бит, т.е. оно использует палитру, то color будет означаать индекс в ней. Если честно, мне не хотелось возиться с палитрой, поэтому все информеры у меня на сайте используют 24-битный PNG, хотя некоторые читатели могут обвинить меня в глупой растрате памяти и трафика.Смысл промежуточных аргументов, думаю, понятен. Для imagesetpixel это координаты закрашиваемой точки (0, 0 означает верхний левый угол, если кто забыл :). Параметры imageline указывают координаты точек, соединяемых линией. К счастью, ребята из Microsoft к библиотеке GD руки не прикладывали, поэтому линии рисуются нормально. (В Windows GDI последняя точка линии не закрашивается.)Теперь нужно создать функцию, которая будет вырисовывать цифры. Функции в PHP объявляются в любом месте программы, но лучше не выкрутасничать, а сделать это сразу после “<?php”. Вставьте туда следующий код:<?php

function PrintChar(
    $hImage,
    $strChar,
    $x,
    $y,
    $nColor
) {
    switch ($strChar) {
        case '0':
            imageline($hImage, $x, $y + 1, $x, $y + 4, $nColor);
            imageline($hImage, $x + 1, $y + 5, $x + 2, $y + 5, $nColor);
            imageline($hImage, $x + 3, $y + 4, $x + 3, $y + 1, $nColor);
            imageline($hImage, $x + 2, $y, $x + 1, $y, $nColor);
            break;
        case '1':
            imageline($hImage, $x + 2, $y, $x + 2, $y + 5, $nColor);
            imagesetpixel($hImage, $x + 1, $y + 1, $nColor);
            break;
        case '2':
            imagesetpixel($hImage, $x, $y + 1, $nColor);
            imageline($hImage, $x + 1, $y, $x + 2, $y, $nColor);
            imagesetpixel($hImage, $x + 3, $y + 1, $nColor);
            imageline($hImage, $x + 3, $y + 2, $x, $y + 5, $nColor);
            imageline($hImage, $x + 1, $y + 5, $x + 3, $y + 5, $nColor);
            break;
        case '3':
            imageline($hImage, $x, $y, $x + 3, $y, $nColor);
            imagesetpixel($hImage, $x + 3, $y + 1, $nColor);
            imageline($hImage, $x + 1, $y + 2, $x + 2, $y + 2, $nColor);
            imageline($hImage, $x + 3, $y + 3, $x + 3, $y + 4, $nColor);
            imageline($hImage, $x + 2, $y + 5, $x + 1, $y + 5, $nColor);
            imagesetpixel($hImage, $x, $y + 4, $nColor);
            break;
        case '4':
            imageline($hImage, $x + 2, $y + 4, $x, $y + 4, $nColor);
            imageline($hImage, $x, $y + 3, $x + 3, $y, $nColor);
            imageline($hImage, $x + 3, $y + 1, $x + 3, $y + 5, $nColor);
            break;
        case '5':
            imageline($hImage, $x + 3, $y, $x, $y, $nColor);
            imagesetpixel($hImage, $x, $y + 1, $nColor);
            imageline($hImage, $x, $y + 2, $x + 2, $y + 2, $nColor);
            imageline($hImage, $x + 3, $y + 3, $x + 3, $y + 4, $nColor);
            imageline($hImage, $x + 2, $y + 5, $x, $y + 5, $nColor);
            break;
        case '6':
            imageline($hImage, $x + 3, $y, $x + 1, $y, $nColor);
            imageline($hImage, $x, $y + 1, $x, $y + 4, $nColor);
            imageline($hImage, $x + 1, $y + 5, $x + 2, $y + 5, $nColor);
            imageline($hImage, $x + 3, $y + 4, $x + 3, $y + 3, $nColor);
            imageline($hImage, $x + 2, $y + 2, $x + 1, $y + 2, $nColor);
            break;
        case '7':
            imagesetpixel($hImage, $x, $y + 1, $nColor);
            imageline($hImage, $x, $y, $x + 3, $y, $nColor);
            imagesetpixel($hImage, $x + 3, $y + 1, $nColor);
            imageline($hImage, $x + 2, $y + 2, $x + 2, $y + 3, $nColor);
            imageline($hImage, $x + 1, $y + 4, $x + 1, $y + 5, $nColor);
            break;
        case '8':
            imagesetpixel($hImage, $x, $y + 1, $nColor);
            imageline($hImage, $x + 1, $y, $x + 2, $y, $nColor);
            imagesetpixel($hImage, $x + 3, $y + 1, $nColor);
            imageline($hImage, $x + 2, $y + 2, $x + 1, $y + 2, $nColor);
            imageline($hImage, $x, $y + 3, $x, $y + 4, $nColor);
            imageline($hImage, $x + 1, $y + 5, $x + 2, $y + 5, $nColor);
            imageline($hImage, $x + 3, $y + 4, $x + 3, $y + 3, $nColor);
            break;
        case '9':
            imageline($hImage, $x, $y + 5, $x + 2, $y + 5, $nColor);
            imageline($hImage, $x + 3, $y + 4, $x + 3, $y + 1, $nColor);
            imageline($hImage, $x + 2, $y, $x + 1, $y, $nColor);
            imageline($hImage, $x, $y + 1, $x, $y + 2, $nColor);
            imageline($hImage, $x + 1, $y + 3, $x + 2, $y + 3, $nColor);
            break;
    }
    return 5;
}Эта функция у нас будет выводить одну цифру. Первый параметр — дескриптор изображения, второй — символ, который надо вывести (здесь будут поддерживаться только числовые). Затем координаты его верхней левой точки и цвет (в таком виде, в котором он будет передан в графические функции). Возвращаемое значение — количество пикселов, на которое позиция вывода сдвинулась вправо. Это даёт возможность в будущем сделать символы переменной ширины, но сейчас мы этим пользоваться не будем. Функция рисует цифры в соответствии со шрифтом, сочинённым мною в перерывах от работы:Вы можете придумать любой другой шрифт, если вам не нравится GetSoft Sans Serif:) На этом рисунке крестики — начала координат для каждой цифры, жёлтые точки — начало рисования, синие линии — переходы мнимой кисти, красные — рисование. Читатели, интересующиеся содержанием, а не оформлением этой статьи, простят мне этот кривой рисунок в Paint.Теперь осталось создать функцию, которая будет получать строку и для каждого символа в ней вызывать PrintChar:<?php

function PrintLine(
    $hImage,
    $str,
    $x,
    $y,
    $nColor
) {
    $nLength = strlen($str);

    for ($i = 0; $i < $nLength; $i++)
        $x += PrintChar($hImage, substr($str, $i, 1), $x, $y, $nColor);
}Все пять аргументов этой функции совпадают с теми, что мы использовали в предыдущей. Разница лишь в том, что x и y здесь означают координаты начала вывода всей строки. Несколько слов об использованных стандартных функциях.int strlen (string str)
string substr (string string, int start [, int length])strlen принимает строку и возвращает её длину. substr вырезает фрагмент, принимая строку, его начало и длину (если последняя не указана, используется вся строка до конца). Таким образом, мы вызываем PrintChar для каждого символа в строке, при этом всякий раз сдвигая позицию вывода вправо путём приращения x (напомню, что эта наша функция возвращает ширину выведенного символа).Остаётся только проверить работу всего кода, который мы написали. Для этого перед той строкой с вызовом imagepng вставьте: PrintLine($hImage, '0123456789', 10, 22, 0);Этот вызов должен напечатать одну за другой все цифры, возможные в нашем информере. Координаты (10, 22) подобраны методом тыка и скорее всего будут отличаться в вашем случае. Снова наберите в браузере http://localhost/counterimage.php, после чего вы должны увидеть такой рисунок:Вторым показан реальный работающий счётчик с моего сайта. Там кроме того, что мы сейчас сделали, ещё выводится знак “+”, а также текст может печататься с выравниванием по правому краю. Чтобы это реализовать, нужно немного доработать PrintLine. Думаю, с этим вы справитесь самостоятельно. Теперь осталось только заставить наш сценарий выводить нужное число, например, количество загрузок, посетителей, подписчиков и т.д. Техническая сторона этого дела полностью определяется структурой конкретного сайта, так что расписать всё это я не смогу. Скажу лишь, что нужно заменить “0123456789” на нужную переменную.Информация, которую будет показывать наш сценарий, скорее всего должна зависеть от какого-нибудь параметра, если только ваш информер не выдаёт фиксированные данные, например погоду у нас в Нижнем Новгороде (и только в нём). Для этого скрипту нужно передать один или несколько аргументов. Вызываться при этом он будет так:http://localhost/counterimage.php?param1=value1?param2=value2В каждом сценарии есть предопределённый ассоциативный массив $_GET, в котором хранятся все переданные аргументы. Когда вам нужно будет получить их значения, сделать это можно будет следующим образом:<?php
$a = $_GET{'param1'}; // $a = "value1"Рассмотрим информер с другим способом подачи информации, который тоже применяется у меня на сайте. Он будет показывать оценку программы в форуме (количество звёзд от 1 до 5) или надпись “нет оценки”. Отличие от предыдущего примера заключается в том, что изображения звёзд сохранены в отдельных файлах. Мы будем их копировать на исходное изображение. Ниже приведены рисунки, соответственно star0.gif, star1.gif, notgraded.gif, image.png. Их мы и будем комбинировать.Рассмотрим одну новую функцию:int imagecopy (
    resource dst_im,
    resource src_im,
    int dst_x,
    int dst_y,
    int src_x,
    int src_y,
    int src_w,
    int src_h
)Первые два аргумента — дескрипторы двух изображений, соответственно приёмника и источника. Следующая пара аргументов — координаты, показывающие куда надо копировать фрагмент. Затем идут координаты копируемого куска на изображении-источнике, последние два параметра — его ширина и высота. Тут следует заметить, что наши GIF-картинки содержат прозрачные участки. К счастью, функция достаточно умна, чтобы корректно их обработать, а не копировать весь заданный прямоугольник, так что волноваться не следует. Используем следующий нехитрый код:<?php

$hImage = imagecreatefrompng('image.png');

if ($nMark) {
    $hImageStar0 = imagecreatefromgif('star0.gif');
    $hImageStar1 = imagecreatefromgif('star1.gif');
    $x = 35;

    for ($i = 0; $i < $nMark; $i++, $x += 10)
        imagecopy($hImage, $hImageStar1, $x, 8, 0, 0, 10, 10);

    for (; $i < 5; $i++, $x += 10)
        imagecopy($hImage, $hImageStar0, $x, 8, 0, 0, 10, 10);
} else {
    $hImageNotGraded = imagecreatefromgif('notgraded.gif');
    imagecopy($hImage, $hImageNotGraded, 35, 8, 0, 0, 50, 10);
}Тут предполагается, что переменная nMark содержит число от 0 до 5, причём 0 означает отсутствие оценки (по крайней мере одной звёздочки достойна каждая программа :) Первый цикл рисует нужное число жёлтых звёзд, а второй — оставшееся количество белых. Если же оценки нет, вместо всего этого мы копируем один соответствующий рисунок. В итоге будет что-то похожее на это:В заключение приведу HTML-код, который нужно будет предоставить пользователям, чтобы они размещали его у себя.<a target="_blank" href="https://getsoft.ru/">
    <img
        src="http://getsoft.ru/counterimage.php?id=777"
        width="88"
        height="31"
        border="0"
    >
</a>Здесь нужно заменить getsoft.ru на ваш адрес, а также 777 на какой-нибудь реальный параметр. Вот, в общем-то и всё, дальше всё зависит только от вашей фантазии. Это моя первая статья, так что мне очень интересно, что вы о ней думаете. Не знаю, где кроме Протоплекса она будет размещена, но если сайт, с которого вы её читаете, поддерживает оставление комментариев, пожалуйста, воспользуйтесь этой возможностью. Если есть идеи, о чём ещё можно написать, я с радостью их выслушаю. Всего хорошего, и пусть ваши информеры будут не менее популярными, чем у mail.ru!    
--------------------------
Ссылка: https://habr.com/ru/companies/alfa/articles/770414/
Как много языков может влезть в одного программиста?
kopytovs  (https://habr.com/ru/users/kopytovs/)
 Всем привет, меня зовут Серёжа, я технический лидер iOS-разработки в Альфа-Банке. Сегодня я хочу поговорить о многогранности мира программирования, а именно о количестве языков и причинах, по которым они нам нужны, и о том, зачем одному программисту несколько языков.Поскольку тема довольно неоднозначна, хочу зафиксировать два момента:Всё нижеизложенное – моё личное и субъективное мнение, основанное на моём опыте.Если с каких-то рассуждений прямо сильно подгорит, приходи в комментарии, будет классно пообщаться!Из основных причин, зачем учить языки (как языки программирования, так и языки в принципе), выделю: непредсказуемое будущее (никогда заранее не знаешь, какой язык тебе пригодится), огромное количество зарубежных материалов и базирование многих вещей на английском (например, название сущностей в том же Swift). В целом с языками программирования так же, как и со знаниями: лишними точно не будут.Как я знакомился с языкамиСколько себя помню, с тех пор, как начал интересоваться ИТ, меня всегда поражало разнообразие языков программирования. В школе мы изучали Pascal, Delphi и позднее C. Я вообще не понимал принципиальную разницу между ними, зачем их так много и для чего мне учить новый язык, если я уже знаю какой-то (справедливости ради, для школьных задач все языки примерно одинаковые). Уже позже, в университете, я узнал, чем же всё-таки отличаются языки, зачем придумывают новые, и как с этим жить. Тогда же у меня и сформировалась чёткая метафора о параллели языков программирования с инструментами. Языки программирования, как инструменты в ящике рабочего. Да, можно взять только молоток, особенно если рабочий — специалист по забиванию гвоздей. Молотком можно забить саморез в стену при большом желании, но, согласитесь, если бы у вас была отвёртка или шуруповёрт, с саморезами вам было бы гораздо проще (да и саморез сел бы надёжнее). Так и с языками, нет единого, который применим вообще ко всему и на отличном уровне (привет, JavaScript). Каждый язык хорош для какой-то одной или даже нескольких задач, но использовать его везде не имеет большого смысла, так как постоянно придётся мириться с ограничениями.Главный вопрос: какие языки учить?Так какие же языки нужно учить программисту? Вопрос довольно сложный и многогранный. Да и слово «нужно» неуместно, так как особо горящей потребности в этом, как правило, нет. Я, как мобильной разработчик, могу говорить с высоты своего опыта только о фронт-разработке (простите за тавтологию), но, думаю, что и у ребят с бэкенда или системной разработки (или любых других мест, разработка живёт не одними программами) идеи будут те же самые.Конечно, тем, кто пишет на более-менее современных языках (JavaScript, Kotlin, Swift) сложно понять, что существуют другие языки зачем нужны другие языки программирования. Все они мультипарадигмальные, и на них можно писать почти что угодно, причём с удобством для себя. В чём же тогда проблема — разберёмся далее.Плагины, скрипты и на чём их пишутВозьмём утилиты, которые помогают нам в работе (я сейчас не про IDE): плагины, скрипты, автоматизаторы. На чём их пишут? Ну, наверное, можно написать на тяжёлом Swift или Kotlin, можно использовать npm и тащить с собой всю телегу с модулями различные библиотеки для JS, и это даже будет работать, но насколько это всё удобно? И что делать, если на рынке уже есть утилита, которая требует другого языка?В таком случае вам пригодятся lightweight-языки для написания простеньких скриптов, особенно те, которые позволяют развернуть нужное окружение на любой рабочей машине и жить в одном окружении всей команде. Так, например, очень удобно писать скрипты на Ruby или Python, потому что они встроены в большинство Unix-систем, а также дают возможность настроить весь workaround очень просто (например, с помощью Bundler). Можно ещё писать скрипты прям для bash (zsh и т.д.), это почти максимальный lightweight, но ты платишь за это удобством, да и не все хорошо владеют Shell сегодня.Многие из нас пишут удобные скрипты для работы, но вот в чём проблема: чтобы запустить скрипты, их нужно скачивать и разворачивать у себя, поддерживать в актуальном состоянии и т.д. Если скрипт маленький — ничего, а вот если большой, для него нужны сторонние ресурсы, которые неизвестно где находятся, а если вам ещё и нужен доступ к базам данных…Мы делаем для такого микросервисы — лайтовые внутренние «ручки», которые можно дёрнуть из внутренней сети и воспроизвести функции, например, добавить ревьюеров  в репозиторий разом во все в проекты. Так исторически сложилось, что эти микросервисы мы пишем на Go. По нынешним меркам довольно молодой язык, который уже успел заслужить одобрение со стороны многих крутых компаний, и они начали применять его в продакшене. Подробно про Go можно почитать тут. Добавлю, что это системный язык программирования, который работает очень быстро. Прикольно, занимая должность iOS-разработчика, параллельно приобретать новый востребованный навык. Понятно, что Go — неспецифический язык для iOS, в отличие от Swift, но для меня это и стало мотивацией поизучать его. На Swift тоже можно писать микросервисы, и ребята в Альфе даже пробовали. Но чаще пишем на Go, как раз чтобы попробовать что-то интересное. Были даже случаи, когда ребята из iOS уходили в бэкенд, просто потому что понравилось. Примеры с CocoaPods и Fastlane и хаосом в линтереВ iOS-разработке есть самый популярный и удобный менеджер пакетов под названием CocoaPods, который написан на Ruby (и да, я знаю про Carthage и SPM, но пока они не отняли весь рынок у CocoaPods). Знание Ruby у нас явно будет не лишним. Также есть безумно популярный и удобный автоматизатор сборки под названием Fastlane, и он тоже на Ruby. Здесь резонный вопрос: ну и что, что они на Ruby, я же их только использую? Да, всё так, но часто компании дорабатывают эти утилиты или пишут свои функции, чтобы расширить возможности. Согласитесь, в таком случае неплохо бы знать Ruby? Необязательно знать все фреймворки, вышедшие для Ruby, или все ультрасложные конструкции языка, но неплохо понимать базовые штуки. Расскажу пример из жизни: у нас в Альфе как-то раз на сборках PR начал происходить просто хаос, код менялся сам собой, причём непонятно на что (это было ещё даже до появления ChatGPT). Мы не могли понять, что происходит со сборками и начали раскапывать историю коммитов, чтобы найти виновника. Виновником оказалась функция на Ruby, которая изменяла приходящие в неё данные через параметры. В Swift для этого нужно указывать специальное слово inout, чтобы приходящие параметры можно было менять, а в Ruby это по умолчанию(сейчас профессионалы Ruby могут меня съесть, потому что это не совсем так, но в данном контексте этого достаточно). iOS-разработчик о таком, естественно, не знал. В общем и целом, забавная вышла история, мы смогли быстро найти проблему и откатить изменения. Ничего страшного не случилось, но могло что-то сломаться при сборке релиза.Теперь о CI/CDКроме скриптов и автоматизаторов, на большинстве мало-мальских серьёзных проектов используется CI/CD. Одна из самых популярных и известных таких систем (а также open source) — это Jenkins. Так вот, если вы хотите писать джобы на Jenkins, то придётся немного подучить Groovy. Если кто не знает, Groovy — это язык, построенный на базе JVM и имеющий обратную совместимость с Java, что хорошо для людей, знакомых с Java, и не очень для тех, кто регулярно пишет на других языках. Да, можно переехать на другую систему CI/CD с другим языком, но менять инструменты только из-за языка как минимум странно и непрактично.В любом случае большинство, если не все системы CI/CD так или иначе привязаны к какому-то языку разработки. Даже если вы напишите свою систему для данной цели, всё равно все джобы и скрипты нужно будет писать на каком-то одном языке. Команда чаще всего мультиплатформенная, и кому-то всё равно придётся осваивать новый язык для поддержки и доработки CI/CD. Иногда, конечно, за CI/CD отвечает отдельный человек или команда, но такое бывает только на крупных проектах. И да, умение развернуть и поддерживать CI/CD никогда лишним не будет.Pet-проекты и микросервисы для себяPet-проекты ребята пишут в свободное время, чтобы сделать что-то по кайфу. Если тебе нравится программировать, логично, что в свободное время ты не против этим заняться, особенно если хочешь сделать что-то конкретное. Я как-то написал за пару вечеров пульт для телевизора, потому что меня выбесили все приложения из стора: во-первых, они дорогие и рекламы очень много, во-вторых, я могу не хуже.У нас в команде кто-то писал fullstack-проекты с фронтом, кто-то — чисто на бэкенде. Кто-то писал софт для тренировок, кто-то — приложение для изучения английского и даже получал деньги за рекламу. Это покрывало затраты на аккаунт, плюс прилетало ещё 100$ в год сверху. Кто-то пишет проект, чтобы изучать новую технологию — это самая частая история. Компания Apple в своё время представила виджеты или Swift UI, и все побежали их пробовать. Мало прочитать гайдлайны и статьи про новые фишки, нужно пописать самому, чтобы всё понять. Для этого можно сделать калькулятор или To do лист. Главное не приложение, а как и с помощью чего ты его написал.Pet-проекты требуют нестандартных подходов и мотивируют изучать новые технологии, чтобы добавить какую-то функциональность. Когда у тебя в багаже умение пользоваться разными языками, ты можешь запросто без изучения чего-то с нуля выполнить новую работу. У тебя расширяется инструментарий. Обычно полноценные проекты разрабатываются большой командой, и до начала самой разработки подготавливаются различные артефакты, изучается рынок, планируется продукт, а также выбираются технологии для реализации. У pet-проекта нет стадии предпродакшена, разработка запускается, и необходимость в новой (и потенциально неизученной) технологии возникает сразу.Если ты когда-то писал сервер на условном Python или Ruby, то можешь пойти и развернуть бэкенд для своего приложения. Если ты этого никогда не делал, потребуется больше времени, и нужно будет изучить новый фреймворк или новый язык и новый фреймворк для него. Те, кто пишут у нас микросервисы, намного охотнее разворачивают какие-то системы, чем те, кто занимается чистым фронтом.Извечный холивар про единый язык и мультиязыкиВсе мы знаем про замечательный JavaScript. Есть огромное количество людей, которые пишут на нём бэкенд, фронтенд, мелкие скрипты, микросервисы и даже мобильные приложения. При всём уважении, это прекрасный язык для своих вещей, я не хейтер JS, но и не сторонник такого мнения. Это быстро, удобно, ты не изучаешь новые технологии. Если вам нравится — спорить не буду, вам это приносит доход и (возможно) удовольствие. Я считаю, что языки разрабатываются под конкретные нужды. У нас есть системные языки, языки для фронта, языки для мобильной разработки. Kotlin и Swift подходят много для чего, кроме того, под что они создавались, за счёт универсальности и продуманности, но имхо, скоуп этих задач ограничен. Если мы выходим за этот скоуп, могут возникнуть проблемы. Одна из проблем — отсутствие большого комьюнити. Если ты пишешь бэкенд на языке, предназначенном для фронта, тебе будет трудно встретить таких же энтузиастов. Это приводит к тому, что некому развивать и двигать технологию, находить ответы на вопросы. Даже на Stack Overflow про Vapor в разы меньше ответов. Возможно, в будущем что-то изменится, но я не Нострадамус, я не знаю. Пока, выбирая фреймворк и язык для написания бэкенда в продакшен, вряд ли большинство компаний выберет Vapor, хотя фреймворк достойный, и его можно использовать.Вернусь к сравнению языков программирования с набором инструментов. Давайте откладывать шуруповёрт, если нужно распилить дерево, и возьмём пилу, не будем заниматься ерундой. Я считаю, что проще писать на том языке, который больше подходит для нашей системы, а не пытаться натянуть его на всё. Если у вас другое мнение, а такое точно будет, буду рад услышать его в комментариях. Даже внутри нашей команды у нас расходятся точки зрения примерно 50/50.Плюсы и минусы изучения нескольких языковЧто хорошего мы получаем:Уменьшение выгорания на основном проекте за счёт изучения чего-то нового.Дополнительные навыки, которые могут пригодиться в будущем.Как следствие — изучение новых инфраструктур, архитектур и прочего.Возможность быстрее и проще решать повседневные задачи и настраивать автоматизации.Общение с новым комьюнити, рост вширь.Потенциальный рост в Full-Stack разработчика, или в DevOps, или в техлида, ну и так далее.А какие минусы могут быть:Повышение порога входа на проект с большим количеством используемых языков (для скриптов, CI и т.д.).Потенциальная каша из языков, если каждый будет использовать свой (привет, скрипты на Python с подключением JS и запуском через Ruby).Синдром самозванца в новой для себя области.Что ещё можно поизучать разработчикуИтак, вернёмся к обсуждению, какой язык учить. Это сильно зависит от вашего рода деятельности и того, чем вы хотите заниматься. Кому-то достаточно JS + React, его не в чем винить, но и осваивать новые языки и фреймворки всё же полезно.Если говорить про мою специальность, я бы посоветовал iOS-разработчику изучать, кроме Swift, опционально Objective-C (iOS легаси все-таки) и Ruby (CocoaPods, Fastlane). Это несколько каверзный язык, у нас неоднократно были с ним проблемы (выше есть даже история об одной из таких проблем). Там много неочевидных вещей, особенно для неподготовленного зрителя. Если ты не изучишь их по книжечке, они могут всплыть рано или поздно.Ещё iOS-разработчику можно изучить Kotlin, если очень хочется, он напрямую связан с Java — популярнейшим языком в мире. На Kotlin сегодня часто пишут бэкенд, поэтому его знание для столь важной части современной разработки точно не будут лишними. Kotlin — основной язык Android, огромной мобильной платформы. Круто понимать, как разрабатывают твои коллеги, особенно в большой компании. Если у тебя есть хотя бы минимальная экспертиза, ты сможешь что-то подсказать. Разрабатывая какую-то технологию, некоторые совсем не учитывают ограничения альтернативной  платформы. Тебе будет нужен оппонент, и да, ты не станешь экспертом в обеих платформах, но у тебя точно будут пререквизиты. У JetBrains есть фреймворк Kotlin Native, чтобы разрабатывать сразу для двух платформ. Пока он сыроват, его мало кто использует в проде, но в перспективе он сможет в будущем сэкономить время разработки вдвое. Несколько человек будут делать системный код, а остальные вёрстку. Уже сейчас можно смотреть в будущее и изучать новые языки.Ещё рекомендую изучить Groovy — язык над Java. Многие ребята, которые пишут на Java и Kotlin, с ним знакомы, как и ребята с Android (что логично). Для чего он айосеру? Напомню, о чём говорил выше: одна из самых популярных систем для CI/CD — это Jenkins. Ну, и чисто моя личная рекомендация — Go. Мне он понравился благодаря ультралёгкому пониманию написанного кода, а также простому вливанию в комьюнити. Язык достаточно простой, чтобы его можно было быстро изучить действующему программисту, зато он предоставляет большие возможности и быструю скорость сборки (что подкупает после Enterprise iOS-проектов). На нём легко писать небольшие скрипты и собирать данные. Я его много для чего использую, не скажу, что он незаменим, но мне Go зашёл.Как дальше жить после прочтения статьиЕсли вы прочитали статью, и вам она показалась полной чушью, значит у вас есть своё альтернативное мнение, это круто!Если статья вам не только показалась чушью, но и вы полностью не согласны с ней, отлично, жду ваших комментариев, буду рад услышать ваше мнение, это всегда интересно.Если статья вам понравилась, но вы считаете её неполной, было бы здорово, чтобы вы написали свои мысли, запилим upd.Если у вас остались вопросы, тоже пишите, отвечу, исходя из своего опыта.Программист мыслит языками программирования, фреймворками, алгоритмами и т.д., поэтому чем шире его база знаний, тем круче будет полёт его мысли. Учите языки, наш мир развивается очень быстро, особенно ИТ-сфера, классно, когда вы на острие. Языки — непосредственный инструмент для создания продукта. Возможно, именно вы придумаете универсальный язык, и нам больше не понадобится куча инструментов. А пока подрезюмируем: в программиста сможет влезть сколько угодно языков, вопрос в том, надо ли оно именно ему.      Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста. Насколько вы согласны с посылом статьи про изучение нескольких языков? 
            75.61%
           Полностью согласен, это классная тема. 
            31
           
            7.32%
           Не совсем согласен, всё-таки есть языки, на которых можно писать почти всё. 
            3
           
            12.2%
           Вообще не согласен, зачем мне 10 языков, если мой любимый {*подставьте свой язык*} умеет всё, что угодно? 
            5
           
            4.88%
           А я вообще не программист. 
            2
            
       Проголосовал 41 пользователь. 

       Воздержались 7 пользователей. 
    
--------------------------
Ссылка: https://habr.com/ru/companies/vstack/articles/780154/
vStack стал спонсором и контрибьютором новой значимой функциональности ZFS: RAIDZ Expansion
vStack  (https://habr.com/ru/users/vStack/)
 Недавно в ZFS появилась новая и очень значимая функциональность RAIDZ  expansion. Задача решалась с 2020 года и vStack принимал в ней весомое участие в качестве контрибьютора и спонсора. RAIDZ Expansion позволяет добавлять в группу RAID-Z один или несколько дисков. Предыстория создания RAIDZ ExpansionВ файловой системе ZFS в контексте zpool существует несколько механизмов резервирования с разными свойствами. Один из них — RAIDZ, по функциональности во многом схожий с RAID 5 и RAID 6, но обладающий одним существенным отличием — все диски внутри группы RAIDZ одинаковы. То есть у всех дисков в группе RAIDZ одинаковый удельный вес и отсутствуют диски с особой ролью (parity, data parity). Такой подход имеет определенные плюсы: размер группы мог быть достаточно большим, а накладные расходы на избыточность ничтожно малыми. Несмотря на то, что в группе RAIDZ, имеющих небольшое количество дисков, накладные расходы на избыточность были бы большими, с увеличением количества дисков накладные расходы на избыточность уменьшались. Например, если создана группа RAIDZ на 10 дисков, то цена избыточности составит всего лишь 10%.ИзбыточностьДля понимания проблематики избыточности нужен небольшой экскурс в резервирование и его экономическую составляющую, а именно накладные расходы.  Дело в том, что модели резервирования (2N, N+1, N+2, N+3) не существуют в вакууме — они решают определенные задачи, и каждое решение имеет свою цену. Модель резервирования 2N (два диска вместо одного) — прекрасное решение с точки зрения резервирования, но затратное с точки зрения накладных расходов, ведь цена избыточности равна 100%. Во многих случаях это неприемлемо, особенно там, где во главе угла находится экономическая эффективность. Модели резервирования N+1, N+2, N+3 — экономичнее, у них накладные расходы на избыточность значительно ниже. Например, у N+1 на группе RAIDZ из 10 дисков избыточность равна 10%, а из 20 — 5%. Это существенно меньше, чем 100% при 2N. Стоит добавить, что на рынке много хороших технологий с высокими накладными расходами, а с низкими — мало. А если к технологиям с низкими накладными расходами добавить целостность и другую востребованную функциональность, то выбор подходящих решений сжимается до небольшого числа  FAANG-технологий. Проблематика RAIDZ ExpansionС момента выхода ZFS в промышленную эксплуатацию механизм резервирования не подразумевал модификации набора дисков группы RAIDZ. То есть если создана группа из семи дисков в формате N+2, то она всегда останется группой из семи дисков. Можно было создать еще одну отдельную группу, но существующая никуда бы не делась. Это обусловлено тем, что модификация размера группы RAIDZ требовала очень длительной операции по перераспределению существующих данных. До недавнего времени в RAIDZ такую операцию выполнить было невозможно.  В сообществе ZFS, как и в любых других опенсорс-сообществах было большое число участников, обсуждавших эту проблему и способы ее решения. При этом специалисты, предлагавшие свои варианты решения проблемы, по разным причинам не брались за их реализацию. К тому же работа требовала значительных временных и вычислительных ресурсов. Фактически задачей в вялотекущем формате занимался один человек и, как следствие, сообщество было очень далеко до ее завершения.    Важно добавить, что многие сложные задачи, создаваемые внутри современных технологий с открытым кодом невозможно сделать силами одного человека. Серьезные задачи могут быть реализованы только внутри сообщества, где значимое количество участников каждый в мере своих возможностей вносят свою лепту, причем не только на окончательных стадиях, но и даже на начальных. Большее количество глаз сообщества с более высокой вероятностью увидит изъяны, появляющиеся в ходе решения, обратит на это внимание, предложит альтернативные варианты. Это критически важное отличие от разработки в одиночку. Мы в vStack приняли осознанное решение: подключиться к решению этой сложной задачи. У нашей команды есть практический опыт, и компетенции, необходимые для решения задач подобного масштаба, и вычислительные ресурсы, необходимые для обеспечения этой работы.В течение трех лет мы двигали задачу вперед. Прогресс в решении сложных задач в opensource-сообществах в такой же степени неравномерный, как и в обычной разработке: возникают и не подтвердившиеся гипотезы, и тупики, и просто заблуждения. Несмотря на все возникавшие трудности, находящиеся на виду у всего сообщества, задача RAIDZ Expansion была решена и одобрена участниками сообщества.После приема RAIDZ Expansion в основную ветку, значимость вклада vStack была отмечена попаданием в список спонсоров этой работы.Что дальшеМы приняли значимое участие в создании новой функциональности в первую очередь для того, чтобы эта возможность появилась в нашем продукте, что позволит потребителям удобнее, комфортнее и более гибко управлять масштабированием нашей гиперконвергентной платформы vStack. Другой не менее важный результат нашей работы — компетенции, появившиеся в ходе решения задачи. Они позволят использовать новую функциональность в нашем продукте в корректном виде, исключающем ошибки. Полезные ссылкиСсылка на raidz expansion feature #15022Подробное описание RAID-Z ExpansionЭта статья поддерживается командой vStackvStack — гиперконвергентная платформа для построения виртуальной инфраструктуры корпоративного уровня. Продукт входит в реестр российского ПО.  •  Наш сайт•  Наш блог про Enterprise IT во всех его проявлениях    
--------------------------
Ссылка: https://habr.com/ru/companies/kaiten/articles/780152/
Как CPO организовать работу продуктового отдела в Kaiten
trubinart  (https://habr.com/ru/users/trubinart/)
 Привет! Меня зовут Трубин Артём, я CPO в облачном провайдере ActiveCloud. Когда я пришел, у продуктового отдела не было выстроенных процессов, задачи ставились через почту, команда ощущала фоновый стресс, а руководство не устраивала скорость выполнения инициатив. Продуктовая команда и ее взаимодействие с другими отделами нуждались в трансформации, чем я и занялся. Как итог: за 6 месяцев мы прошли путь с нуля до третьего уровня зрелости по Kanban Maturity Model, на котором выстроена синхронная кросс-функциональная работа разных команд для достижения единых результатов.Стоит отметить, что на текущий момент мы только перешли на этот уровень и еще предстоит много работы по его «закреплению», но уже сейчас хотел бы поделиться опытом, как мы внедряли Канбан-метод в работу, начиная с простой визуализации задач в таск-трекере Kaiten и заканчивая прозрачным кросс-командным взаимодействием.О компанииActiveCloud — облачный провайдер. Мы занимаемся продажей облачных ресурсов и ПО для организации работы офиса, информационной безопасностью и другими облачными продуктами для организации ИТ-инфраструктуры бизнеса. В моем подчинении 4 менеджера продукта, один аналитик, а также в кросс-функциональном управлении находятся отделы R&D и биллинга.В рамках онбординга в компанию я выделил ряд проблем:коммуникация по задачам велась через почту;заказчики не видели, что происходит с их задачами, и требовали соблюдения сроков;разные команды зависели от действий друг друга, но не видели общей картины этих взаимосвязей;не было понятной приоритизации;ежедневные встречи могли длиться по два часа, но не приносить результатов.Компания не достигала целевых показателей из-за отсутствия слаженной работы всех команд и отделов.После ряда консультаций с экспертами, я пришел к выводу, что в нашем случае нужно внедрять Kanban-метод c примесью Scum-процессов, так как мы по ряду организационных ограничений не могли выделить кросс-функциональные команды, в которых были бы все необходимые специалисты для достижения результата в формате «end-to-end», что является одним из обязательных критериев для внедрения Scrum в чистом виде.Мне повезло с тем, что CEO поддерживал изменения, да и продуктовая команда была к ним предрасположена. В качестве рабочего инструмента выбрали Kaiten, так как в нем есть необходимые инструменты для выстраивания процессов по Kanban-методу. Визуализация проектовВ первую очередь нам нужно было видеть, над какими проектами мы работаем и какие команды в них вовлечены. Для этого мы завели пространство «Координация» и поделили доску на стримы (дорожки) по разным направлениям. Каждый проект фиксируется на этой доске в виде карточки на нужной дорожке и двигается по этапам процесса. В свою очередь многие из этапов делятся на дополнительные подэтапы с помощью колонок 2-го уровня. Такая визуализация необходима, чтобы сделать систему вытягивающей. Например, этап разработки делится на подэтапы «В работе» и «Готово». Если карточка находится в колонке «Разработка – Готово», значит, на стороне разработчиков задача выполнена, готова к тестированию и будет перемещена на следующий этап, только когда будут ресурсы на тестирование.Так мы получаем верхнеуровневую визуализацию работы с портфелем инициатив.Доска для визуализации инициатив в KaitenПроработка инициатив и точка принятия обязательствДальше доска делится на 2 большие части: Upstream — включает этапы по проработке инициатив и принятию решений «Делать/Не делать»; Downstream — этапы по реализации инициатив в продакшен. Здесь важно отметить, что в этот процесс включены инициативы не только по разработке и изменению продуктов. Это могут быть инициативы и по изменению бизнес-процессов компании или внедрению внутренних систем для повышения эффективности. В этом случае в Upstream мы исследуем и тестируем предлагаемое изменение, а в Downstream – внедряем изменения в регулярный бизнес-процесс.Эти части разделяются этапом «На разработку» — это точка принятия обязательств. Если карточка инициативы прошла все этапы Upstream и попала в эту колонку, значит, эта инициатива имеет проверенный и достаточный бизнес-эффект и команде необходимо взяться за ее выполнение и довести эту инициативу до продакшен.Вузуализация Upstream и DownstreamТакая система позволяет эффективно управлять потоком входящих инициатив: брать в работу только те, которые несут подтвержденную ценность для внутренних и внешних клиентов.Upstream: принятие решения о ценности инициативыНаша задача — снизить бизнес-риски и делать только то, что подтверждает положительный эффект. Для этого мы внедрили этапы валидации, исследования и тестирования инициатив.Этапы UpstreamМы собираем инициативы со всей компании (как это происходит – это тема для отдельной статьи). Собранные инициативы сначала отправляются в общую очередь и хранятся в ней. Далее менеджеры продукта берут инициативу на валидацию и назначают инициативе тип: продукт, проблема или идея. После чего пропускают ее через фильтр, например, соответствует ли она OKR или Roadmap. Это необходимо, чтобы дальше двигались только те инициативы, которые соответствуют текущим целям команды и компании. Поле «Фильтр взятия в работу» в карточке инициативыТакже, в зависимости от типа, в карточках автоматически появляются различные чек-листы с критериями, которые позволяют менеджерам продукта решить, стоит ли двигать эту инициативу дальше или отбросить ее. То есть принесет ли ее выполнение положительный эффект и реально ли это. Эти критерии могут отличаться: для валидации продукта нужны одни критерии, а для проблемы — другие.Автоматический чек-лист появляется в карточке инициативы на этапе «Валидация»Если инициатива двигается дальше, в результате валидации должна быть сформулирована гипотеза по определенному шаблону: в карточках автоматически появляются комментарии с шаблоном гипотезы. Это помогает снизить когнитивную нагрузку и задать определенные правила, которым нужно следовать. Автоматический комментарий с шаблоном гипотезы в карточке инициативыЕсли инициатива прошла валидацию, мы двигаем карточку дальше в колонку «Исследования». На этом этапе собираем данные и готовимся к тестированию.А дальше — тест. Прежде чем передавать инициативу в разработку, мы должны получить фидбэк от внутренних или внешних клиентов. Проводим А/Б тесты, фокус-группы, демонстрируем MVP и пр. Если тестирование прошло успешно, то дальше переносим инициативу на этап «Анализ и оценка для внедрения в продакшен». Здесь мы детализируем затраты, ТЗ и другие вопросы, которые необходимо прояснить для реализации инициативы в продакшен. На каждом из вышеописанных этапов в карточки тоже подтягиваются автоматические чек-листы, которые помогают соблюдать регламенты по процессу.Автоматический чек-лист в карточке, который появляется на этапе «Анализ»Когда инициатива проходит все эти этапы, то попадает в колонку «На разработку». Это значит, что мы уверены в том, что ее выполнение принесет бизнес-ценность, ресурсы по ней не будут потрачены впустую и у нас есть вся необходимая информация для ее выполнения. Здесь наступает точка принятия обязательств, то есть инициативы из этой колонки будут взяты в реализацию на продакшен и будут двигаться по процессу в Downstream. Точка принятия обязательств — колонка «На разработку»Тут нужно отметить, что далеко не все инициативы попадают в продакшен. Есть метрика, которая показывает процент отсеивания — Discard Rate. У нас она сегодня держится на уровне 30% — это то, что не идет в продакшен. Но это не значит, что инициатива полностью отсеивается, мы можем «припарковать» ее на будущее, если видим потенциал, но сейчас не время или нет ресурсов.Downstream: внедряем инициативы в продакшенВторая часть процесса направлена на реализацию задач. Downstream включает такие этапы, как:разработка, тестирование, оформление документации и обучение, ограниченный запуск, публичный запуск,анализ обратной связи.Такое детальное распределение этапов позволяет быстро определять состояние задач и находить узкие места в процессе. Например, если мы видим, что в очереди на тестирование лежит много карточек, то начинаем искать причины и устранять их. Этапы DownstreamДекомпозиция инициатив на задачи и их распределение среди командКогда инициативы идут в работу, они декомпозируются на задачи для конкретных команд и специалистов. Задачи в виде дочерних карточек создаются на отдельных рабочих досках команд: Менеджеров продукта, Аналитики, RND, Маркетинга,  Биллинга, Службы поддержки и Эксплуатации. Эти пространства спроектированы по-разному, в зависимости от нюансов организации работы команды и отдела. Например, на рабочем пространстве команды менеджеров продукта мы разделили доску на дорожки, чтобы развести задачи членов команды. Доска продакт-менеджеров поделена на дорожки по исполнителямВ главной карточке инициативы будут видны все связанные задачи и прогресс по ним. А каждая команда будет видеть только свои задачи и работать с ними на отдельных пространствах. Список подзадач, которые необходимо выполнить для реализации инициативыВзаимосвязи между отделами: прозрачное кросс-командное взаимодействиеИзначально в компании было много непрозрачных взаимозависимостей между разными командами и отделами. Одна команда отправляла задачи другой, но не знала, что с ними происходит дальше и когда будет готово. Чтобы это исправить, мы реализовали сервисный подход к внутреннему взаимодействию с помощью модуля Service desk в Kaiten.На пространствах каждой команды есть отдельная доска для приема входящих задач.Если команда-заказчик есть в Kaiten, то они просто создают карточку-задачу на пространстве команды-исполнителя в колонке «Запросы к …..» и подписываются на нее, чтобы получать уведомления о любых изменениях в этой задаче. Доска-очередь для запросов к продакт-менеджерам от других командА если команды-заказчика нет в Kaiten, то они отправляют запрос через специальный сервис модуля Service Desk, после чего команда-исполнитель увидит эту заявку в виде карточки на своей доске. Команда-исполнитель получит уведомление о том, что к ним поступила новая задача, и может дальше двигать ее по доске. А заказчик получит автоматическое оповещение, как только увидит, что его его заявка принята в работу.Пример карточки-заявки от команды заказчикаМы договариваемся на определенный SLA. Например, гарантируем, что в течение 4 часов команда-заказчик получит первичную реакцию на свой запрос. Мы не обязуемся за 4 часа выполнить заявку, но даем понять, что команда увидела обращение и оно будет обработано в соответствии с регламентом приоритетов. При таком взаимодействии запросы от команд друг к другу становятся видимыми и не теряются. К тому же мы учимся мыслить на уровне общего результата, который команды поставляют совместно.Результаты спустя полгода с момента начала внедрения Kanban-методаЗа 6 месяцев мы прошли путь от первого уровня зрелости команд по Kanban Maturity Model, на котором каждый специалист работает самостоятельно, до третьего, на котором выстроена синхронная кросс-функциональная работа разных команд для достижения общих результатов. Если кратко, то скорость решения вопросов выросла в три раза: что раньше решалось годы, сегодня решается за квартал.Конечно, на скорость изменений сильно повлиял тот факт, что запрос на трансформацию был у всех участников рабочего процесса: топ-менеджеров, линейных руководителей и членов многих команд. Но процесс изменения и дальнейшего развития не имеет конца, поэтому есть куда расти!Повысилась эффективность координационных встреч между командами: встречи длятся 15-60 минут с конкретными повестками, которые понятны благодаря визуализации процесса.Появилось явное управление приоритетами: когда все проекты собраны на координационной доске, всем становится проще жить. Топ-менеджмент видит, на чем сегодня сфокусированы ресурсы, гибко ими управляет и точнее прогнозирует сроки. А команды явно понимают, над какими задачами нужно работать в конкретный момент времени. Всё это высвобождает время и энергию всех участников процесса на достижение комплексного бизнес-результата, вместо микроменеджмента и безуспешных синхронизаций команд.    
--------------------------
